{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, RobertaModel,\n",
    "    BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/data/test_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/opt/ml/custom/new_modified_elastic_top100_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "omit_corpus = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ")  # set 은 매번 순서가 바뀌므로\n",
    "\n",
    "corpus = []\n",
    "for v in wiki.values() :\n",
    "    corpus.append(v['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bb093410114bc48ccf90332f95ce8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "elastic_indices = []\n",
    "elastic_scores = []\n",
    "for i in tqdm(range(len(data))) :\n",
    "    tt = eval(data['candidate_ids'][i])\n",
    "    tmp_indices = []\n",
    "    tmp_scores = []\n",
    "    for j in range(5) :# range(len(tt)) :\n",
    "        original_index = omit_corpus.index(corpus[int(tt[j][0])])\n",
    "        tmp_indices.append(original_index)\n",
    "        tmp_scores.append(tt[j][1])\n",
    "    elastic_indices.append(tmp_indices)\n",
    "    elastic_scores.append(tmp_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898dd7216cf34b7c86582279c9d63477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=600.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": elastic_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [omit_corpus[pid] for pid in elastic_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        final.append(tmp)\n",
    "\n",
    "cqas_50 = pd.DataFrame(final)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqas_50.to_csv('test_new_special_elastic.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/opt/ml/custom/passage_embedding_special_customsample_augmentation_20.bin', 'rb') as file :\n",
    "    p_embs = pickle.load(file)\n",
    "p_embs = p_embs\n",
    "\n",
    "q_encoder = torch.load('/opt/ml/custom/q_encoder_special_customsample_augmentation_20.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"klue/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, RobertaPreTrainedModel, RobertaModel\n",
    "\n",
    "class RoBertaEncoder(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(RoBertaEncoder, self).__init__(config)\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.init_weights()\n",
    "        classifier_dropout=(\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(classifier_dropout)\n",
    "        self.linear = torch.nn.Linear(config.hidden_size, 1)\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            #token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            #token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        output = self.linear(pooled_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_encoder = torch.load('/opt/ml/custom/c_roberta_encoder_e40_b16.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9264cac6c8c4f5896c698d2a6f9956d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-b74f2516a952>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids' : torch.tensor(tokenized_examples['input_ids'][i].unsqueeze(dim=0)).to('cuda'),\n",
      "<ipython-input-22-b74f2516a952>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask' : torch.tensor(tokenized_examples['attention_mask'][i].unsqueeze(dim=0)).to('cuda'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question_data = dataset['validation']['question']\n",
    "with torch.no_grad() : \n",
    "    c_encoder.eval()\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in tqdm(range(len(question_data))) :\n",
    "        question = question_data[i]\n",
    "        question_score = []\n",
    "        for indice in doc_indices[i] :\n",
    "            passage = omit_corpus[int(indice)]\n",
    "            tokenized_examples = tokenizer(\n",
    "                question,\n",
    "                passage,\n",
    "                truncation=\"only_second\",\n",
    "                max_length=512,\n",
    "                stride=128,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                return_token_type_ids=False,  # roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\n",
    "                padding=\"max_length\",\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            score = 0\n",
    "            for i in range(len(tokenized_examples['input_ids'])) :\n",
    "                c_input = {\n",
    "                    'input_ids' : torch.tensor(tokenized_examples['input_ids'][i].unsqueeze(dim=0)).to('cuda'),\n",
    "                    'attention_mask' : torch.tensor(tokenized_examples['attention_mask'][i].unsqueeze(dim=0)).to('cuda'),\n",
    "                    # 'token_type_ids' : torch.tensor(tokenized_examples['token_type_ids'][i].unsqueeze(dim=0)).to('cuda')\n",
    "                }\n",
    "                tmp_score = c_encoder(**c_input).to('cpu')\n",
    "                score += tmp_score\n",
    "            score = score / len(tokenized_examples['input_ids'])\n",
    "            question_score.append(score)\n",
    "        sort_result = torch.sort(torch.tensor(question_score), descending=True)\n",
    "        scores, index_list = sort_result[0], sort_result[1]\n",
    "\n",
    "        result_scores.append(scores.tolist())\n",
    "        result_indices.append(index_list.tolist())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# with open('test_bi_ce_indices.csv', 'w', newline='') as f: \n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(result_indices)\n",
    "# with open('test_bi_ce_scores.csv', 'w', newline='') as f: \n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(result_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('test_bi_ce_indices.csv', 'r', encoding='utf-8') as f:\n",
    "    rdr = csv.reader(f)\n",
    "    for i, line in enumerate(rdr) :\n",
    "        if i == 0 :\n",
    "            check_indices = line\n",
    "with open('test_bi_ce_scores.csv', 'r', encoding='utf-8') as f:\n",
    "    rdr = csv.reader(f)\n",
    "    for i, line in enumerate(rdr) :\n",
    "        if i == 0 :\n",
    "            check_scores = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_indices = []\n",
    "result_scores = []\n",
    "for i in range(len(check_indices)) :\n",
    "    result_indices.append(eval(check_indices[i]))\n",
    "    result_scores.append(eval(check_scores[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 끝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_indices = []\n",
    "final_scores = []\n",
    "for i in range(len(doc_indices)) :\n",
    "    t_list = [doc_indices[i][result_indices[i][k]] for k in range(5)]\n",
    "    s_list = result_scores[i][:5]\n",
    "    final_indices.append(t_list)\n",
    "    final_scores.append(s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6faef62ad184136b21d7a3c5ac35f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=600.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": final_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [omit_corpus[pid] for pid in final_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        final.append(tmp)\n",
    "\n",
    "cqas_50 = pd.DataFrame(final)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqas_50.to_csv('test_original.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6f96731bd3434499e23723b981a35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "scale_ce = []\n",
    "scale_elastic = []\n",
    "for ce, elastic in tqdm(zip(final_scores, elastic_scores)) :\n",
    "    temp_ce = np.array(ce).reshape(-1, 1)\n",
    "    temp_el = np.array(elastic).reshape(-1, 1)\n",
    "    scale_ce.append(scaler.fit_transform(temp_ce).reshape(-1).tolist())\n",
    "    scale_elastic.append(scaler.fit_transform(temp_el).reshape(-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_scores = []\n",
    "real_indices = []\n",
    "for i in range(len(scale_ce)) :\n",
    "    doc = {}\n",
    "    for j in range(len(scale_ce[0])) :\n",
    "        doc[final_indices[i][j]] = scale_ce[i][j] \n",
    "    \n",
    "    for j in range(len(scale_elastic[0])) :\n",
    "        if elastic_indices[i][j] in doc.keys() :\n",
    "            doc[elastic_indices[i][j]] += scale_elastic[i][j]\n",
    "        else :\n",
    "            doc[elastic_indices[i][j]] = scale_elastic[i][j]\n",
    "    doc = dict(sorted(doc.items(), key = lambda x: x[1], reverse = True))\n",
    "    real_scores.append(list(doc.values()))\n",
    "    real_indices.append(list(doc.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_end_indices = []\n",
    "for i in range(len(real_indices)) :\n",
    "    t_list = [real_indices[i][k] for k in range(5)]\n",
    "    the_end_indices.append(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b607fbd8511c46e8857039c02596dda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=600.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": the_end_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [omit_corpus[pid] for pid in the_end_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        final.append(tmp)\n",
    "\n",
    "cqas_50 = pd.DataFrame(final)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqas_50.to_csv('test_original_sepecial_ensemble.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
