{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "class elastic:\n",
    "    def __init__(self, INDEX_NAME, context_path=\"../data/wikipedia_documents.json\"):\n",
    "        self.index_name = INDEX_NAME\n",
    "        try:\n",
    "            self.es.transport.close()\n",
    "        except:\n",
    "            pass\n",
    "        self.context_path = context_path\n",
    "        self.es = Elasticsearch(timeout=100, max_retries=10, retry_on_timeout=True)\n",
    "        self.index_setting = {\n",
    "            \"settings\": {\n",
    "                \"index\": {\n",
    "                    \"analysis\": {\n",
    "                        \"analyzer\": {\n",
    "                            \"korean\": {\n",
    "                                \"type\": \"custom\",\n",
    "                                \"tokenizer\": \"nori_tokenizer\",\n",
    "                                \"filter\": [\"shingle\"],\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"korean\",\n",
    "                        \"search_analyzer\": \"korean\",\n",
    "                    },\n",
    "                    \"title\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"korean\",\n",
    "                        \"search_analyzer\": \"korean\",\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def build_elatic(self):\n",
    "        with open(self.context_path) as file:\n",
    "            json_data = json.load(file)\n",
    "        docs = []\n",
    "        for i, j in json_data.items():\n",
    "            docs.append(\n",
    "                {\n",
    "                    \"_index\": \"wikipedia\",\n",
    "                    \"_source\": {\"text\": j[\"text\"], \"title\": j[\"title\"]},\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if self.es.indices.exists(self.index_name):\n",
    "            pass\n",
    "        else:\n",
    "            self.es.indices.create(index=self.index_name, body=self.index_setting)\n",
    "            helpers.bulk(self.es, docs)\n",
    "\n",
    "    def retrieve(self, query_or_dataset, topk):\n",
    "        datas = []\n",
    "        for i in tqdm(range(len(query_or_dataset))):\n",
    "            cp = {i: v for i, v in query_or_dataset[i].items()}\n",
    "            if (\n",
    "                \"context\" in query_or_dataset[i].keys()\n",
    "                and \"answers\" in query_or_dataset[i].keys()\n",
    "            ):\n",
    "                cp[\"original_context\"] = query_or_dataset[i][\"context\"]\n",
    "\n",
    "            query = query_or_dataset[i][\"question\"]\n",
    "            query = query.replace(\"/\", \"\")\n",
    "            query = query.replace(\"~\", \" \")\n",
    "            res = self.es.search(index=self.index_name, q=query, size=topk)\n",
    "            x = res[\"hits\"][\"hits\"]\n",
    "            context = []\n",
    "            for docu in x:\n",
    "                context.append(docu[\"_source\"][\"text\"])\n",
    "            cp[\"context\"] = \"///\".join(context)\n",
    "            datas.append(cp)\n",
    "\n",
    "        return pd.DataFrame(datas)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    dataset = load_from_disk(\"../data/train_dataset\")\n",
    "    datasets = dataset[\"validation\"]\n",
    "    x = elastic(\"wikipedia\")\n",
    "    x.build_elatic()\n",
    "    p = x.retrieve(datasets, 3)\n",
    "    print(p.head())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
