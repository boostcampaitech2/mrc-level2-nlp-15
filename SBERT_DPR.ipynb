{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, RobertaModel,\n",
    "    BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamp import AdamP\n",
    "\n",
    "class DenseRetrieval:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        p_encoder,\n",
    "        q_encoder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        학습과 추론에 사용될 여러 셋업을 마쳐봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "        self.bert = SentenceTransformer(sbert_model_name)\n",
    "        self.tokenizer = sbert_model.tokenizer\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "\n",
    "    def train(self, args=None, tokenizer = None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        if tokenizer is None :\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        # q_seqs = tokenizer(self.dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        q_seqs = tokenizer(self.dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt', max_length=512)\n",
    "        # p_seqs = tokenizer(self.dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        p_seqs = tokenizer(self.dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            p_seqs['input_ids'], \n",
    "            p_seqs['attention_mask'], \n",
    "            p_seqs['token_type_ids'], \n",
    "            q_seqs['input_ids'], \n",
    "            q_seqs['attention_mask'], \n",
    "            q_seqs['token_type_ids']\n",
    "            )\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=args.per_device_train_batch_size\n",
    "            )\n",
    "\n",
    "        no_decay = [\"bias\" ,\"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamP(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            # eps=args.adam_epsilon\n",
    "        )\n",
    "\n",
    "        # t_total = len(train_dataloader) * args.num_train_epochs\n",
    "        # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "        \n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "        for epoch, _ in enumerate(train_iterator):\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            # loss_value=0 # Accumulation할 때 진행\n",
    "            losses = 0\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.q_encoder.train()\n",
    "                self.p_encoder.train()\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "                p_inputs = {'input_ids': batch[0],\n",
    "                            'attention_mask': batch[1],\n",
    "                            'token_type_ids': batch[2]\n",
    "                            }\n",
    "                \n",
    "                q_inputs = {'input_ids': batch[3],\n",
    "                            'attention_mask': batch[4],\n",
    "                            'token_type_ids': batch[5]}\n",
    "                \n",
    "                \n",
    "                p_outputs = self.p_encoder(\n",
    "                    input_ids = p_inputs[\"input_ids\"],\n",
    "                    attention_mask=p_inputs[\"attention_mask\"],\n",
    "                    token_type_ids=p_inputs[\"token_type_ids\"]\n",
    "                )  # (batch_size, emb_dim)\n",
    "                \n",
    "                q_outputs = self.q_encoder(\n",
    "                    input_ids = q_inputs[\"input_ids\"],\n",
    "                    attention_mask=q_inputs[\"attention_mask\"],\n",
    "                    token_type_ids=q_inputs[\"token_type_ids\"]\n",
    "                )  # (batch_size, emb_dim)\n",
    "\n",
    "                # Calculate similarity score & loss\n",
    "                sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "\n",
    "                # target: position of positive samples = diagonal element \n",
    "                targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "                if torch.cuda.is_available():\n",
    "                    targets = targets.to('cuda')\n",
    "\n",
    "                # almost same as cross entropy loss\n",
    "                sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "                loss = F.nll_loss(sim_scores, targets)\n",
    "\n",
    "                losses += loss.item()\n",
    "                if step % 100 == 0 :\n",
    "                    print(f'{epoch}epoch loss: {losses/(step+1)}') # Accumulation할 경우 주석처리\n",
    "\n",
    "                loss.backward()\n",
    "                #################ACCUMULATION###############################\n",
    "                # loss_value += loss\n",
    "                # if (step+1) % args.gradient_accumulation_steps == 0 :\n",
    "                #     optimizer.step()\n",
    "                #     scheduler.step()\n",
    "                #     self.q_encoder.zero_grad()\n",
    "                #     self.p_encoder.zero_grad()\n",
    "                #     global_step += 1\n",
    "                #     print(loss_value/args.gradient_accumulation_steps)\n",
    "                #     loss_value = 0\n",
    "                ############################################################\n",
    "                optimizer.step()\n",
    "                # scheduler.step()\n",
    "                self.q_encoder.zero_grad()\n",
    "                self.p_encoder.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                del p_inputs, q_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        # self.bert = BertModel(config)\n",
    "        sbert_model_name = 'KR-SBERT/KR-SBERT-V40K-klueNLI-augSTS'\n",
    "        # self.sbert_model = SentenceTransformer(sbert_model_name)\n",
    "        config = SentenceTransformer(sbert_model_name)._first_module().auto_model.config # for bert token embeddings\n",
    "        self.sbert_model = BertModel(config)\n",
    "        self.tokenizer = sbert_model.tokenizer\n",
    "        self.init_weights()\n",
    "    \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        model_output = self.sbert_model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        sentence_embeddings = self.mean_pooling(model_output, attention_mask)\n",
    "\n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/data/train_dataset')\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retrieval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5, # recommended learning rate is 1e-5\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"KR-SBERT/KR-SBERT-V40K-klueNLI-augSTS/0_Transformer\",\n",
       "  \"architectures\": [\n",
       "    \"BertModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 40000\n",
       "}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_checkpoint = \"klue/bert-base\"\n",
    "sbert_model_name = 'KR-SBERT/KR-SBERT-V40K-klueNLI-augSTS'\n",
    "sbert_model = SentenceTransformer(sbert_model_name)\n",
    "config = sbert_model._first_module().auto_model.config # for bert token embeddings\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "p_encoder = BertEncoder(config).to(args.device)\n",
    "q_encoder = BertEncoder(config).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='KR-SBERT/KR-SBERT-V40K-klueNLI-augSTS/0_Transformer', vocab_size=40000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/UKPLab/sentence-transformers/issues/794\n",
    "tokenizer = sbert_model.tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 12888, 5178, 3599, 5672, 16, 2340, 10815, 13726, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model = SentenceTransformer('KR-SBERT/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "encoded_input = tokenizer(\"내게로 와줘, 내 일상 속으로\")\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4859ce0475f494cbc587238216c557c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=247.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0epoch loss: 3.412809371948242\n"
     ]
    }
   ],
   "source": [
    "# Retriever는 아래와 같이 사용할 수 있도록 코드를 짜봅시다.\n",
    "retriever = DenseRetrieval(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder\n",
    ")\n",
    "retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "corpus = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ")  # set 은 매번 순서가 바뀌므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43779f74a8af47d9a1874ff4acb532f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56737.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p_encoder = retriever.p_encoder\n",
    "q_encoder = retriever.q_encoder\n",
    "with torch.no_grad() :\n",
    "    p_encoder.eval()\n",
    "\n",
    "    p_embs = []\n",
    "    for p in tqdm(corpus) :\n",
    "        p = tokenizer([p], padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "        p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "        p_embs.append(p_emb)\n",
    "p_embs = torch.Tensor(p_embs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개요 형태로 나열하고 있다.\\n\\n이 목록은 명료화를 위해 두 부분으로 나뉘어 있다.\\n\\n# 첫 번째 부분은 바티칸 시국과 팔레스타인을 포함하여 유엔 등 국제 기구에 가입되어 국제적인 승인을 널리 받았다고 여기는 195개 나라를 나열하고 있다.\\n# 두 번째 부분은 일부 지역의 주권을 사실상 (데 팍토) 행사하고 있지만, 아직 국제적인 승인을 널리 받지 않았다고 여기는 11개 나라를 나열하고 있다.\\n\\n두 목록은 모두 가나다 순이다.\\n\\n일부 국가의 경우 국가로서의 자격에 논쟁의 여부가 있으며, 이 때문에 이러한 목록을 엮는 것은 매우 어렵고 논란이 생길 수 있는 과정이다. 이 목록을 구성하고 있는 국가를 선정하는 기준에 대한 정보는 \"포함 기준\" 단락을 통해 설명하였다. 나라에 대한 일반적인 정보는 \"국가\" 문서에서 설명하고 있다.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8959, -0.6740,  0.8944,  ..., -0.6470,  0.8301,  0.9416],\n",
       "        [ 0.8310, -0.7759,  0.8776,  ..., -0.5936,  0.8365,  0.9866],\n",
       "        [ 0.8491, -0.6105,  0.8313,  ..., -0.8094,  0.8443,  0.9948],\n",
       "        ...,\n",
       "        [ 0.6796, -0.6418,  0.8452,  ..., -0.4144,  0.7531,  0.9738],\n",
       "        [ 0.5337, -0.7360,  0.8792,  ..., -0.4087,  0.9095,  0.9789],\n",
       "        [ 0.3616, -0.6952,  0.8875,  ..., -0.5358,  0.9320,  0.9518]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(query, q_encoder, p_embs, k=1) :\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        \n",
    "        q_seqs_val = tokenizer(\n",
    "                    [query],\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "        ).to(args.device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to(\"cpu\")  # (num_query=1, emb_dim)\n",
    "\n",
    "    dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "    rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "\n",
    "    return dot_prod_scores, rank[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47955, 22916])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'][1], q_encoder, p_embs, k = 2)\n",
    "doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries: List, q_encoder, p_embs, k=1) :\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_embs = []\n",
    "        for q in queries :\n",
    "            q = tokenizer([q], padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "            q_emb = q_encoder(**q).to('cpu').numpy()\n",
    "            q_embs.append(q_emb)\n",
    "    q_embs = torch.Tensor(q_embs).squeeze()\n",
    "\n",
    "    result = torch.matmul(q_embs, torch.transpose(p_embs, 0, 1))\n",
    "    if not isinstance(result, np.ndarray) :\n",
    "        result = result.cpu().detach().numpy()\n",
    "\n",
    "    doc_scores = []\n",
    "    doc_indices = []\n",
    "    for i in range(result.shape[0]) :\n",
    "        sorted_result = np.argsort(result[i, :][::-1])\n",
    "        doc_scores.append(result[i, :][sorted_result].tolist()[:k])\n",
    "        doc_indices.append(sorted_result.tolist()[:k])\n",
    "\n",
    "    return result, doc_scores, doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "result, doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04114fca67b443fa8a63243174ada36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Sparse retrieval: ', max=240.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, example in enumerate(\n",
    "    tqdm(dataset['validation'], desc=\"Sparse retrieval: \")\n",
    "):\n",
    "    tmp = {\n",
    "        # Query와 해당 id를 반환합니다.\n",
    "        \"question\": example[\"question\"],\n",
    "        \"id\": example[\"id\"],\n",
    "        # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "        \"context_id\": doc_indices[idx],\n",
    "        \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "            [corpus[pid] for pid in doc_indices[idx]]\n",
    "        ),\n",
    "    }\n",
    "    if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "        # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "        tmp[\"original_context\"] = example[\"context\"]\n",
    "        tmp[\"answers\"] = example[\"answers\"]\n",
    "    total.append(tmp)\n",
    "cqas = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    Sequence,\n",
    "    Value,\n",
    "    Features,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    ")\n",
    "f = Features(\n",
    "    {\n",
    "        \"answers\": Sequence(\n",
    "            feature={\n",
    "                \"text\": Value(dtype=\"string\", id=None),\n",
    "                \"answer_start\": Value(dtype=\"int32\", id=None),\n",
    "            },\n",
    "            length=-1,\n",
    "            id=None,\n",
    "        ),\n",
    "        \"context\": Value(dtype=\"string\", id=None),\n",
    "        \"id\": Value(dtype=\"string\", id=None),\n",
    "        \"question\": Value(dtype=\"string\", id=None),\n",
    "    }\n",
    ")\n",
    "datasets = DatasetDict({\"validation\": Dataset.from_pandas(cqas, features=f)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_start': [284], 'text': ['한보철강']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['validation']['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>context_id</th>\n",
       "      <th>context</th>\n",
       "      <th>original_context</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?</td>\n",
       "      <td>mrc-0-003264</td>\n",
       "      <td>[54872, 36706]</td>\n",
       "      <td>형의 회사엔 새로운 여직원이 들어왔고 형은 호감을 느끼게 된다. 그렇게 집에서 형은...</td>\n",
       "      <td>순천여자고등학교 졸업, 1973년 이화여자대학교를 졸업하고 1975년 제17회 사법...</td>\n",
       "      <td>{'answer_start': [284], 'text': ['한보철강']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>스카버러 남쪽과 코보콘그 마을의 철도 노선이 처음 연장된 연도는?</td>\n",
       "      <td>mrc-0-004762</td>\n",
       "      <td>[20806, 47909]</td>\n",
       "      <td>1읍 1구 21리로 구성된다.\\n\\n* 문덕읍 (文德邑)\\n* 상북동리 (上北洞里)...</td>\n",
       "      <td>요크 카운티 동쪽에 처음으로 여객 열차 운행이 시작한 시점은 1868년 토론토 &amp; ...</td>\n",
       "      <td>{'answer_start': [146], 'text': ['1871년']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>촌락에서 운영 위원 후보자 이름을 쓰기위해 사용된 것은?</td>\n",
       "      <td>mrc-1-001810</td>\n",
       "      <td>[2952, 8211]</td>\n",
       "      <td>제2차 세계 대전 중 붉은 군대의 전략적 작전\\n동부 전선에서의 독일 작전은 특정 ...</td>\n",
       "      <td>촐라 정부\\n 촐라의 정부 체제는 전제군주제였으며,2001 촐라의 군주는 절대적인 ...</td>\n",
       "      <td>{'answer_start': [517], 'text': ['나뭇잎']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>로타이르가 백조를 구하기 위해 사용한 것은?</td>\n",
       "      <td>mrc-1-000219</td>\n",
       "      <td>[53178, 47331]</td>\n",
       "      <td>존 라잇풋(John Lightfoot, 1602년 3월 29일 - 1675년 12월...</td>\n",
       "      <td>프랑스의 십자군 무훈시는 1099년 예루살렘 왕국의 통치자가 된 고드프루아 드 부용...</td>\n",
       "      <td>{'answer_start': [1109], 'text': ['금대야']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>의견을 자유롭게 나누는 것은 조직 내 어떤 관계에서 가능한가?</td>\n",
       "      <td>mrc-1-000285</td>\n",
       "      <td>[18113, 21840]</td>\n",
       "      <td>천보(天保)는 중국 북제(北齊) 문선제(文宣帝)의 연호이다. 550년 5월에서 55...</td>\n",
       "      <td>탈관료제화는 현대사회에서 관료제 성격이 약화되는 현상이다. 현대사회에서 관료제는 약...</td>\n",
       "      <td>{'answer_start': [386], 'text': ['수평적 관계']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>전단이 연나라와의 전쟁에서 승리했을 당시 제나라의 왕은 누구인가?</td>\n",
       "      <td>mrc-0-000484</td>\n",
       "      <td>[23539, 39234]</td>\n",
       "      <td>김동준 (金東俊, 1981년 4월 19일 ~)은 대한민국의 전직 스타크래프트 프로게...</td>\n",
       "      <td>연나라 군대의 사령관이 악의에서 기겁으로 교체되자, 전단은 스스로 신령의 계시를 받...</td>\n",
       "      <td>{'answer_start': [1084], 'text': ['제 양왕']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>공놀이 경기장 중 일부는 어디에 위치하고 있나?</td>\n",
       "      <td>mrc-0-002095</td>\n",
       "      <td>[20806, 4053]</td>\n",
       "      <td>1읍 1구 21리로 구성된다.\\n\\n* 문덕읍 (文德邑)\\n* 상북동리 (上北洞里)...</td>\n",
       "      <td>현재 우리가 볼 수 있는 티칼의 모습은 펜실베이니아 대학교와 과테말라 정부의 협조 ...</td>\n",
       "      <td>{'answer_start': [343], 'text': [''일곱 개의 신전 광장...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>창씨개명령의 시행일을 미루는 것을 수락한 인물은?</td>\n",
       "      <td>mrc-0-003083</td>\n",
       "      <td>[6872, 53505]</td>\n",
       "      <td>봉황대기 전국고교야구대회(鳳凰大旗全國高敎野球大會)는 한국일보사가 주최하는 전국 규모...</td>\n",
       "      <td>1940년 5월 1일 오전 창씨개명에 비협조적이라는 이유로 조선총독부 경무국에서 소...</td>\n",
       "      <td>{'answer_start': [247], 'text': ['미나미 지로']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>망코 잉카가 쿠스코를 되찾기 위해 마련한 군사는 총 몇 명인가?</td>\n",
       "      <td>mrc-0-002978</td>\n",
       "      <td>[10706, 10797]</td>\n",
       "      <td>울루그 베그는 알데라민을 알피르크(세페우스자리 베타), 알키드르(세페우스자리 에타)...</td>\n",
       "      <td>빌카밤바 지역은 파차쿠티 황제 때 부터 잉카 제국에 속해있던 지역이었다. 스페인 군...</td>\n",
       "      <td>{'answer_start': [563], 'text': ['200,000명']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>마르크스주의자들의 사상은?</td>\n",
       "      <td>mrc-1-000622</td>\n",
       "      <td>[15083, 489]</td>\n",
       "      <td>게자리 55 c의 공전 궤도는 55 항성계의 다른 행성들에 비해 상대적으로 이심률이...</td>\n",
       "      <td>사회주의 혁명은 오로지 선진노동자계급에 기초한 계급투쟁으로서 이루어질 수 있다고 주...</td>\n",
       "      <td>{'answer_start': [811], 'text': ['공산주의']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 question            id      context_id  \\\n",
       "0          처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?  mrc-0-003264  [54872, 36706]   \n",
       "1    스카버러 남쪽과 코보콘그 마을의 철도 노선이 처음 연장된 연도는?  mrc-0-004762  [20806, 47909]   \n",
       "2         촌락에서 운영 위원 후보자 이름을 쓰기위해 사용된 것은?  mrc-1-001810    [2952, 8211]   \n",
       "3                로타이르가 백조를 구하기 위해 사용한 것은?  mrc-1-000219  [53178, 47331]   \n",
       "4      의견을 자유롭게 나누는 것은 조직 내 어떤 관계에서 가능한가?  mrc-1-000285  [18113, 21840]   \n",
       "..                                    ...           ...             ...   \n",
       "235  전단이 연나라와의 전쟁에서 승리했을 당시 제나라의 왕은 누구인가?  mrc-0-000484  [23539, 39234]   \n",
       "236            공놀이 경기장 중 일부는 어디에 위치하고 있나?  mrc-0-002095   [20806, 4053]   \n",
       "237           창씨개명령의 시행일을 미루는 것을 수락한 인물은?  mrc-0-003083   [6872, 53505]   \n",
       "238   망코 잉카가 쿠스코를 되찾기 위해 마련한 군사는 총 몇 명인가?  mrc-0-002978  [10706, 10797]   \n",
       "239                        마르크스주의자들의 사상은?  mrc-1-000622    [15083, 489]   \n",
       "\n",
       "                                               context  \\\n",
       "0    형의 회사엔 새로운 여직원이 들어왔고 형은 호감을 느끼게 된다. 그렇게 집에서 형은...   \n",
       "1    1읍 1구 21리로 구성된다.\\n\\n* 문덕읍 (文德邑)\\n* 상북동리 (上北洞里)...   \n",
       "2    제2차 세계 대전 중 붉은 군대의 전략적 작전\\n동부 전선에서의 독일 작전은 특정 ...   \n",
       "3    존 라잇풋(John Lightfoot, 1602년 3월 29일 - 1675년 12월...   \n",
       "4    천보(天保)는 중국 북제(北齊) 문선제(文宣帝)의 연호이다. 550년 5월에서 55...   \n",
       "..                                                 ...   \n",
       "235  김동준 (金東俊, 1981년 4월 19일 ~)은 대한민국의 전직 스타크래프트 프로게...   \n",
       "236  1읍 1구 21리로 구성된다.\\n\\n* 문덕읍 (文德邑)\\n* 상북동리 (上北洞里)...   \n",
       "237  봉황대기 전국고교야구대회(鳳凰大旗全國高敎野球大會)는 한국일보사가 주최하는 전국 규모...   \n",
       "238  울루그 베그는 알데라민을 알피르크(세페우스자리 베타), 알키드르(세페우스자리 에타)...   \n",
       "239  게자리 55 c의 공전 궤도는 55 항성계의 다른 행성들에 비해 상대적으로 이심률이...   \n",
       "\n",
       "                                      original_context  \\\n",
       "0    순천여자고등학교 졸업, 1973년 이화여자대학교를 졸업하고 1975년 제17회 사법...   \n",
       "1    요크 카운티 동쪽에 처음으로 여객 열차 운행이 시작한 시점은 1868년 토론토 & ...   \n",
       "2    촐라 정부\\n 촐라의 정부 체제는 전제군주제였으며,2001 촐라의 군주는 절대적인 ...   \n",
       "3    프랑스의 십자군 무훈시는 1099년 예루살렘 왕국의 통치자가 된 고드프루아 드 부용...   \n",
       "4    탈관료제화는 현대사회에서 관료제 성격이 약화되는 현상이다. 현대사회에서 관료제는 약...   \n",
       "..                                                 ...   \n",
       "235  연나라 군대의 사령관이 악의에서 기겁으로 교체되자, 전단은 스스로 신령의 계시를 받...   \n",
       "236  현재 우리가 볼 수 있는 티칼의 모습은 펜실베이니아 대학교와 과테말라 정부의 협조 ...   \n",
       "237  1940년 5월 1일 오전 창씨개명에 비협조적이라는 이유로 조선총독부 경무국에서 소...   \n",
       "238  빌카밤바 지역은 파차쿠티 황제 때 부터 잉카 제국에 속해있던 지역이었다. 스페인 군...   \n",
       "239  사회주의 혁명은 오로지 선진노동자계급에 기초한 계급투쟁으로서 이루어질 수 있다고 주...   \n",
       "\n",
       "                                               answers  \n",
       "0            {'answer_start': [284], 'text': ['한보철강']}  \n",
       "1           {'answer_start': [146], 'text': ['1871년']}  \n",
       "2             {'answer_start': [517], 'text': ['나뭇잎']}  \n",
       "3            {'answer_start': [1109], 'text': ['금대야']}  \n",
       "4          {'answer_start': [386], 'text': ['수평적 관계']}  \n",
       "..                                                 ...  \n",
       "235         {'answer_start': [1084], 'text': ['제 양왕']}  \n",
       "236  {'answer_start': [343], 'text': [''일곱 개의 신전 광장...  \n",
       "237        {'answer_start': [247], 'text': ['미나미 지로']}  \n",
       "238      {'answer_start': [563], 'text': ['200,000명']}  \n",
       "239          {'answer_start': [811], 'text': ['공산주의']}  \n",
       "\n",
       "[240 rows x 6 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cqas"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
