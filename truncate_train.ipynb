{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- [Chris Mccormick finetuning BERT for SQUAD](https://colab.research.google.com/drive/16VjEulbATgok4mELTSaq7GTQdh3JGhGy#scrollTo=Xm1wTn09RAR7)\n",
    "- [Discussion Regarding finetuning T5](https://github.com/huggingface/transformers/issues/4426) | [Exploring T5 by patil suraj](https://github.com/patil-suraj/exploring-T5)\n",
    "    - [SQUAD QA finetuning for T5](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=KdmKlMkfcLa0)\n",
    "    - [T5 finetuning for non extractive tasks](https://colab.research.google.com/drive/176NSaYjc2eeI-78oLH_F9-YV3po3qQQO?usp=sharing)\n",
    "- [Google's T5 fine tuning example for QA](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb#scrollTo=6rU32DjyeLuL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes, as dict.key_name, not as dict[\"key_name\"] \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'klue/roberta-large',\n",
       " 'save_steps': 100,\n",
       " 'num_train_epochs': 3,\n",
       " 'learning_rate': 5e-05,\n",
       " 'batch_size': 32,\n",
       " 'warmup_steps': 300,\n",
       " 'weight_decay': 0.01,\n",
       " 'validation': False,\n",
       " 'max_length': 512,\n",
       " 'DEBUG': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Read config.yaml file\n",
    "with open(\"config.yaml\") as infile:\n",
    "    SAVED_CFG = yaml.load(infile, Loader=yaml.FullLoader)\n",
    "    SAVED_CFG = dotdict(SAVED_CFG)\n",
    "\n",
    "# arguments setting\n",
    "data_args = dotdict(SAVED_CFG.data)\n",
    "model_args = dotdict(SAVED_CFG.custom_model)\n",
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from Fine_Tune_BERT_on_SQuAD_v1_1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "datasets = load_from_disk(data_args.dataset_name)\n",
    "train_dataset_from_huggingface = datasets['train']\n",
    "valid_dataset_from_huggingface = datasets['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '미국 상원',\n",
       " 'context': '미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국 의회의 상원이다.\\\\n\\\\n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1/3씩 상원의원을 새로 선출하여 연방에 보낸다.\\\\n\\\\n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할(하원의 법안을 거부할 권한 등)을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션(공공건강보험기관)의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다.날짜=2017-02-05',\n",
       " 'question': '대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?',\n",
       " 'id': 'mrc-1-000067',\n",
       " 'answers': {'answer_start': [235], 'text': ['하원']},\n",
       " 'document_id': 18293,\n",
       " '__index_level_0__': 42}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_from_huggingface[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def pull_out_dictionary(df_input: pd.DataFrame):\n",
    "    \"\"\"pull out str `{}` values from the pandas dataframe and shape it as a new column\"\"\"\n",
    "\n",
    "    df = df_input.copy()\n",
    "\n",
    "    # assign subject_entity and object_entity column values type as dictionary\n",
    "    # df[\"answers\"] = df[\"answers\"].apply(lambda x: eval(x))\n",
    "    \n",
    "    df = df.assign(\n",
    "        # subject_entity\n",
    "        answer_start=lambda x: x[\"answers\"].apply(lambda x: x[\"answer_start\"]),\n",
    "        text=lambda x: x[\"answers\"].apply(lambda x: x[\"text\"]),\n",
    "    )\n",
    "\n",
    "    # drop subject_entity and object_entity column\n",
    "    df = df.drop([\"answers\"], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def pull_out_list(df_input: pd.DataFrame):\n",
    "    \"\"\" pull out single item out of the list \"\"\"\n",
    "    \n",
    "    df = df_input.copy()\n",
    "\n",
    "    df[\"answer_start\"] = df[\"answer_start\"].apply(lambda x: int(x[0]))\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: x[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미국 상원</td>\n",
       "      <td>미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...</td>\n",
       "      <td>대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?</td>\n",
       "      <td>mrc-1-000067</td>\n",
       "      <td>18293</td>\n",
       "      <td>42</td>\n",
       "      <td>235</td>\n",
       "      <td>하원</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>인사조직관리</td>\n",
       "      <td>'근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...</td>\n",
       "      <td>현대적 인사조직관리의 시발점이 된 책은?</td>\n",
       "      <td>mrc-0-004397</td>\n",
       "      <td>51638</td>\n",
       "      <td>2873</td>\n",
       "      <td>212</td>\n",
       "      <td>《경영의 실제》</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    title                                            context  \\\n",
       "0   미국 상원  미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...   \n",
       "1  인사조직관리  '근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...   \n",
       "\n",
       "                           question            id  document_id  \\\n",
       "0  대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?  mrc-1-000067        18293   \n",
       "1            현대적 인사조직관리의 시발점이 된 책은?  mrc-0-004397        51638   \n",
       "\n",
       "   __index_level_0__  answer_start      text  \n",
       "0                 42           235        하원  \n",
       "1               2873           212  《경영의 실제》  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>전효숙</td>\n",
       "      <td>순천여자고등학교 졸업, 1973년 이화여자대학교를 졸업하고 1975년 제17회 사법...</td>\n",
       "      <td>처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?</td>\n",
       "      <td>mrc-0-003264</td>\n",
       "      <td>9027</td>\n",
       "      <td>2146</td>\n",
       "      <td>284</td>\n",
       "      <td>한보철강</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>스토우빌선</td>\n",
       "      <td>요크 카운티 동쪽에 처음으로 여객 열차 운행이 시작한 시점은 1868년 토론토 &amp; ...</td>\n",
       "      <td>스카버러 남쪽과 코보콘그 마을의 철도 노선이 처음 연장된 연도는?</td>\n",
       "      <td>mrc-0-004762</td>\n",
       "      <td>51765</td>\n",
       "      <td>3106</td>\n",
       "      <td>146</td>\n",
       "      <td>1871년</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   title                                            context  \\\n",
       "0    전효숙  순천여자고등학교 졸업, 1973년 이화여자대학교를 졸업하고 1975년 제17회 사법...   \n",
       "1  스토우빌선  요크 카운티 동쪽에 처음으로 여객 열차 운행이 시작한 시점은 1868년 토론토 & ...   \n",
       "\n",
       "                               question            id  document_id  \\\n",
       "0        처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?  mrc-0-003264         9027   \n",
       "1  스카버러 남쪽과 코보콘그 마을의 철도 노선이 처음 연장된 연도는?  mrc-0-004762        51765   \n",
       "\n",
       "   __index_level_0__  answer_start   text  \n",
       "0               2146           284   한보철강  \n",
       "1               3106           146  1871년  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Converting train and validation dataset to Pandas dataframe for convenience \"\"\"\n",
    "\n",
    "train_df = pull_out_dictionary(pd.DataFrame.from_records(datasets['train']))\n",
    "val_df = pull_out_dictionary(pd.DataFrame.from_records(datasets['validation']))\n",
    "\n",
    "train_df = pull_out_list(train_df)\n",
    "val_df = pull_out_list(val_df)\n",
    "\n",
    "display(train_df.head(2))\n",
    "display(val_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'context', 'question', 'id', 'document_id',\n",
      "       '__index_level_0__', 'answer_start', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>corpus_source</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>html</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>TODO</td>\n",
       "      <td>None</td>\n",
       "      <td>나라 목록</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이 목록에 실린 국가 기준은 1933년 몬테비데오 협약 1장을 참고로 하였다. 협정...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>TODO</td>\n",
       "      <td>None</td>\n",
       "      <td>나라 목록</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text corpus_source   url  \\\n",
       "0  이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개...         위키피디아  TODO   \n",
       "1  이 목록에 실린 국가 기준은 1933년 몬테비데오 협약 1장을 참고로 하였다. 협정...         위키피디아  TODO   \n",
       "\n",
       "  domain  title author  html document_id  \n",
       "0   None  나라 목록   None  None           0  \n",
       "1   None  나라 목록   None  None           1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# load test dataset as dataframe\n",
    "with open(\"/opt/ml/data/wikipedia_documents.json\", \"r\", encoding=\"utf-8\") as reader:\n",
    "    input_data = json.load(reader)\n",
    "test_df = pd.DataFrame(input_data).T\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "Fixed: roberta not receiving sequence ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence id not used: klue/roberta-large\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "# load tokenizer and configuration according to the model (ex: klue/roberta-large)\n",
    "if \"roberta\" in model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, \n",
    "        model_input_names = [\"input_ids\", \"attention_mask\"],\n",
    "        use_fast=True # use rust based tokenizer    \n",
    "    )\n",
    "    print(\"sequence id not used:\", model_args.model_name_or_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "    print(model_args.model_name_or_path)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'현대 ##적 인사 ##조 ##직 ##관리 ##의 시발점 ##이 된 책 ##은 ?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample tokenization\n",
    "tokens = tokenizer.tokenize(train_dataset_from_huggingface[1]['question'])\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['크리스토', '##포', '알', '##하우스']\n",
      "Wrong Example: tensor([[21533,     7,     7,  1862,  1381,     7,     7,  6634]])\n",
      "Correctly Encoded: tensor([[21533,  2208,  1381, 17975]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# test batch tokenization\n",
    "# https://github.com/huggingface/transformers/issues/10297#issuecomment-783464293\n",
    "sample_answer_token = ['크리스토', '##포', '알', '##하우스']\n",
    "print(sample_answer_token)\n",
    "print(\"Wrong Example:\", tokenizer.encode(sample_answer_token, add_special_tokens=False, return_tensors='pt', is_split_into_words=True))\n",
    "# apply int for torch Tensor\n",
    "print(\"Correctly Encoded:\" ,torch.IntTensor([tokenizer.convert_tokens_to_ids(sample_answer_token)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3952, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '미국 상원', 'context': '미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국 의회의 상원이다.\\\\n\\\\n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1/3씩 상원의원을 새로 선출하여 연방에 보낸다.\\\\n\\\\n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할(하원의 법안을 거부할 권한 등)을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션(공공건강보험기관)의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다.날짜=2017-02-05', 'question': '대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?', 'id': 'mrc-1-000067', 'document_id': 18293, '__index_level_0__': 42, 'answer_start': 235, 'text': '하원'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'context', 'question', 'id', 'document_id', '__index_level_0__', 'answer_start', 'text'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change dataframe to dictionary\n",
    "train_df_dict = train_df.to_dict('records')\n",
    "valid_df_dict = val_df.to_dict('records')\n",
    "print(train_df_dict[0])\n",
    "train_df_dict[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['하원']\n",
      "[MASK]\n",
      "235\n",
      "237\n",
      "미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국 의회의 상원이다.\\n\\n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1/3씩 상원의원을 새로 선출하여 연방에 보낸다.\\n\\n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. [MASK]이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할(하원의 법안을 거부할 권한 등)을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션(공공건강보험기관)의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다.날짜=2017-02-05\n",
      "{'input_ids': tensor([[    0,  3698,  2069,  3954,  2470,  3666,  2079,  7895,  8586,  2207,\n",
      "          2069,   554,  2259,  3728,  3860,  2073,    35,     2,  3666, 10346,\n",
      "          2252,  4013,  3666, 10450,    12, 29963, 30605,  2041, 19148,  2012,\n",
      "          9230,    13,  1497,  1402,  2252,  2021,  2179,  3666,  4570,  2079,\n",
      "         10450, 28674,    18,     3,    81,     3,    81,  2044,  2226, 17352,\n",
      "          2052, 10450,  2079, 27345,  3622,    18,   544, 12881,    22,  2211,\n",
      "          2079, 10450,  5069,  2052,  6940,  2496,  2051,  3911,  2211,  2079,\n",
      "         10450,  5069,  6233,  3896,  2496,  2051,  1513,  2062,    18,  6724,\n",
      "          2259,    26,  2440,  2052,  2307,    16,    22,  2440, 10598,  3956,\n",
      "          2019,  2223,  1570,    21,    19,    23,  3292, 10450,  5069,  2069,\n",
      "          3755,  6940,  7488,  7145,  2170, 14352,    18,     3,    81,     3,\n",
      "            81,  2044,  2226, 10450,  2073,  3666, 11119,  2145,  2259,  4405,\n",
      "          2318,  3666,  3698,  2069, 12104,  6233,  1889,  2259,  3666,  7145,\n",
      "          7895,  2170,  4424,  5187,  2138,  1889,  2259,  3860, 28674,    18,\n",
      "             4,  1504,  5387,  2145,  3674,  2170,  3618,  5851,    16,  3698,\n",
      "          2069,  3954,  2470,  8199,  2079,  4668,  2069, 25154,  2085,  5851,\n",
      "          2069,   554,  2088,  1513,  2259,  3735,  2069,  3661,  2205,  2259,\n",
      "          3860,  2179,  4305, 10450,  2073,  3666,  2079,  1564,  2138,  3661,\n",
      "          4538,    18,  1577, 25008,    16, 14971, 16748,  2223,  3848,  1564,\n",
      "          3659,  2522,  1564,  4570,  2138,  3661,  2205,  2259,  3860, 28674,\n",
      "            18, 25687,  5426,  2173,  6017,  2079, 16849,    16,  7517,  2079,\n",
      "          6019,  2170,  3618,  5187,    16,  4042,  8604,  2170,  3618,  5887,\n",
      "           886,  6786,  2069, 15965,  2259,  5851,  2073,  3702, 10450,  2170,\n",
      "          2318,  2154,  1513,  2062,    18,  3673, 11119,  2170,  3618,  8586,\n",
      "          4008,    12, 11119,  2079,  5698,  2069,  4966,  2085,  5851,   886,\n",
      "            13,  1498,  4476,  4538,    18,    22,  2440,  2079,  6724,  2200,\n",
      "          5426,  2173, 14801,  2125,  2210,  1295,  2772,  2170,  1415,  2259,\n",
      "         11119,  2073,  6349,  2318, 14801, 31221,  5698,  2069,  3679,  2015,\n",
      "          1311,  2062,    18,  3661, 31221,  1439,  2200,  3996, 12349,  4413,\n",
      "          3817, 11119,  2052,  3666,  7145,  7895,  2170,  2318, 20634,  2870,\n",
      "          9356,    12,  4437,  2332,  2280, 12349,  2015,  2133,    13,  1503,\n",
      "          7196,  2052,  1513,  2259,  4305, 10450,  2079,  3640, 11119,  2283,\n",
      "          2052,  6349,  2318,  5387,  2052,  3732, 26539, 23048, 20634,  2870,\n",
      "          9356,  7196,  2069,  4656, 19521,  1187,  2122,  2059,  2332,  2280,\n",
      "         12349,  2015,  2133, 15351,  4048,  6166,  2116,  4476,  2205, 16818,\n",
      "          1891,   575, 28674,    18,  1504,  3640,  7925, 10450,  2073, 11119,\n",
      "         15351,  9405,  2558,  2289,  2021,  2116,  5893,  2015,  6655,  3728,\n",
      "          2031,  2079,  3906,  7925,   572,  2946,  5643,  3888,  2205,  2259,\n",
      "          4570,  2079,  6763,  2290, 31221,  4821,  2138,  5402,  2205,  2259,\n",
      "          3860, 28674,    18, 10450,  2073, 22846,  2470,  4211, 22640,  2079,\n",
      "          3640,  2116,  6342,  5698,  2069,  4019,   732,  2259,  3640,  2116,\n",
      "          8992,  2088, 11119,  2052,  4260,  5698,  2069,  5413,  7488,  3690,\n",
      "         11119,  2170, 15877,  2178,  7151,    18,  3839,  2470,  3981,  6233,\n",
      "          8046,  2021,  2116,  5893,  2015,  6655, 10997,  2069,  5134,  5402,\n",
      "          2205,  2259,   575, 28674,    18,  9384,    33,  5668,    17,  7255,\n",
      "            17,  8341,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([    0,  3698,  2069,  3954,  2470,  3666,  2079,  7895,  8586,  2207,\n",
      "         2069,   554,  2259,  3728,  3860,  2073,    35,     2,  3666, 10346,\n",
      "         2252,  4013,  3666, 10450,    12, 29963, 30605,  2041, 19148,  2012,\n",
      "         9230,    13,  1497,  1402,  2252,  2021,  2179,  3666,  4570,  2079,\n",
      "        10450, 28674,    18,     3,    81,     3,    81,  2044,  2226, 17352,\n",
      "         2052, 10450,  2079, 27345,  3622,    18,   544, 12881,    22,  2211,\n",
      "         2079, 10450,  5069,  2052,  6940,  2496,  2051,  3911,  2211,  2079,\n",
      "        10450,  5069,  6233,  3896,  2496,  2051,  1513,  2062,    18,  6724,\n",
      "         2259,    26,  2440,  2052,  2307,    16,    22,  2440, 10598,  3956,\n",
      "         2019,  2223,  1570,    21,    19,    23,  3292, 10450,  5069,  2069,\n",
      "         3755,  6940,  7488,  7145,  2170, 14352,    18,     3,    81,     3,\n",
      "           81,  2044,  2226, 10450,  2073,  3666, 11119,  2145,  2259,  4405,\n",
      "         2318,  3666,  3698,  2069, 12104,  6233,  1889,  2259,  3666,  7145,\n",
      "         7895,  2170,  4424,  5187,  2138,  1889,  2259,  3860, 28674,    18,\n",
      "            4,  1504,  5387,  2145,  3674,  2170,  3618,  5851,    16,  3698,\n",
      "         2069,  3954,  2470,  8199,  2079,  4668,  2069, 25154,  2085,  5851,\n",
      "         2069,   554,  2088,  1513,  2259,  3735,  2069,  3661,  2205,  2259,\n",
      "         3860,  2179,  4305, 10450,  2073,  3666,  2079,  1564,  2138,  3661,\n",
      "         4538,    18,  1577, 25008,    16, 14971, 16748,  2223,  3848,  1564,\n",
      "         3659,  2522,  1564,  4570,  2138,  3661,  2205,  2259,  3860, 28674,\n",
      "           18, 25687,  5426,  2173,  6017,  2079, 16849,    16,  7517,  2079,\n",
      "         6019,  2170,  3618,  5187,    16,  4042,  8604,  2170,  3618,  5887,\n",
      "          886,  6786,  2069, 15965,  2259,  5851,  2073,  3702, 10450,  2170,\n",
      "         2318,  2154,  1513,  2062,    18,  3673, 11119,  2170,  3618,  8586,\n",
      "         4008,    12, 11119,  2079,  5698,  2069,  4966,  2085,  5851,   886,\n",
      "           13,  1498,  4476,  4538,    18,    22,  2440,  2079,  6724,  2200,\n",
      "         5426,  2173, 14801,  2125,  2210,  1295,  2772,  2170,  1415,  2259,\n",
      "        11119,  2073,  6349,  2318, 14801, 31221,  5698,  2069,  3679,  2015,\n",
      "         1311,  2062,    18,  3661, 31221,  1439,  2200,  3996, 12349,  4413,\n",
      "         3817, 11119,  2052,  3666,  7145,  7895,  2170,  2318, 20634,  2870,\n",
      "         9356,    12,  4437,  2332,  2280, 12349,  2015,  2133,    13,  1503,\n",
      "         7196,  2052,  1513,  2259,  4305, 10450,  2079,  3640, 11119,  2283,\n",
      "         2052,  6349,  2318,  5387,  2052,  3732, 26539, 23048, 20634,  2870,\n",
      "         9356,  7196,  2069,  4656, 19521,  1187,  2122,  2059,  2332,  2280,\n",
      "        12349,  2015,  2133, 15351,  4048,  6166,  2116,  4476,  2205, 16818,\n",
      "         1891,   575, 28674,    18,  1504,  3640,  7925, 10450,  2073, 11119,\n",
      "        15351,  9405,  2558,  2289,  2021,  2116,  5893,  2015,  6655,  3728,\n",
      "         2031,  2079,  3906,  7925,   572,  2946,  5643,  3888,  2205,  2259,\n",
      "         4570,  2079,  6763,  2290, 31221,  4821,  2138,  5402,  2205,  2259,\n",
      "         3860, 28674,    18, 10450,  2073, 22846,  2470,  4211, 22640,  2079,\n",
      "         3640,  2116,  6342,  5698,  2069,  4019,   732,  2259,  3640,  2116,\n",
      "         8992,  2088, 11119,  2052,  4260,  5698,  2069,  5413,  7488,  3690,\n",
      "        11119,  2170, 15877,  2178,  7151,    18,  3839,  2470,  3981,  6233,\n",
      "         8046,  2021,  2116,  5893,  2015,  6655, 10997,  2069,  5134,  5402,\n",
      "         2205,  2259,   575, 28674,    18,  9384,    33,  5668,    17,  7255,\n",
      "           17,  8341,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "tensor([140])\n",
      "['한', '##보', '##철', '##강']\n",
      "[MASK] [MASK] [MASK] [MASK]\n",
      "284\n",
      "288\n",
      "순천여자고등학교 졸업, 1973년 이화여자대학교를 졸업하고 1975년 제17회 사법시험에 합격하여 판사로 임용되었고 대법원 재판연구관, 수원지법 부장판사, 사법연수원 교수, 특허법원 부장판사 등을 거쳐 능력을 인정받았다. 2003년 최종영 대법원장의 지명으로 헌법재판소 재판관을 역임하였다.\\n\\n경제민주화위원회(위원장 장하성이 소액주주들을 대표해 한보철강 부실대출에 책임이 있는 이철수 전 제일은행장 등 임원 4명을 상대로 제기한 손해배상청구소송에서 서울지방법원 민사합의17부는 1998년 7월 24일에 \"[MASK] [MASK] [MASK] [MASK]에 부실 대출하여 은행에 막대한 손해를 끼친 점이 인정된다\"며 \"원고가 배상을 청구한 400억원 전액을 은행에 배상하라\"고 하면서 부실 경영인에 대한 최초의 배상 판결을 했다. \\n\\n2004년 10월 신행정수도의건설을위한특별조치법 위헌 확인 소송에서 9인의 재판관 중 유일하게 각하 견해를 내었다. 소수의견에서 전효숙 재판관은 다수견해의 문제점을 지적하면서 관습헌법 법리를 부정하였다. 전효숙 재판관은 서울대학교 근대법학교육 백주년 기념관에서 열린 강연에서, 국회가 고도의 정치적인 사안을 정치로 풀기보다는 헌법재판소에 무조건 맡겨서 해결하려는 자세는 헌법재판소에게 부담스럽다며 소회를 밝힌 바 있다.\n",
      "{'input_ids': tensor([[    0,  3790,  6233,  6307, 20220,  2170,  3618,  5175,  5779,  2138,\n",
      "          1122,  2073,  3769,  2259,    35,     2,  8526,  2173,  2155,  2088,\n",
      "          2491,  4622,  4934,    16, 12959,  2440, 10687,  2173,  2155,  2104,\n",
      "          4622,  2138,  4934, 19521, 12924,  2440,  1545, 20835,  2124,  6150,\n",
      "         17057,  2170,  6050,  7488,  6146,  2200, 11401,  2496,  2359,  2088,\n",
      "          6739,  4503, 10363,  2133,    16,  4873,  2118,  2520,  5137,  2025,\n",
      "          2063,    16, 24772,  3865,    16,  6271,  2520,  2252,  5137,  2025,\n",
      "          2063,   886,  2069,  4811,  4053,  2069,  6157,  2886,  2062,    18,\n",
      "          5994,  2440,  4679,  2122, 13777,  2079,  7119,  6233,  5130, 21634,\n",
      "          2024, 12802,  2069, 10335,  2205,  2507,  2062,    18,     3,    81,\n",
      "             3,    81, 11769, 12095,  2267, 10355,    12,  4120, 25646, 11604,\n",
      "         10977,  2223,  2223,  2031,  2069,  3661,  2097,  1891,  2178,  2313,\n",
      "          2280,  6307,  2104,  2102,  2170,  3998,  2052,  1513,  2259, 16807,\n",
      "          2113,  1537, 30136,  2121,   886,  6101,    24,  2211,  2069,  3904,\n",
      "          2200,  4319,  2470, 19961,  2270,  2251, 18098, 27135,  3671, 16311,\n",
      "          2520,  2252, 12046,  2092,  2079, 20835,  2144,  2259,  6813,  2440,\n",
      "            27,  2429,  4056,  2210,  2170,     6,     4,     4,     4,     4,\n",
      "          1421,  6307,  4394,  7488,  3924,  2170,  7670,  2470,  5567,  2138,\n",
      "         17933,  1540,  2052,  4089,  2897,  2062,     6,  1072,     6,  7577,\n",
      "          2116,  7053,  2069,  5300,  2470,  5808,  2028,  2252,  8922,  2069,\n",
      "          3924,  2170,  7053, 16599,     6,   594,  1889, 31369,  6307, 20220,\n",
      "          2170,  3618,  4418,  2079,  7053,  5253,  2069,  1902,  2062,    18,\n",
      "             3,    81,     3,    81,  5267,  2082,  2050,  2440,  3633,  2429,\n",
      "          1327, 21002, 22757,  2079,  6668,  2069,  2090,  2470, 20000, 23557,\n",
      "          2520, 13835,  3915,  5023, 27135,    29,  2179,  2079, 12802,  1570,\n",
      "          5027,  2205,  2318, 17052,  6270,  2138,   732,  2359,  2062,    18,\n",
      "          6700,  2079,  2606, 27135,  1537,  2380,  2300, 12802,  2073,  5931,\n",
      "          2606,  2097,  2079,  6178,  2069,  3970,  5643,  2112, 11233,  2627,\n",
      "          2520, 19404,  2138,  4533,  2205,  2507,  2062,    18,  1537,  2380,\n",
      "          2300, 12802,  2073,  5689,  4622,  5496,  2520,  4622,  2193,  1132,\n",
      "          2223,  2440, 12127, 27135,  4306,  6866, 27135,    16,  3906,  2116,\n",
      "          6497,  2079,  3713, 31221,  6463,  2069,  3713,  2200,  1872,  2015,\n",
      "          2178,  4000,  5130, 21634,  2024,  2170,  6512, 10178,  2112,  4052,\n",
      "          2205,  2370,  2259,  4558,  2259,  5130, 21634,  2024,  2170,  2318,\n",
      "          4147, 19661,  2062,  2307, 24843,  2138,  6020,  1118,  1513,  2062,\n",
      "            18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([    0,  3790,  6233,  6307, 20220,  2170,  3618,  5175,  5779,  2138,\n",
      "         1122,  2073,  3769,  2259,    35,     2,  8526,  2173,  2155,  2088,\n",
      "         2491,  4622,  4934,    16, 12959,  2440, 10687,  2173,  2155,  2104,\n",
      "         4622,  2138,  4934, 19521, 12924,  2440,  1545, 20835,  2124,  6150,\n",
      "        17057,  2170,  6050,  7488,  6146,  2200, 11401,  2496,  2359,  2088,\n",
      "         6739,  4503, 10363,  2133,    16,  4873,  2118,  2520,  5137,  2025,\n",
      "         2063,    16, 24772,  3865,    16,  6271,  2520,  2252,  5137,  2025,\n",
      "         2063,   886,  2069,  4811,  4053,  2069,  6157,  2886,  2062,    18,\n",
      "         5994,  2440,  4679,  2122, 13777,  2079,  7119,  6233,  5130, 21634,\n",
      "         2024, 12802,  2069, 10335,  2205,  2507,  2062,    18,     3,    81,\n",
      "            3,    81, 11769, 12095,  2267, 10355,    12,  4120, 25646, 11604,\n",
      "        10977,  2223,  2223,  2031,  2069,  3661,  2097,  1891,  2178,  2313,\n",
      "         2280,  6307,  2104,  2102,  2170,  3998,  2052,  1513,  2259, 16807,\n",
      "         2113,  1537, 30136,  2121,   886,  6101,    24,  2211,  2069,  3904,\n",
      "         2200,  4319,  2470, 19961,  2270,  2251, 18098, 27135,  3671, 16311,\n",
      "         2520,  2252, 12046,  2092,  2079, 20835,  2144,  2259,  6813,  2440,\n",
      "           27,  2429,  4056,  2210,  2170,     6,     4,     4,     4,     4,\n",
      "         1421,  6307,  4394,  7488,  3924,  2170,  7670,  2470,  5567,  2138,\n",
      "        17933,  1540,  2052,  4089,  2897,  2062,     6,  1072,     6,  7577,\n",
      "         2116,  7053,  2069,  5300,  2470,  5808,  2028,  2252,  8922,  2069,\n",
      "         3924,  2170,  7053, 16599,     6,   594,  1889, 31369,  6307, 20220,\n",
      "         2170,  3618,  4418,  2079,  7053,  5253,  2069,  1902,  2062,    18,\n",
      "            3,    81,     3,    81,  5267,  2082,  2050,  2440,  3633,  2429,\n",
      "         1327, 21002, 22757,  2079,  6668,  2069,  2090,  2470, 20000, 23557,\n",
      "         2520, 13835,  3915,  5023, 27135,    29,  2179,  2079, 12802,  1570,\n",
      "         5027,  2205,  2318, 17052,  6270,  2138,   732,  2359,  2062,    18,\n",
      "         6700,  2079,  2606, 27135,  1537,  2380,  2300, 12802,  2073,  5931,\n",
      "         2606,  2097,  2079,  6178,  2069,  3970,  5643,  2112, 11233,  2627,\n",
      "         2520, 19404,  2138,  4533,  2205,  2507,  2062,    18,  1537,  2380,\n",
      "         2300, 12802,  2073,  5689,  4622,  5496,  2520,  4622,  2193,  1132,\n",
      "         2223,  2440, 12127, 27135,  4306,  6866, 27135,    16,  3906,  2116,\n",
      "         6497,  2079,  3713, 31221,  6463,  2069,  3713,  2200,  1872,  2015,\n",
      "         2178,  4000,  5130, 21634,  2024,  2170,  6512, 10178,  2112,  4052,\n",
      "         2205,  2370,  2259,  4558,  2259,  5130, 21634,  2024,  2170,  2318,\n",
      "         4147, 19661,  2062,  2307, 24843,  2138,  6020,  1118,  1513,  2062,\n",
      "           18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "tensor([166, 167, 168, 169])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEBUG_MODE = True\n",
    "\n",
    "def drop_truncated_data(dict_df):\n",
    "\n",
    "    # Lists to store the encoded samples.\n",
    "    all_input_ids = []\n",
    "    attention_masks = []\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    num_dropped = 0\n",
    "\n",
    "    for num, item in enumerate(dict_df):\n",
    "        answer_tokens = tokenizer.tokenize(item['text'], add_special_tokens=False)\n",
    "        # print(answer_tokens)\n",
    "        sentinel_str = \" \".join([tokenizer.mask_token]*len(answer_tokens))\n",
    "        # print(sentinel_str)\n",
    "        start_char_i = item['answer_start']\n",
    "        # print(start_char_i)\n",
    "        end_char_i = start_char_i + len(item['text'])\n",
    "        # print(end_char_i)\n",
    "        \n",
    "        context_w_sentinel = \\\n",
    "            item['context'][:start_char_i] \\\n",
    "            + sentinel_str \\\n",
    "            + item['context'][end_char_i:]\n",
    "        # print(context_w_sentinel)\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            item['question'],\n",
    "            context_w_sentinel,\n",
    "            add_special_tokens=True,\n",
    "            max_length = model_args.max_seq_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation = True\n",
    "        )\n",
    "        # print(encoded_dict)\n",
    "\n",
    "        input_ids = encoded_dict['input_ids']\n",
    "        # print(input_ids[0])\n",
    "        is_mask_token = (input_ids[0] == tokenizer.mask_token_id)\n",
    "        # print(is_mask_token)\n",
    "        mask_token_indeces = is_mask_token.nonzero(as_tuple=False)[:, 0]\n",
    "        # print(mask_token_indeces)\n",
    "        if not len(mask_token_indeces) == len(answer_tokens):\n",
    "            num_dropped += 1\n",
    "            continue\n",
    "\n",
    "        start_index = mask_token_indeces[0]\n",
    "        end_index = mask_token_indeces[-1]\n",
    "        # print(start_index, end_index)\n",
    "\n",
    "        # change start_index tensor and end_index tensor into integer type\n",
    "        start_index = start_index.item()\n",
    "        end_index = end_index.item()\n",
    "        # print(start_index, end_index)\n",
    "\n",
    "\n",
    "        # print(answer_tokens)\n",
    "        answer_token_ids = tokenizer.convert_tokens_to_ids(answer_tokens)\n",
    "        # print(answer_token_ids)\n",
    "        # print(input_ids)\n",
    "        input_ids[0, start_index : end_index + 1] = torch.tensor(answer_token_ids)\n",
    "        \n",
    "        \n",
    "        all_input_ids.append(input_ids)\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        start_positions.append(start_index)\n",
    "        end_positions.append(end_index)\n",
    "        break\n",
    "\n",
    "\n",
    "    # Convert the lists of tensors into 2D tensors.\n",
    "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    # Convert the \"labels\" (the start and end indeces) into tensors.\n",
    "    start_positions = torch.tensor(start_positions)\n",
    "    end_positions = torch.tensor(end_positions)\n",
    "\n",
    "    return all_input_ids, attention_masks, start_positions, end_positions\n",
    "\n",
    "drop_truncated_data(train_df_dict)\n",
    "drop_truncated_data(valid_df_dict)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'klue/roberta-large',\n",
       " 'save_steps': 100,\n",
       " 'num_train_epochs': 3,\n",
       " 'learning_rate': 5e-05,\n",
       " 'batch_size': 10,\n",
       " 'warmup_steps': 300,\n",
       " 'weight_decay': 0.01,\n",
       " 'validation': False,\n",
       " 'max_length': 512,\n",
       " 'DEBUG': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.batch_size = 10\n",
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_truncated = drop_truncated_data(train_df_dict)\n",
    "train_dataset = TensorDataset(train_truncated[0], train_truncated[1], train_truncated[2], train_truncated[3])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=model_args.batch_size, shuffle=True)\n",
    "\n",
    "valid_truncated = drop_truncated_data(valid_df_dict)\n",
    "valid_dataset = TensorDataset(valid_truncated[0], valid_truncated[1], valid_truncated[2], valid_truncated[3])\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=model_args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3952, 8)\n",
      "3706\n",
      "(240, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(len(train_dataset))\n",
    "\n",
    "print(val_df.shape)\n",
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoConfig\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=model_config,\n",
    "    )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Sat Oct 23 14:06:08 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   34C    P0    36W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "model_args.DEBUG = False\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() and model_args.DEBUG == False else 'cpu')\n",
    "print(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 23 14:06:12 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   34C    P0    39W / 250W |   2487MiB / 32510MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamp import AdamP\n",
    "\n",
    "optimizer = AdamP(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = model_args.num_train_epochs\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msnoop2head\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/happyface-boostcamp/KLUE-QA/runs/1bcu1gjx\" target=\"_blank\">pious-water-26</a></strong> to <a href=\"https://wandb.ai/happyface-boostcamp/KLUE-QA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training 371 batches...\n",
      "\n",
      "  Average training loss: 0.89\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation Loss: 0.58\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training 371 batches...\n",
      "\n",
      "  Average training loss: 0.22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.67\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training 371 batches...\n",
      "\n",
      "  Average training loss: 0.07\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.88\n",
      "  Validation Loss: 0.85\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import wandb \n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "# https://wandb.ai/happyface-boostcamp/KLUE-QA\n",
    "wandb.init(project='KLUE-QA', entity='happyface-boostcamp')\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    \n",
    "    print('Training {:,} batches...'.format(len(train_dataloader)))\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        # b_seg_ids = batch[\"segment_ids\"].to(device)\n",
    "        b_start_pos = batch[2].to(device)\n",
    "        b_end_pos = batch[3].to(device)\n",
    "\n",
    "        # print(batch)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(\n",
    "            b_input_ids, \n",
    "            attention_mask=b_input_mask, \n",
    "            start_positions=b_start_pos,\n",
    "            end_positions=b_end_pos,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    t0_val = time.time()\n",
    "\n",
    "    pred_start, pred_end, true_start, true_end = [], [], [], []\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in valid_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        # b_seg_ids = batch[\"segment_ids\"].to(device)\n",
    "        b_start_pos = batch[2].to(device)\n",
    "        b_end_pos = batch[3].to(device)\n",
    "\n",
    "        # print(batch)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(\n",
    "                b_input_ids, \n",
    "                # token_type_ids=b_seg_ids, \n",
    "                attention_mask=b_input_mask,\n",
    "                start_positions=b_start_pos,\n",
    "                end_positions=b_end_pos,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits        \n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        start_logits = start_logits.detach().cpu().numpy()\n",
    "        end_logits = end_logits.detach().cpu().numpy()\n",
    "        \n",
    "        b_start_pos = b_start_pos.to('cpu').numpy()\n",
    "        b_end_pos = b_end_pos.to('cpu').numpy()\n",
    "\n",
    "        answer_start = np.argmax(start_logits, axis=1)\n",
    "        answer_end = np.argmax(end_logits, axis=1)\n",
    "\n",
    "        pred_start.append(answer_start)\n",
    "        pred_end.append(answer_end)\n",
    "        true_start.append(b_start_pos)\n",
    "        true_end.append(b_end_pos)\n",
    "\n",
    "    pred_start = np.concatenate(pred_start, axis=0)\n",
    "    pred_end = np.concatenate(pred_end, axis=0)\n",
    "    true_start = np.concatenate(true_start, axis=0)\n",
    "    true_end = np.concatenate(true_end, axis=0)\n",
    "        \n",
    "    num_start_correct = np.sum(pred_start == true_start)\n",
    "    num_end_correct = np.sum(pred_end == true_end)\n",
    "\n",
    "    total_correct = num_start_correct + num_end_correct\n",
    "    total_indeces = len(true_start) + len(true_end)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = float(total_correct) / float(total_indeces)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(valid_dataloader)\n",
    "    \n",
    "    # validation_time = format_time(time.time() - t0_val)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    # print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    wandb.log(\n",
    "                    {\n",
    "                        'train/loss':avg_train_loss, \n",
    "                        'train/learning_rate':optimizer.param_groups[0]['lr'], \n",
    "                        'eval/loss':avg_val_loss,\n",
    "                        'eval/accuracy':avg_val_accuracy,\n",
    "                        }\n",
    "            )\n",
    "    \n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 23 14:27:52 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   58C    P0    50W / 250W |  27421MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datasets = load_from_disk(data_args.test_dataset_name)\n",
    "test_dataset =test_datasets['validation']\n",
    "\n",
    "# 240 public questions + 360 private questions = 600 total questions\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"유령'은 어느 행성에서 지구로 왔는가?\", 'id': 'mrc-1-000653'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
