{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae168a9-a489-4f1b-979e-db6e1360fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DprDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f35e82-d619-4a1d-967e-f56886240c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd435c56-0bca-49f3-b94e-9d380e6cbb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_dir = 'dpr_eval_dataset.csv'\n",
    "train_data_dir = 'dpr_train_dataset.csv'\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001063b1-9e3e-41e1-86e1-1cff152ea167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml/data/dpr_dataset/dpr_eval_dataset.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4d0a73-a88d-4ab5-bc20-e0069008c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = DprDataset(eval_data_dir, tokenizer)\n",
    "train_dataset = DprDataset(train_data_dir, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6507f9-39b9-4b24-bb80-8df8cf43a9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  3698,  2069,  ..., 28674,    18,     3],\n",
       "         [    2,  3698,  2069,  ...,     0,     0,     0],\n",
       "         [    2,  3698,  2069,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    2,  3718,  3710,  ...,  1551,  2088,     3],\n",
       "         [    2,  3718,  3710,  ...,  2170,  3757,     3],\n",
       "         [    2,  3718,  3710,  ...,  1633,  2079,     3]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "164336d5-d050-421c-944b-eb590cc507f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은? [SEP] 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션 ( 공공건강보험기관 ) 의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다. 날짜 = 2017 - 02 - 05 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc38c0a6-268b-47a9-836b-571cbb10a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ca534ae-5710-4aac-bde0-0149e9c8a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    metric_for_best_model='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "873a280b-99d5-4db2-b33f-0f305511ed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "152aed00-02ca-40bf-83e2-c2135470d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3293bb0-df1f-4a57-8fcc-9e49591aecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9923fb4-63bd-4842-b103-7b097c112578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='100' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 02:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1bbe9d8-0470-4919-9a10-075547ae9e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5467286109924316,\n",
       " 'eval_accuracy': 0.7236842105263158,\n",
       " 'eval_f1': 0.1301775147928994,\n",
       " 'eval_precision': 0.7857142857142857,\n",
       " 'eval_recall': 0.07096774193548387,\n",
       " 'eval_runtime': 11.6606,\n",
       " 'eval_samples_per_second': 136.871,\n",
       " 'init_mem_cpu_alloc_delta': 12288,\n",
       " 'init_mem_gpu_alloc_delta': 0,\n",
       " 'init_mem_cpu_peaked_delta': 0,\n",
       " 'init_mem_gpu_peaked_delta': 0,\n",
       " 'eval_mem_cpu_alloc_delta': 0,\n",
       " 'eval_mem_gpu_alloc_delta': 0,\n",
       " 'eval_mem_cpu_peaked_delta': 0,\n",
       " 'eval_mem_gpu_peaked_delta': 717603328}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efb35b31-4e08-4b73-a865-4ece176ce066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='4130' max='4130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4130/4130 1:26:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>0.461752</td>\n",
       "      <td>0.780075</td>\n",
       "      <td>0.654187</td>\n",
       "      <td>0.603636</td>\n",
       "      <td>0.713978</td>\n",
       "      <td>11.795000</td>\n",
       "      <td>135.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>0.420887</td>\n",
       "      <td>0.805764</td>\n",
       "      <td>0.628297</td>\n",
       "      <td>0.710027</td>\n",
       "      <td>0.563441</td>\n",
       "      <td>11.741000</td>\n",
       "      <td>135.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.398032</td>\n",
       "      <td>0.814536</td>\n",
       "      <td>0.646778</td>\n",
       "      <td>0.726542</td>\n",
       "      <td>0.582796</td>\n",
       "      <td>11.744700</td>\n",
       "      <td>135.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.494651</td>\n",
       "      <td>0.770050</td>\n",
       "      <td>0.684437</td>\n",
       "      <td>0.570201</td>\n",
       "      <td>0.855914</td>\n",
       "      <td>11.729100</td>\n",
       "      <td>136.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.399100</td>\n",
       "      <td>0.411756</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.709369</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.797849</td>\n",
       "      <td>11.731900</td>\n",
       "      <td>136.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.406600</td>\n",
       "      <td>0.385008</td>\n",
       "      <td>0.825188</td>\n",
       "      <td>0.710280</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.735484</td>\n",
       "      <td>11.760600</td>\n",
       "      <td>135.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.396400</td>\n",
       "      <td>0.419427</td>\n",
       "      <td>0.799499</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.621849</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>11.731800</td>\n",
       "      <td>136.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.409346</td>\n",
       "      <td>0.826441</td>\n",
       "      <td>0.691193</td>\n",
       "      <td>0.717593</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>11.760700</td>\n",
       "      <td>135.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.298700</td>\n",
       "      <td>0.414712</td>\n",
       "      <td>0.834586</td>\n",
       "      <td>0.722105</td>\n",
       "      <td>0.707216</td>\n",
       "      <td>0.737634</td>\n",
       "      <td>11.732400</td>\n",
       "      <td>136.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.438498</td>\n",
       "      <td>0.832080</td>\n",
       "      <td>0.725410</td>\n",
       "      <td>0.692759</td>\n",
       "      <td>0.761290</td>\n",
       "      <td>11.759000</td>\n",
       "      <td>135.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.386713</td>\n",
       "      <td>0.827068</td>\n",
       "      <td>0.724551</td>\n",
       "      <td>0.675978</td>\n",
       "      <td>0.780645</td>\n",
       "      <td>11.743500</td>\n",
       "      <td>135.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.400203</td>\n",
       "      <td>0.833960</td>\n",
       "      <td>0.729316</td>\n",
       "      <td>0.694553</td>\n",
       "      <td>0.767742</td>\n",
       "      <td>11.747100</td>\n",
       "      <td>135.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.357888</td>\n",
       "      <td>0.843358</td>\n",
       "      <td>0.715262</td>\n",
       "      <td>0.760291</td>\n",
       "      <td>0.675269</td>\n",
       "      <td>11.746500</td>\n",
       "      <td>135.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.238900</td>\n",
       "      <td>0.389074</td>\n",
       "      <td>0.838346</td>\n",
       "      <td>0.743028</td>\n",
       "      <td>0.692022</td>\n",
       "      <td>0.802151</td>\n",
       "      <td>11.730200</td>\n",
       "      <td>136.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.252100</td>\n",
       "      <td>0.372923</td>\n",
       "      <td>0.848997</td>\n",
       "      <td>0.722030</td>\n",
       "      <td>0.778607</td>\n",
       "      <td>0.673118</td>\n",
       "      <td>11.721800</td>\n",
       "      <td>136.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.392635</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.728830</td>\n",
       "      <td>0.653584</td>\n",
       "      <td>0.823656</td>\n",
       "      <td>11.703400</td>\n",
       "      <td>136.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.513387</td>\n",
       "      <td>0.839599</td>\n",
       "      <td>0.729387</td>\n",
       "      <td>0.717256</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>11.705100</td>\n",
       "      <td>136.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.460465</td>\n",
       "      <td>0.848371</td>\n",
       "      <td>0.721839</td>\n",
       "      <td>0.775309</td>\n",
       "      <td>0.675269</td>\n",
       "      <td>11.725000</td>\n",
       "      <td>136.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.490098</td>\n",
       "      <td>0.848997</td>\n",
       "      <td>0.738328</td>\n",
       "      <td>0.745614</td>\n",
       "      <td>0.731183</td>\n",
       "      <td>11.729700</td>\n",
       "      <td>136.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.124800</td>\n",
       "      <td>0.507273</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.712984</td>\n",
       "      <td>0.757869</td>\n",
       "      <td>0.673118</td>\n",
       "      <td>11.704400</td>\n",
       "      <td>136.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.475809</td>\n",
       "      <td>0.829574</td>\n",
       "      <td>0.708155</td>\n",
       "      <td>0.706638</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>11.719700</td>\n",
       "      <td>136.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.494070</td>\n",
       "      <td>0.837093</td>\n",
       "      <td>0.720430</td>\n",
       "      <td>0.720430</td>\n",
       "      <td>0.720430</td>\n",
       "      <td>11.745000</td>\n",
       "      <td>135.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>0.527690</td>\n",
       "      <td>0.836466</td>\n",
       "      <td>0.708380</td>\n",
       "      <td>0.737209</td>\n",
       "      <td>0.681720</td>\n",
       "      <td>11.713100</td>\n",
       "      <td>136.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.530196</td>\n",
       "      <td>0.830201</td>\n",
       "      <td>0.724873</td>\n",
       "      <td>0.686538</td>\n",
       "      <td>0.767742</td>\n",
       "      <td>11.724400</td>\n",
       "      <td>136.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.649467</td>\n",
       "      <td>0.838346</td>\n",
       "      <td>0.724947</td>\n",
       "      <td>0.718816</td>\n",
       "      <td>0.731183</td>\n",
       "      <td>11.713900</td>\n",
       "      <td>136.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.677938</td>\n",
       "      <td>0.848371</td>\n",
       "      <td>0.732301</td>\n",
       "      <td>0.753986</td>\n",
       "      <td>0.711828</td>\n",
       "      <td>11.744700</td>\n",
       "      <td>135.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.772151</td>\n",
       "      <td>0.825188</td>\n",
       "      <td>0.724036</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.787097</td>\n",
       "      <td>11.742200</td>\n",
       "      <td>135.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.686964</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.714136</td>\n",
       "      <td>0.695918</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>11.785800</td>\n",
       "      <td>135.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>0.711690</td>\n",
       "      <td>0.840226</td>\n",
       "      <td>0.716352</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.692473</td>\n",
       "      <td>11.764800</td>\n",
       "      <td>135.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.686193</td>\n",
       "      <td>0.832707</td>\n",
       "      <td>0.719832</td>\n",
       "      <td>0.702869</td>\n",
       "      <td>0.737634</td>\n",
       "      <td>11.716500</td>\n",
       "      <td>136.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.682654</td>\n",
       "      <td>0.832707</td>\n",
       "      <td>0.734328</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.793548</td>\n",
       "      <td>11.738300</td>\n",
       "      <td>135.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.697208</td>\n",
       "      <td>0.835840</td>\n",
       "      <td>0.735354</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.782796</td>\n",
       "      <td>11.721200</td>\n",
       "      <td>136.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>0.727980</td>\n",
       "      <td>0.843985</td>\n",
       "      <td>0.733690</td>\n",
       "      <td>0.729787</td>\n",
       "      <td>0.737634</td>\n",
       "      <td>11.717200</td>\n",
       "      <td>136.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.730907</td>\n",
       "      <td>0.839599</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.704501</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>11.725200</td>\n",
       "      <td>136.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.818487</td>\n",
       "      <td>0.846491</td>\n",
       "      <td>0.743992</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.765591</td>\n",
       "      <td>11.720900</td>\n",
       "      <td>136.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.789145</td>\n",
       "      <td>0.852757</td>\n",
       "      <td>0.736251</td>\n",
       "      <td>0.769953</td>\n",
       "      <td>0.705376</td>\n",
       "      <td>11.717200</td>\n",
       "      <td>136.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>0.787413</td>\n",
       "      <td>0.847744</td>\n",
       "      <td>0.744479</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.761290</td>\n",
       "      <td>11.722400</td>\n",
       "      <td>136.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.816782</td>\n",
       "      <td>0.847744</td>\n",
       "      <td>0.745550</td>\n",
       "      <td>0.726531</td>\n",
       "      <td>0.765591</td>\n",
       "      <td>11.753200</td>\n",
       "      <td>135.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.838932</td>\n",
       "      <td>0.847744</td>\n",
       "      <td>0.740662</td>\n",
       "      <td>0.735169</td>\n",
       "      <td>0.746237</td>\n",
       "      <td>11.742600</td>\n",
       "      <td>135.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.815727</td>\n",
       "      <td>0.850251</td>\n",
       "      <td>0.748686</td>\n",
       "      <td>0.732510</td>\n",
       "      <td>0.765591</td>\n",
       "      <td>11.738600</td>\n",
       "      <td>135.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.819466</td>\n",
       "      <td>0.849624</td>\n",
       "      <td>0.748428</td>\n",
       "      <td>0.730061</td>\n",
       "      <td>0.767742</td>\n",
       "      <td>11.734300</td>\n",
       "      <td>136.012000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4130, training_loss=0.17616025395601195, metrics={'train_runtime': 5165.8864, 'train_samples_per_second': 0.799, 'total_flos': 3.365886562866432e+16, 'epoch': 5.0, 'train_mem_cpu_alloc_delta': 48287744, 'train_mem_gpu_alloc_delta': 886121472, 'train_mem_cpu_peaked_delta': 294920192, 'train_mem_gpu_peaked_delta': 16545918976})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c69b9-02a6-4f84-b314-60bb25fe0cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
