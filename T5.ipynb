{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- [Chris Mccormick finetuning BERT for SQUAD](https://colab.research.google.com/drive/16VjEulbATgok4mELTSaq7GTQdh3JGhGy#scrollTo=Xm1wTn09RAR7)\n",
    "- [Discussion Regarding finetuning T5](https://github.com/huggingface/transformers/issues/4426) | [Exploring T5 by patil suraj](https://github.com/patil-suraj/exploring-T5)\n",
    "    - [SQUAD QA finetuning for T5](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=KdmKlMkfcLa0)\n",
    "    - [T5 finetuning for non extractive tasks](https://colab.research.google.com/drive/176NSaYjc2eeI-78oLH_F9-YV3po3qQQO?usp=sharing)\n",
    "- [Google's T5 fine tuning example for QA](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb#scrollTo=6rU32DjyeLuL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from Baseline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes, as dict.key_name, not as dict[\"key_name\"] \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from typing import List, Callable, NoReturn, NewType, Any\n",
    "import dataclasses\n",
    "from datasets import load_metric, load_from_disk, Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "from baseline.utils_qa import postprocess_qa_predictions, check_no_error\n",
    "from baseline.trainer_qa import QuestionAnsweringTrainer\n",
    "from baseline.retrieval import SparseRetrieval\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Read config.yaml file\n",
    "with open(\"config.yaml\") as infile:\n",
    "    SAVED_CFG = yaml.load(infile, Loader=yaml.FullLoader)\n",
    "    SAVED_CFG = dotdict(SAVED_CFG)\n",
    "\n",
    "# arguments setting\n",
    "data_args = dotdict(SAVED_CFG.data)\n",
    "model_args = dotdict(SAVED_CFG.custom_model)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # output directory\n",
    "    save_total_limit=5,  # number of total save model.\n",
    "    save_steps=model_args.save_steps,  # model saving step.\n",
    "    num_train_epochs=model_args.num_train_epochs,  # total number of training epochs\n",
    "    learning_rate=model_args.learning_rate,  # learning_rate\n",
    "    per_device_train_batch_size=model_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=model_args.batch_size,  # batch size for evaluation\n",
    "    warmup_steps=model_args.warmup_steps,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=model_args.weight_decay,  # strength of weight decay\n",
    "    logging_dir=\"./logs\",  # directory for storing logs\n",
    "    logging_steps=100,  # log saving step.\n",
    "    evaluation_strategy=\"steps\",  # evaluation strategy to adopt during training\n",
    "    # `no`: No evaluation during training.\n",
    "    # `steps`: Evaluate every `eval_steps`.\n",
    "    # `epoch`: Evaluate every end of epoch.\n",
    "    eval_steps=500,  # evaluation step.\n",
    "    load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW,AutoTokenizer, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from datasets import load_metric, load_from_disk, Dataset, DatasetDict\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "datasets = load_from_disk(data_args.dataset_name)\n",
    "train_dataset_from_huggingface = datasets['train']\n",
    "valid_dataset_from_huggingface = datasets['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset, train dataloader\n",
    "q_seqs = tokenizer(\n",
    "    train_dataset_from_huggingface['question'], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,  # for RoBERTa\n",
    "    )\n",
    "p_seqs = tokenizer(\n",
    "    train_dataset_from_huggingface['context'], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,  # for RoBERTa\n",
    "    )\n",
    "# print(q_seqs[0])\n",
    "train_dataset = TensorDataset(\n",
    "    p_seqs['input_ids'], \n",
    "    p_seqs['attention_mask'], \n",
    "    # p_seqs['token_type_ids'],\n",
    "    q_seqs['input_ids'], \n",
    "    q_seqs['attention_mask'], \n",
    "    # q_seqs['token_type_ids']\n",
    "    )\n",
    "train_loader = DataLoader(train_dataset,batch_size=model_args.batch_size)\n",
    "\n",
    "#valid dataset, valid dataloader\n",
    "q_seqs = tokenizer(\n",
    "    valid_dataset_from_huggingface['question'], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,  # for RoBERTa\n",
    "    )\n",
    "p_seqs = tokenizer(\n",
    "    valid_dataset_from_huggingface['context'], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,  # for RoBERTa\n",
    "    )\n",
    "# print(q_seqs[0])\n",
    "valid_dataset = TensorDataset(\n",
    "    p_seqs['input_ids'], \n",
    "    p_seqs['attention_mask'], \n",
    "    # p_seqs['token_type_ids'],\n",
    "    q_seqs['input_ids'], \n",
    "    q_seqs['attention_mask'], \n",
    "    # q_seqs['token_type_ids']\n",
    "    )\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=model_args.batch_size\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from Fine_Tune_BERT_on_SQuAD_v1_1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_info': DatasetInfo(description='', citation='', homepage='', license='', features={'__index_level_0__': Value(dtype='int64', id=None), 'answers': {'answer_start': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'context': Value(dtype='string', id=None), 'document_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name=None, config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None),\n",
       " '_split': None,\n",
       " '_indexes': {},\n",
       " '_data': pyarrow.Table\n",
       " title: string\n",
       " context: string\n",
       " question: string\n",
       " id: string\n",
       " answers: struct<answer_start: list<item: int64>, text: list<item: string>>\n",
       "   child 0, answer_start: list<item: int64>\n",
       "       child 0, item: int64\n",
       "   child 1, text: list<item: string>\n",
       "       child 0, item: string\n",
       " document_id: int64\n",
       " __index_level_0__: int64,\n",
       " '_indices': pyarrow.Table\n",
       " indices: uint64,\n",
       " '_data_files': [{'filename': '/opt/ml/data/train_dataset/train/dataset.arrow'}],\n",
       " '_indices_data_files': [{'filename': '/opt/ml/data/train_dataset/train/indices.arrow'}],\n",
       " '_inplace_history': [{'transforms': []}],\n",
       " '_format_type': None,\n",
       " '_format_kwargs': {},\n",
       " '_format_columns': None,\n",
       " '_output_all_columns': False,\n",
       " '_fingerprint': 'd8cf0ec86e1a54fc'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look inside dataset at huggingface\n",
    "vars(train_dataset_from_huggingface)\n",
    "\n",
    "#  title: string\n",
    "#  context: string\n",
    "#  question: string\n",
    "#  id: string\n",
    "#  answers: struct < answer_start: list<item: int64>, text: list<item: string> >\n",
    "#    child 0, answer_start: list<item: int64>\n",
    "#        child 0, item: int64\n",
    "#    child 1, text: list<item: string>\n",
    "#        child 0, item: string\n",
    "#  document_id: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '인사조직관리',\n",
       " 'context': \"'근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 1950년대이다. 2차 세계대전을 마치고, 6.25전쟁의 시기로 유럽은 전후 재건에 집중하고, 유럽 제국주의의 식민지가 독립하여 아프리카, 아시아, 아메리카 대륙에서 신생국가가 형성되는 시기였고, 미국은 전쟁 이후 경제적 변화에 기업이 적응을 해야 하던 시기였다. 특히 1954년 피터 드러커의 저서 《경영의 실제》는 현대적 경영의 기준을 제시하여서, 기존 근대적 인사조직관리를 넘어선 현대적 인사조직관리의 전환점이 된다. 드러커는 경영자의 역할을 강조하며 경영이 현시대 최고의 예술이자 과학이라고 주장하였고 , 이 주장은 21세기 인사조직관리의 역할을 자리매김했다.\\\\n\\\\n현대적 인사조직관리와 근대 인사조직관리의 가장 큰 차이는 통합이다. 19세기의 영향을 받던 근대적 경영학(고전적 경영)의 흐름은 기능을 강조하였지만, 1950년대 이후의 현대 경영학은 통합을 강조하였다. 기능이 분화된 '기계적인 기업조직' 이해에서 다양한 기능을 인사조직관리의 목적, 경영의 목적을 위해서 다양한 분야를 통합하여 '유기적 기업 조직' 이해로 전환되었다. 이 통합적 접근방식은 과정, 시스템, 상황을 중심으로 하는 인사조직관리 방식을 형성했다.\",\n",
       " 'question': '현대적 인사조직관리의 시발점이 된 책은?',\n",
       " 'id': 'mrc-0-004397',\n",
       " 'answers': {'answer_start': [212], 'text': ['《경영의 실제》']},\n",
       " 'document_id': 51638,\n",
       " '__index_level_0__': 2873}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_from_huggingface[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5! T5!\n",
    "\n",
    "![img](https://pbs.twimg.com/media/D2RWVuzWoAEw8fm?format=jpg&name=medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'ke-t5'...\n",
      "remote: Enumerating objects: 197, done.\u001b[K\n",
      "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
      "remote: Compressing objects: 100% (172/172), done.\u001b[K\n",
      "remote: Total 197 (delta 111), reused 85 (delta 13), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (197/197), 783.74 KiB | 1.08 MiB/s, done.\n",
      "Resolving deltas: 100% (111/111), done.\n",
      "/opt/ml/mrc-level2-nlp-15/ke-t5\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting t5==0.9.1\n",
      "  Downloading t5-0.9.1-py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 21.9 MB/s \n",
      "\u001b[?25hCollecting seqio\n",
      "  Downloading seqio-0.0.6-py3-none-any.whl (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 89.6 MB/s \n",
      "\u001b[?25hCollecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from t5==0.9.1->-r requirements.txt (line 1)) (0.24.1)\n",
      "Collecting tfds-nightly\n",
      "  Downloading tfds_nightly-4.4.0.dev202110180107-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 82.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from t5==0.9.1->-r requirements.txt (line 1)) (1.1.4)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.4.0-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 8.3 MB/s \n",
      "\u001b[?25hCollecting mesh-tensorflow[transformer]>=0.1.13\n",
      "  Downloading mesh_tensorflow-0.1.19-py3-none-any.whl (366 kB)\n",
      "\u001b[K     |████████████████████████████████| 366 kB 81.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.14 in /opt/conda/lib/python3.8/site-packages (from t5==0.9.1->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from t5==0.9.1->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from t5==0.9.1->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: transformers>=2.7.0 in /opt/conda/lib/python3.8/site-packages (from t5==0.9.1->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from t5==0.9.1->-r requirements.txt (line 1)) (1.19.2)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 75.5 MB/s \n",
      "\u001b[?25hCollecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.6.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 76.9 MB/s \n",
      "\u001b[?25hCollecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 16.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: babel in /opt/conda/lib/python3.8/site-packages (from t5==0.9.1->-r requirements.txt (line 1)) (2.9.1)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-0.14.1-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 93.5 MB/s \n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 75.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->t5==0.9.1->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->t5==0.9.1->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5==0.9.1->-r requirements.txt (line 1)) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5==0.9.1->-r requirements.txt (line 1)) (2.24.0)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-5.2.3-py3-none-any.whl (27 kB)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5==0.9.1->-r requirements.txt (line 1)) (4.41.1)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.2.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 10.4 MB/s \n",
      "\u001b[?25hCollecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 73.2 MB/s \n",
      "\u001b[?25hCollecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5==0.9.1->-r requirements.txt (line 1)) (3.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->t5==0.9.1->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas->t5==0.9.1->-r requirements.txt (line 1)) (2020.5)\n",
      "Collecting tensorflow-datasets; extra == \"transformer\"\n",
      "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 70.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->t5==0.9.1->-r requirements.txt (line 1)) (3.10.0.2)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5==0.9.1->-r requirements.txt (line 1)) (0.0.46)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5==0.9.1->-r requirements.txt (line 1)) (3.0.12)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5==0.9.1->-r requirements.txt (line 1)) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5==0.9.1->-r requirements.txt (line 1)) (2021.10.8)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5==0.9.1->-r requirements.txt (line 1)) (0.10.3)\n",
      "Collecting tensorflow<2.7,>=2.6.0\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl (458.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 458.4 MB 27 kB/s \n",
      "\u001b[?25hCollecting tensorflow-hub>=0.8.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 94.9 MB/s \n",
      "\u001b[?25hCollecting portalocker\n",
      "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.8/site-packages (from sacrebleu->t5==0.9.1->-r requirements.txt (line 1)) (0.4.4)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->t5==0.9.1->-r requirements.txt (line 1)) (8.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tfds-nightly->t5==0.9.1->-r requirements.txt (line 1)) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tfds-nightly->t5==0.9.1->-r requirements.txt (line 1)) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tfds-nightly->t5==0.9.1->-r requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tfds-nightly->t5==0.9.1->-r requirements.txt (line 1)) (2.10)\n",
      "Collecting zipp>=3.1.0; python_version < \"3.10\"\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 86.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-datasets; extra == \"transformer\"->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.1->-r requirements.txt (line 1)) (21.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers>=2.7.0->t5==0.9.1->-r requirements.txt (line 1)) (2.4.7)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.9 MB/s \n",
      "\u001b[?25hCollecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 74.5 MB/s \n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 6.1 MB/s \n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.41.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 51.8 MB/s \n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 54.1 MB/s \n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 80.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->t5==0.9.1->-r requirements.txt (line 1)) (0.35.1)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 50.4 MB/s \n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 7.4 MB/s \n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text->t5==0.9.1->-r requirements.txt (line 1)) (50.3.1.post20201107)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text->t5==0.9.1->-r requirements.txt (line 1)) (2.0.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 66.9 MB/s \n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 13.0 MB/s \n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 70.3 MB/s \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.0-py2.py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 67.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text->t5==0.9.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 90.4 MB/s \n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text->t5==0.9.1->-r requirements.txt (line 1)) (3.1.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 10.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: promise, future, termcolor, wrapt, clang\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=956ed639c05984f48d78a398f13f5e172ecf072f1ba03dac6088d3433afa3400\n",
      "  Stored in directory: /opt/ml/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=902b48f1babbe501234dafefa394338f3be415ce598c9c34080ad1585b2d81bd\n",
      "  Stored in directory: /opt/ml/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=069256c018e749757bb77323fdd2c47b86b92f8f6a4e1a97d6e268da123d2500\n",
      "  Stored in directory: /opt/ml/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=78012 sha256=779be710a21e22f3664164bed6b0c00c5343bf479cc6843217e90953db38e517\n",
      "  Stored in directory: /opt/ml/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30705 sha256=1fb9399adccfc0478deb0e2ee2fec592a33b8775ea4ea08b460aaf59e97e0740\n",
      "  Stored in directory: /opt/ml/.cache/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "Successfully built promise future termcolor wrapt clang\n",
      "Installing collected packages: absl-py, nltk, rouge-score, zipp, importlib-resources, promise, googleapis-common-protos, tensorflow-metadata, future, termcolor, tfds-nightly, gin-config, tensorflow-datasets, mesh-tensorflow, sentencepiece, keras-preprocessing, h5py, google-pasta, astunparse, grpcio, wrapt, keras, clang, tensorflow-estimator, tensorboard-data-server, markdown, tensorboard-plugin-wit, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-auth-oauthlib, tensorboard, opt-einsum, flatbuffers, gast, tensorflow, tensorflow-hub, tensorflow-text, seqio, portalocker, tabulate, sacrebleu, t5\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow-metadata 1.2.0 requires absl-py<0.13,>=0.9, but you'll have absl-py 0.14.1 which is incompatible.\n",
      "tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you'll have typing-extensions 3.10.0.2 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.14.1 astunparse-1.6.3 cachetools-4.2.4 clang-5.0 flatbuffers-1.12 future-0.18.2 gast-0.4.0 gin-config-0.4.0 google-auth-2.3.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 googleapis-common-protos-1.53.0 grpcio-1.41.0 h5py-3.1.0 importlib-resources-5.2.3 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 mesh-tensorflow-0.1.19 nltk-3.6.5 opt-einsum-3.3.0 portalocker-2.3.2 promise-2.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 rouge-score-0.0.4 rsa-4.7.2 sacrebleu-2.0.0 sentencepiece-0.1.96 seqio-0.0.6 t5-0.9.1 tabulate-0.8.9 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-datasets-4.4.0 tensorflow-estimator-2.6.0 tensorflow-hub-0.12.0 tensorflow-metadata-1.2.0 tensorflow-text-2.6.0 termcolor-1.1.0 tfds-nightly-4.4.0.dev202110180107 wrapt-1.12.1 zipp-3.6.0\n",
      "/opt/ml/mrc-level2-nlp-15\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:AIRC-KETI/ke-t5.git\n",
    "%cd ke-t5\n",
    "!pip3 install -r requirements.txt\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83dae4f3875460ca2e7dfc7537a3ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=599.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93c47ec197542d792cfc65b9f8dc42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=990048016.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# https://github.com/AIRC-KETI/ke-t5\n",
    "# downstream tasks: ke_t5_korquad_allanswers, klue_mrc_gen, ke_t5_ko_en_qa_equal, ke_t5_trivi_qa_v010\n",
    "# ke_t5_ko_en_qa_proportional, ke_t5_ko_en_qa_equal, ke_t5_korquad_allanswers, ke_t5_squad_v010_allanswers, ke_t5_trivia_qa_v010\n",
    "# Summarization과 Extractive QA의 경우 입력이 모델들의 최대 시퀀스 길이인 512를 넘을 수가 있습니다. 이 경우 넘는 부분을 그냥 버리고 학습과 성능측정을 진행했습니다. 즉, 요약의 경우 512 토큰을 초과하는 부분은 요약이 되지 않고, extractive QA의 경우 question에 대한 정답이 512 토큰을 초과하는 부분에 존재하면 답을 찾을 수 없습니다. 이러한 경우를 처리하여 성능을 높이기 위해서는 직접 학습 프로그램을 만들어 사용하셔야 합니다. (e.g. Extractive QA의 경우, BERT에서처럼 document span으로 분리하여 해당 span에 정답이 있는지 없는지로 학습.\n",
    "model_name = 'KETI-AIR/ke-t5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
