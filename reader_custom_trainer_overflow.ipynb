{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- [Chris Mccormick finetuning BERT for SQUAD](https://colab.research.google.com/drive/16VjEulbATgok4mELTSaq7GTQdh3JGhGy#scrollTo=Xm1wTn09RAR7)\n",
    "- [Discussion Regarding finetuning T5](https://github.com/huggingface/transformers/issues/4426) | [Exploring T5 by patil suraj](https://github.com/patil-suraj/exploring-T5)\n",
    "    - [SQUAD QA finetuning for T5](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=KdmKlMkfcLa0)\n",
    "    - [T5 finetuning for non extractive tasks](https://colab.research.google.com/drive/176NSaYjc2eeI-78oLH_F9-YV3po3qQQO?usp=sharing)\n",
    "- [Google's T5 fine tuning example for QA](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb#scrollTo=6rU32DjyeLuL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes, as dict.key_name, not as dict[\"key_name\"] \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'klue/roberta-large',\n",
       " 'save_steps': 100,\n",
       " 'num_train_epochs': 3,\n",
       " 'learning_rate': 2e-05,\n",
       " 'batch_size': 10,\n",
       " 'warmup_steps': 300,\n",
       " 'weight_decay': 0.01,\n",
       " 'validation': False,\n",
       " 'max_length': 512,\n",
       " 'DEBUG': True,\n",
       " 'num_rnn_layers': 2,\n",
       " 'num_folds': 4,\n",
       " 'gamma': 1.0,\n",
       " 'smoothing': 0.2,\n",
       " 'doc_stride': 128}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Read config.yaml file\n",
    "with open(\"config.yaml\") as infile:\n",
    "    SAVED_CFG = yaml.load(infile, Loader=yaml.FullLoader)\n",
    "    SAVED_CFG = dotdict(SAVED_CFG)\n",
    "\n",
    "# arguments setting\n",
    "data_args = dotdict(SAVED_CFG.data)\n",
    "model_args = dotdict(SAVED_CFG.custom_model)\n",
    "\n",
    "# adding additional arguments\n",
    "# model_args.model_name_or_path = \"jaeyoung/klue-roberta-large-wiki-mlm\"\n",
    "model_args.batch_size = 10\n",
    "model_args.num_rnn_layers = 2\n",
    "model_args.learning_rate = 2e-5\n",
    "model_args.num_folds = 4\n",
    "model_args.gamma = 1.0\n",
    "model_args.smoothing = 0.2\n",
    "model_args.doc_stride = 128\n",
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from Fine_Tune_BERT_on_SQuAD_v1_1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "datasets = load_from_disk(data_args.dataset_name)\n",
    "train_dataset_from_huggingface = datasets['train']\n",
    "valid_dataset_from_huggingface = datasets['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '미국 상원',\n",
       " 'context': '미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국 의회의 상원이다.\\\\n\\\\n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1/3씩 상원의원을 새로 선출하여 연방에 보낸다.\\\\n\\\\n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할(하원의 법안을 거부할 권한 등)을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션(공공건강보험기관)의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다.날짜=2017-02-05',\n",
       " 'question': '대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?',\n",
       " 'id': 'mrc-1-000067',\n",
       " 'answers': {'answer_start': [235], 'text': ['하원']},\n",
       " 'document_id': 18293,\n",
       " '__index_level_0__': 42}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_from_huggingface[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def pull_out_dictionary(df_input: pd.DataFrame):\n",
    "    \"\"\"pull out str `{}` values from the pandas dataframe and shape it as a new column\"\"\"\n",
    "\n",
    "    df = df_input.copy()\n",
    "\n",
    "    # assign subject_entity and object_entity column values type as dictionary\n",
    "    # df[\"answers\"] = df[\"answers\"].apply(lambda x: eval(x))\n",
    "    \n",
    "    df = df.assign(\n",
    "        # subject_entity\n",
    "        answer_start=lambda x: x[\"answers\"].apply(lambda x: x[\"answer_start\"]),\n",
    "        text=lambda x: x[\"answers\"].apply(lambda x: x[\"text\"]),\n",
    "    )\n",
    "\n",
    "    # drop subject_entity and object_entity column\n",
    "    df = df.drop([\"answers\"], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def pull_out_list(df_input: pd.DataFrame):\n",
    "    \"\"\" pull out single item out of the list \"\"\"\n",
    "    \n",
    "    df = df_input.copy()\n",
    "\n",
    "    df[\"answer_start\"] = df[\"answer_start\"].apply(lambda x: int(x[0]))\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: x[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3952, 8)\n",
      "(240, 8)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Converting train and validation dataset to Pandas dataframe for convenience \"\"\"\n",
    "\n",
    "train_df = pull_out_dictionary(pd.DataFrame.from_records(datasets['train']))\n",
    "val_df = pull_out_dictionary(pd.DataFrame.from_records(datasets['validation']))\n",
    "\n",
    "train_df = pull_out_list(train_df)\n",
    "val_df = pull_out_list(val_df)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'context', 'question', 'id', 'document_id',\n",
      "       '__index_level_0__', 'answer_start', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>corpus_source</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>html</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>TODO</td>\n",
       "      <td>None</td>\n",
       "      <td>나라 목록</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이 목록에 실린 국가 기준은 1933년 몬테비데오 협약 1장을 참고로 하였다. 협정...</td>\n",
       "      <td>위키피디아</td>\n",
       "      <td>TODO</td>\n",
       "      <td>None</td>\n",
       "      <td>나라 목록</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text corpus_source   url  \\\n",
       "0  이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개...         위키피디아  TODO   \n",
       "1  이 목록에 실린 국가 기준은 1933년 몬테비데오 협약 1장을 참고로 하였다. 협정...         위키피디아  TODO   \n",
       "\n",
       "  domain  title author  html document_id  \n",
       "0   None  나라 목록   None  None           0  \n",
       "1   None  나라 목록   None  None           1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# load test dataset as dataframe\n",
    "with open(\"/opt/ml/data/wikipedia_documents.json\", \"r\", encoding=\"utf-8\") as reader:\n",
    "    input_data = json.load(reader)\n",
    "test_df = pd.DataFrame(input_data).T\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "Fixed: roberta not receiving sequence ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence id not used: klue/roberta-large\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "# load tokenizer and configuration according to the model (ex: klue/roberta-large)\n",
    "if \"roberta\" in model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, \n",
    "        model_input_names = [\"input_ids\", \"attention_mask\"],\n",
    "        use_fast=True # use rust based tokenizer    \n",
    "    )\n",
    "    print(\"sequence id not used:\", model_args.model_name_or_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "    print(model_args.model_name_or_path)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'현대 ##적 인사 ##조 ##직 ##관리 ##의 시발점 ##이 된 책 ##은 ?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample tokenization\n",
    "tokens = tokenizer.tokenize(train_dataset_from_huggingface[1]['question'])\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['크리스토', '##포', '알', '##하우스']\n",
      "Wrong Example: tensor([[21533,     7,     7,  1862,  1381,     7,     7,  6634]])\n",
      "Correctly Encoded: tensor([[21533,  2208,  1381, 17975]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# test batch tokenization\n",
    "# https://github.com/huggingface/transformers/issues/10297#issuecomment-783464293\n",
    "sample_answer_token = ['크리스토', '##포', '알', '##하우스']\n",
    "print(sample_answer_token)\n",
    "print(\"Wrong Example:\", tokenizer.encode(sample_answer_token, add_special_tokens=False, return_tensors='pt', is_split_into_words=True))\n",
    "# apply int for torch Tensor\n",
    "print(\"Correctly Encoded:\" ,torch.IntTensor([tokenizer.convert_tokens_to_ids(sample_answer_token)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3952, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '미국 상원', 'context': '미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국 의회의 상원이다.\\\\n\\\\n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1/3씩 상원의원을 새로 선출하여 연방에 보낸다.\\\\n\\\\n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할(하원의 법안을 거부할 권한 등)을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션(공공건강보험기관)의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다.날짜=2017-02-05', 'question': '대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?', 'id': 'mrc-1-000067', 'document_id': 18293, '__index_level_0__': 42, 'answer_start': 235, 'text': '하원'}\n",
      "dict_keys(['title', 'context', 'question', 'id', 'document_id', '__index_level_0__', 'answer_start', 'text'])\n",
      "3952\n"
     ]
    }
   ],
   "source": [
    "# change dataframe to dictionary\n",
    "train_df_dict = train_df.to_dict('records')\n",
    "valid_df_dict = val_df.to_dict('records')\n",
    "print(train_df_dict[0])\n",
    "print(train_df_dict[0].keys())\n",
    "print(len(train_df_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'klue/roberta-large',\n",
       " 'save_steps': 100,\n",
       " 'num_train_epochs': 3,\n",
       " 'learning_rate': 2e-05,\n",
       " 'batch_size': 10,\n",
       " 'warmup_steps': 300,\n",
       " 'weight_decay': 0.01,\n",
       " 'validation': False,\n",
       " 'max_length': 512,\n",
       " 'DEBUG': True,\n",
       " 'num_rnn_layers': 2,\n",
       " 'num_folds': 4,\n",
       " 'gamma': 1.0,\n",
       " 'smoothing': 0.2,\n",
       " 'doc_stride': 128}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def prepare_train_features(examples):\n",
    "    # 주어진 텍스트를 토크나이징 한다. 이 때 텍스트의 길이가 max_seq_length를 넘으면 stride만큼 슬라이딩하며 여러 개로 쪼갬.\n",
    "    # 즉, 하나의 example에서 일부분이 겹치는 여러 sequence(feature)가 생길 수 있음.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",  # max_seq_length까지 truncate한다. pair의 두번째 파트(context)만 잘라냄.\n",
    "        max_length=model_args.max_length,\n",
    "        stride=model_args.doc_stride,\n",
    "        return_overflowing_tokens=True, # 길이를 넘어가는 토큰들을 반환할 것인지\n",
    "        return_offsets_mapping=True,  # 각 토큰에 대해 (char_start, char_end) 정보를 반환한 것인지\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # example 하나가 여러 sequence에 대응하는 경우를 위해 매핑이 필요함.\n",
    "    overflow_to_sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # offset_mappings으로 토큰이 원본 context 내 몇번째 글자부터 몇번째 글자까지 해당하는지 알 수 있음.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 정답지를 만들기 위한 리스트\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        # 해당 example에 해당하는 sequence를 찾음.\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        \n",
    "        # sequence가 속하는 example을 찾는다.\n",
    "        example_index = overflow_to_sample_mapping[i]\n",
    "        answers = examples[\"answers\"][example_index]\n",
    "        \n",
    "        # 텍스트에서 answer의 시작점, 끝점\n",
    "        answer_start_offset = answers[\"answer_start\"][0]\n",
    "        answer_end_offset = answer_start_offset + len(answers[\"text\"][0])\n",
    "\n",
    "        # 텍스트에서 현재 span의 시작 토큰 인덱스\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "        \n",
    "        # 텍스트에서 현재 span 끝 토큰 인덱스\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        # answer가 현재 span을 벗어났는지 체크\n",
    "        if not (offsets[token_start_index][0] <= answer_start_offset and offsets[token_end_index][1] >= answer_end_offset):\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # token_start_index와 token_end_index를 answer의 시작점과 끝점으로 옮김\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start_offset:\n",
    "                token_start_index += 1\n",
    "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "            while offsets[token_end_index][1] >= answer_end_offset:\n",
    "                token_end_index -= 1\n",
    "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "prepare_train_features(train_dataset_from_huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'klue/roberta-large',\n",
       " 'save_steps': 100,\n",
       " 'num_train_epochs': 3,\n",
       " 'learning_rate': 2e-05,\n",
       " 'batch_size': 10,\n",
       " 'warmup_steps': 300,\n",
       " 'weight_decay': 0.01,\n",
       " 'validation': False,\n",
       " 'max_length': 384,\n",
       " 'DEBUG': True,\n",
       " 'num_folds': 3,\n",
       " 'question_max_seq_length': 100}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.batch_size = 10\n",
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 3698, 2069, 3954, 2470, 3666, 2079, 7895, 8586, 2207, 2069,  554,\n",
      "         2259, 3728, 3860, 2073,   35,    2,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-453772717193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_truncated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_multi_passage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_truncated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train_dataset = TensorDataset(train_truncated[0], train_truncated[1], train_truncated[2], train_truncated[3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-a498bcf1e06e>\u001b[0m in \u001b[0;36mmake_multi_passage\u001b[0;34m(dict_df)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_encoded_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_encoded_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mquestion_encoded_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_q_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0;31m# question_encoded_dict['token_type_ids'].append(tmp_q_seq['token_type_ids'][0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mquestion_encoded_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_q_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_truncated = make_multi_passage(train_df_dict)\n",
    "print(train_truncated[0])\n",
    "# train_dataset = TensorDataset(train_truncated[0], train_truncated[1], train_truncated[2], train_truncated[3])\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=model_args.batch_size, shuffle=True)\n",
    "\n",
    "# valid_truncated = make_multi_passage(valid_df_dict)\n",
    "# valid_dataset = TensorDataset(valid_truncated[0], valid_truncated[1], valid_truncated[2], valid_truncated[3])\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=model_args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3952, 8)\n",
      "3706\n",
      "(240, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(len(train_dataset))\n",
    "\n",
    "print(val_df.shape)\n",
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Mon Nov  1 07:10:27 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   34C    P0    36W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "model_args.DEBUG = False\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() and model_args.DEBUG == False else 'cpu')\n",
    "print(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForQuestionAnswering, AutoConfig\n",
    "\n",
    "# \"\"\" default model \"\"\"\n",
    "# model_config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "#     model_args.model_name_or_path,\n",
    "#     config=model_config,\n",
    "#     )\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForQuestionAnswering\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\"\"\" https://github.com/snexus/nlp-question-answering-system/blob/main/model.py \"\"\"\n",
    "\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self, MODEL_NAME, dropout_proba=0.1):\n",
    "        super().__init__()        \n",
    "        self.model_config= AutoConfig.from_pretrained(MODEL_NAME)\n",
    "        self.transformer = AutoModel.from_pretrained(MODEL_NAME, config= self.model_config)\n",
    "        self.embed_dim = self.model_config.hidden_size # roberta hidden dim = 1024\n",
    "        #self.device=\"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "        # We replace the head with lstm layer\n",
    "        self.qa_head = nn.Linear(in_features=self.embed_dim, out_features=2, bias=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_proba)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, start_positions, end_positions):\n",
    "        \"\"\"\n",
    "        Forward step for the question-answering model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_enc - encoding dictionary from the tokenizer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        start_logits - logit corresponding to the start position of the answer (batch_size, sentence_size, 1)\n",
    "        start_positions - true start position (batch_size, 1) or None\n",
    "        end_logits - logit corresponding to the end position of the answer (batch_size, sentence_size, 1)\n",
    "        end_positions - ture end position (batch_size, 1) or None\n",
    "        \"\"\"\n",
    "        #input_ids=input_ids.to(self.device)\n",
    "        #attention_mask=attention_mask.to(self.device)\n",
    "        trans_out = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract hidden state from the transformer\n",
    "        hidden_out = trans_out.last_hidden_state  # (batch_size, len_sentence, embed_dim)\n",
    "\n",
    "        \n",
    "\n",
    "        #cat_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "        #print(cat_hidden.shape)\n",
    "\n",
    "        # Pass through the linear layer, we need to learn it's parameters\n",
    "        out = self.qa_head(hidden_out)  # (batch_size, len_sentence, 2)\n",
    "        #print(\"out : \",out)\n",
    "        #print(\"out : \",out.shape)\n",
    "        start_logits, end_logits = out.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)  # (bs, max_query_len)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        # # Pass through classification\n",
    "        # out_cls = F.relu(self.class_layer(hidden_out)).permute(2, 0, 1)  # (1, batch_size, len_sentence)\n",
    "        # out_cls_logits = F.max_pool1d(out_cls, kernel_size=out_cls.shape[-1]).squeeze(-1).permute(1,\n",
    "        #                                                                                           0)  # (batch_size, 2)\n",
    "\n",
    "        # if start_positions is not None and end_positions is not None:\n",
    "        #     start_positions = torch.LongTensor(start_positions)\n",
    "        #     end_positions = torch.LongTensor(end_positions)\n",
    "            # start_positions = torch.tensor(start_positions, dtype=torch.long).reshape(-1, 1).squeeze(-1)\n",
    "            # end_positions = torch.tensor(end_positions, dtype=torch.long).reshape(-1, 1).squeeze(-1)  # (batch_size)\n",
    "\n",
    "            # is_impossible = torch.tensor(is_impossible, dtype=torch.long).detach().reshape(-1, 1).squeeze(\n",
    "            #     -1)  # (batch_size)\n",
    "\n",
    "            # ignored_index = start_logits.size(1)\n",
    "            # start_positions.clamp_(0, ignored_index)\n",
    "            # end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "        return {\n",
    "            \"start_logits\": start_logits, \n",
    "            \"start_positions\": start_positions,\n",
    "            \"end_logits\": end_logits,\n",
    "            \"end_positions\": end_positions\n",
    "            }  # , ignored_index\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, path: str, epoch: int, train_iter: float, optimizer, train_loss: float, eval_loss: float):\n",
    "        \"\"\"\n",
    "        Persist model on disk.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'eval_loss': eval_loss,\n",
    "            'train_iter': train_iter\n",
    "        }, path)\n",
    "\n",
    "    def compute_loss(self, start_logits, start_positions, end_logits, end_positions, return_outputs=False):\n",
    "        #print(start_logits.size(), start_positions.size())\n",
    "        if len(start_positions.size()) > 1:\n",
    "            start_positions = start_positions.squeeze(-1)\n",
    "        if len(end_positions.size()) > 1:\n",
    "            end_positions = end_positions.squeeze(-1)\n",
    "        \n",
    "        # train의 prepare_train_features함수에서 index안에 정답이 없으면 cls토큰을 정답위치로 해주기 떄문에\n",
    "        # 주석부분은 사용하지 않습니다!\n",
    "        # ignored_index = start_logits.size(1)\n",
    "        # start_positions = start_positions.clamp(0, ignored_index)\n",
    "        # end_positions = end_positions.clamp(0, ignored_index)\n",
    "        # loss_fct = FocalLoss(gamma=2.0) # 원하는 loss를 사용하면 됩니다\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        #start_logit_g = start_logits.to(self.device, non_blocking=True)\n",
    "        #end_logit_g = end_logits.to(self.device, non_blocking=True)\n",
    "        #start_pos_g = start_positions.to(self.device, non_blocking=True)\n",
    "        #end_pos_g = end_positions.to(self.device, non_blocking=True)\n",
    "        #print(start_logits.size(), start_positions.size())\n",
    "        #print(start_logits, start_positions)\n",
    "        start_loss = loss_fct(start_logits, start_positions)\n",
    "        end_loss = loss_fct(end_logits, end_positions)\n",
    "        total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "\n",
    "# from transformers import DistilBertModel\n",
    "# from preprocess import SquadPreprocessor\n",
    "#\n",
    "# sp = SquadPreprocessor()\n",
    "# train_enc, val_enc = sp.get_encodings(random_sample_train=0.0005, random_sample_val=0.1, return_tensors=\"pt\")\n",
    "#\n",
    "# # Decoding\n",
    "# #    print(sp.tokenizer.decode(train_enc['input_ids'][0]))\n",
    "#\n",
    "# dbm = DistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "# model = QAModel(transformer_model=dbm, device=torch.device(\"cpu\"))\n",
    "#\n",
    "# start_logit, start_pos_, end_logits, end_pos_, out_cls_logits, is_impossible = model(train_enc)\n",
    "# loss_fun_ = nn.CrossEntropyLoss()\n",
    "# print(model.compute_loss(start_logit, start_pos_, end_logits, end_pos_, out_cls_logits))\n",
    "\n",
    "model = QAModel(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  1 07:10:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   34C    P0    36W / 250W |   2487MiB / 32510MiB |     12%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamp import AdamP\n",
    "\n",
    "optimizer = AdamP(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = model_args.num_train_epochs\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msnoop2head\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/happyface-boostcamp/KLUE-QA/runs/u8f6lvqa\" target=\"_blank\">custom-trainer-CE</a></strong> to <a href=\"https://wandb.ai/happyface-boostcamp/KLUE-QA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training 371 batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3727c1b9c8554ffdb690b9d0b7dc42c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=371.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 Step: 100 Train Loss: 1.7256\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384faac24c6a4819ba6a687e75320d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/3 Step: 100 Dev Loss: 0.6757 Dev Acc: 0.7591\n",
      "Saved model with lowest validation loss: 0.6757\n",
      "Epoch: 1/3 Step: 200 Train Loss: 0.5113\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfe5b25a5554fc79357ffccbecf4bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/3 Step: 200 Dev Loss: 0.6066 Dev Acc: 0.7636\n",
      "Saved model with lowest validation loss: 0.6066\n",
      "Epoch: 1/3 Step: 300 Train Loss: 0.4050\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2ed2ceacf840a0b3270b2991b7e97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/3 Step: 300 Dev Loss: 0.6048 Dev Acc: 0.7682\n",
      "Saved model with lowest validation loss: 0.6048\n",
      "\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training 371 batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29302c430ad347c7b590389808bdf9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=371.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/3 Step: 100 Train Loss: 0.2437\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d977b40c014485b832ecc01cb1d1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/3 Step: 100 Dev Loss: 0.6749 Dev Acc: 0.7682\n",
      "Epoch: 2/3 Step: 200 Train Loss: 0.1601\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8592994c63a3404788b64fcb7daeb0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/3 Step: 200 Dev Loss: 0.6609 Dev Acc: 0.7682\n",
      "Epoch: 2/3 Step: 300 Train Loss: 0.1558\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da02b3d2ba1540d686dd665be0b0ab82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/3 Step: 300 Dev Loss: 0.6499 Dev Acc: 0.8091\n",
      "\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training 371 batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e053a11571d49e4ad30ed3e679076fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=371.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/3 Step: 100 Train Loss: 0.0819\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea5a9aa0c7441bdaa1b57e5f992db9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3/3 Step: 100 Dev Loss: 0.8787 Dev Acc: 0.8045\n",
      "Epoch: 3/3 Step: 200 Train Loss: 0.0399\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744935b53b50465ba1a81df5d1d5562a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3/3 Step: 200 Dev Loss: 0.8426 Dev Acc: 0.8136\n",
      "Epoch: 3/3 Step: 300 Train Loss: 0.0259\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4511807a4caa43ac871ef4bd99fc81aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3/3 Step: 300 Dev Loss: 0.8566 Dev Acc: 0.8045\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import wandb \n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "# https://wandb.ai/happyface-boostcamp/KLUE-QA\n",
    "wandb.init(\n",
    "    project='KLUE-QA', \n",
    "    entity='happyface-boostcamp', \n",
    "    name='custom-trainer-CE', \n",
    "    config=model_args\n",
    "    )\n",
    "\n",
    "\n",
    "fold_num = 0\n",
    "best_eval_loss = 1.0\n",
    "\n",
    "# Initialize using Metrics \n",
    "train_acc, train_loss = Metrics(), Metrics()\n",
    "dev_acc, dev_loss = Metrics(), Metrics()\n",
    "dev_start_acc, dev_end_acc = Metrics(), Metrics()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))    \n",
    "    print('Training {:,} batches...'.format(len(train_dataloader)))\n",
    "\n",
    "    # change the model to train mode\n",
    "    model.train()\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        # get info from batch\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_start_pos = batch[2].to(device)\n",
    "        b_end_pos = batch[3].to(device)\n",
    "\n",
    "        # get outputs from the model\n",
    "        model.zero_grad()        \n",
    "        outputs = model(\n",
    "            b_input_ids, \n",
    "            attention_mask=b_input_mask, \n",
    "            start_positions=b_start_pos,\n",
    "            end_positions=b_end_pos,\n",
    "        )\n",
    "\n",
    "        # get logits(probability) and loss\n",
    "        start_logits = outputs['start_logits']\n",
    "        end_logits = outputs['end_logits']\n",
    "        loss = model.compute_loss(\n",
    "            start_logits = start_logits,\n",
    "            start_positions = outputs['start_positions'],\n",
    "            end_logits = end_logits,\n",
    "            end_positions = outputs['end_positions'],\n",
    "        )\n",
    "        \n",
    "        # record train loss\n",
    "        # print(loss.item())\n",
    "        train_loss.update(loss.item(), len(b_input_ids))\n",
    "\n",
    "        # backward propagation & optimizer stepping\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # EVALUATE every 100 steps\n",
    "        if step != 0 and step % 100 == 0:\n",
    "            print('Epoch: {}/{}'.format(epoch_i+1, model_args.num_train_epochs), 'Step: {}'.format(step), 'Train Loss: {:.4f}'.format(train_loss.avg))\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            # initialize prediction and labels for future accuracy calculation\n",
    "            pred_start, pred_end, true_start, true_end = [], [], [], []\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in tqdm(valid_dataloader):\n",
    "                \n",
    "                # get info from batch\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_input_mask = batch[1].to(device)\n",
    "                b_start_pos = batch[2].to(device)\n",
    "                b_end_pos = batch[3].to(device)\n",
    "\n",
    "                # Evaluate and get outputs\n",
    "                model.eval()\n",
    "                with torch.no_grad():        \n",
    "                    outputs = model(\n",
    "                        b_input_ids, \n",
    "                        # token_type_ids=b_seg_ids, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        start_positions=b_start_pos,\n",
    "                        end_positions=b_end_pos,\n",
    "                    )\n",
    "\n",
    "                # get logits(probability) and loss\n",
    "                start_logits = outputs['start_logits']\n",
    "                end_logits = outputs['end_logits']        \n",
    "                loss = model.compute_loss(\n",
    "                    start_logits = start_logits,\n",
    "                    start_positions = outputs['start_positions'],\n",
    "                    end_logits = end_logits,\n",
    "                    end_positions = outputs['end_positions'],\n",
    "                )\n",
    "\n",
    "                # record evaluation loss\n",
    "                dev_loss.update(loss.item(), len(b_input_ids))\n",
    "\n",
    "                # get logits and predictions for evaluation accuracy\n",
    "                start_logits = start_logits.detach().cpu().numpy()\n",
    "                end_logits = end_logits.detach().cpu().numpy()                \n",
    "                b_start_pos = b_start_pos.to('cpu').numpy()\n",
    "                b_end_pos = b_end_pos.to('cpu').numpy()\n",
    "                answer_start = np.argmax(start_logits, axis=1)\n",
    "                answer_end = np.argmax(end_logits, axis=1)\n",
    "\n",
    "                # append the prediction and ground truth to the list for comparison\n",
    "                pred_start.append(answer_start)\n",
    "                pred_end.append(answer_end)\n",
    "                true_start.append(b_start_pos)\n",
    "                true_end.append(b_end_pos)\n",
    "\n",
    "            # compare start for accuracy calculation\n",
    "            pred_start = np.concatenate(pred_start, axis=0)\n",
    "            true_start = np.concatenate(true_start, axis=0)\n",
    "            num_start_correct = np.sum(pred_start == true_start)\n",
    "            dev_start_acc.update((pred_start == true_start).mean(), len(true_start))\n",
    "            \n",
    "            # compare end for accuracy calculation\n",
    "            pred_end = np.concatenate(pred_end, axis=0)\n",
    "            true_end = np.concatenate(true_end, axis=0)\n",
    "            num_end_correct = np.sum(pred_end == true_end)\n",
    "            dev_end_acc.update((pred_end == true_end).mean(), len(true_end))\n",
    "            \n",
    "            # compare both start and end for EM accuracy calculation\n",
    "            total_correct = num_start_correct + num_end_correct\n",
    "            total_indeces = len(true_start) + len(true_end)\n",
    "            \n",
    "            # get both cases where pred_start == true_start and pred_end == true_end\n",
    "            both_correct = np.mean(np.logical_and(pred_start == true_start, pred_end == true_end))\n",
    "            dev_acc.update(both_correct, len(b_input_ids))\n",
    "\n",
    "            # Report the final accuracy for this validation run.\n",
    "            print('Epoch: {}/{}'.format(epoch_i+1, epochs), 'Step: {}'.format(step), 'Dev Loss: {:.4f}'.format(dev_loss.avg), 'Dev Acc: {:.4f}'.format(dev_acc.avg))\n",
    "            \n",
    "            # log on wandb\n",
    "            wandb.log(\n",
    "                    {\n",
    "                        'train/loss': train_loss.avg,\n",
    "                        'train/learning_rate':optimizer.param_groups[0]['lr'], \n",
    "                        'eval/loss': dev_loss.avg,\n",
    "                        'eval/accuracy':dev_acc.avg,\n",
    "                        'eval/start_accuracy':dev_start_acc.avg,\n",
    "                        'eval/end_accuracy':dev_end_acc.avg,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # save the best model based on evaluation loss\n",
    "            if best_eval_loss > dev_loss.avg:\n",
    "                best_eval_loss = dev_loss.avg\n",
    "                torch.save(model.state_dict(), './results/{}-fold-{}-best-eval-loss-model.pt'.format(fold_num+1, model_args.num_folds))\n",
    "                print('Saved model with lowest validation loss: {:.4f}'.format(best_eval_loss))\n",
    "                wandb.log({'best_eval_loss':best_eval_loss})\n",
    "\n",
    "            # reset metrics\n",
    "            train_loss.reset()\n",
    "            dev_loss.reset()\n",
    "            dev_acc.reset()\n",
    "            dev_start_acc.reset()\n",
    "            dev_end_acc.reset()\n",
    "            \n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datasets = load_from_disk(data_args.test_dataset_name)\n",
    "test_dataset =test_datasets['validation']\n",
    "\n",
    "# 240 public questions + 360 private questions = 600 total questions\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"유령'은 어느 행성에서 지구로 왔는가?\", 'id': 'mrc-1-000653'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
