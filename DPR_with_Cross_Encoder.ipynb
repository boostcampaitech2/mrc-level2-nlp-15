{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, RobertaModel,\n",
    "    BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/data/train_dataset')\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "corpus = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ")  # set 은 매번 순서가 바뀌므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/opt/ml/custom/passage_embedding_special_shuffle_10.bin', 'rb') as file :\n",
    "    p_embs = pickle.load(file)\n",
    "p_embs = p_embs\n",
    "\n",
    "q_encoder = torch.load('/opt/ml/custom/q_encoder_special_shuffle_10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f5265bc68c44f9bb6b19d786fe160f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# Bi-Encoder Retrieval 정확도 출력\n",
    "a_total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        a_total.append(tmp)\n",
    "\n",
    "b_cqas_50 = pd.DataFrame(a_total)\n",
    "correct_length = []\n",
    "for i in range(len(b_cqas_50)) :\n",
    "    if b_cqas_50['original_context'][i] in b_cqas_50['context'][i] :\n",
    "        correct_length.append(i)\n",
    "print(len(correct_length) / len(dataset['validation']))\n",
    "# 출력결과 0.916666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross_Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "        classifier_dropout=(\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(classifier_dropout)\n",
    "        self.linear = torch.nn.Linear(config.hidden_size, 1)\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        output = self.linear(pooled_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_encoder = torch.load('/opt/ml/custom/c_encoder_e5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data의 경우 약 40분 소요\n",
    "question_data = dataset['validation']['question']\n",
    "with torch.no_grad() : \n",
    "    c_encoder.eval()\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in tqdm(range(len(question_data))) :\n",
    "        question = question_data[i]\n",
    "        question_score = []\n",
    "        for indice in tqdm(doc_indices[i]) :\n",
    "            passage = corpus[indice]\n",
    "            tokenized_examples = tokenizer(\n",
    "                question,\n",
    "                passage,\n",
    "                truncation=\"only_second\",\n",
    "                max_length=512,\n",
    "                stride=128,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                #return_token_type_ids=False,  # roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\n",
    "                padding=\"max_length\",\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            score = 0\n",
    "            for i in range(len(tokenized_examples['input_ids'])) :\n",
    "                c_input = {\n",
    "                    'input_ids' : torch.tensor(tokenized_examples['input_ids'][i].unsqueeze(dim=0)).to('cuda'),\n",
    "                    'attention_mask' : torch.tensor(tokenized_examples['attention_mask'][i].unsqueeze(dim=0)).to('cuda'),\n",
    "                    'token_type_ids' : torch.tensor(tokenized_examples['token_type_ids'][i].unsqueeze(dim=0)).to('cuda')\n",
    "                }\n",
    "                tmp_score = c_encoder(**c_input).to('cpu')\n",
    "                score += tmp_score\n",
    "            score = score / len(tokenized_examples['input_ids'])\n",
    "            question_score.append(score)\n",
    "        sort_result = torch.sort(torch.tensor(question_score), descending=True)\n",
    "        scores, index_list = sort_result[0], sort_result[1]\n",
    "\n",
    "        result_scores.append(scores.tolist())\n",
    "        result_indices.append(index_list.tolist())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_indices = []\n",
    "for i in range(len(doc_indices)) :\n",
    "    t_list = [doc_indices[i][result_indices[i][k]] for k in range(50)]\n",
    "    final_indices.append(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Encoder 정확도 출력\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": final_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in final_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_50 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력결과 0.8708\n",
    "correct_length = []\n",
    "for i in range(len(cqas_50)) :\n",
    "    if cqas_50['original_context'][i] in cqas_50['context'][i] :\n",
    "        correct_length.append(i)\n",
    "print(len(correct_length) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqas_50.to_csv('b16_special_shuffle_e10_t500_ce_t50.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
