{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, RobertaModel,\n",
    "    BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamp import AdamP\n",
    "class DenseRetrieval:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        p_encoder,\n",
    "        q_encoder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        학습과 추론에 사용될 여러 셋업을 마쳐봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "\n",
    "    def train(self, args=None, tokenizer = None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        if tokenizer is None :\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        q_seqs = tokenizer(self.dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        p_seqs = tokenizer(self.dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "        train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=args.per_device_train_batch_size)\n",
    "\n",
    "        no_decay = [\"bias\" ,\"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamP(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            # eps=args.adam_epsilon\n",
    "        )\n",
    "\n",
    "        # t_total = len(train_dataloader) * args.num_train_epochs\n",
    "        # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "        \n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "        for epoch, _ in enumerate(train_iterator):\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            # loss_value=0 # Accumulation할 때 진행\n",
    "            losses = 0\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.q_encoder.train()\n",
    "                self.p_encoder.train()\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "                p_inputs = {'input_ids': batch[0],\n",
    "                            'attention_mask': batch[1],\n",
    "                            'token_type_ids': batch[2]\n",
    "                            }\n",
    "                \n",
    "                q_inputs = {'input_ids': batch[3],\n",
    "                            'attention_mask': batch[4],\n",
    "                            'token_type_ids': batch[5]}\n",
    "            \n",
    "                p_outputs = self.p_encoder(**p_inputs)  # (batch_size, emb_dim)\n",
    "                q_outputs = self.q_encoder(**q_inputs)  # (batch_size, emb_dim)\n",
    "\n",
    "                # Calculate similarity score & loss\n",
    "                sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "\n",
    "                # target: position of positive samples = diagonal element \n",
    "                targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "                if torch.cuda.is_available():\n",
    "                    targets = targets.to('cuda')\n",
    "\n",
    "                # almost same as cross entropy loss\n",
    "                sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "                loss = F.nll_loss(sim_scores, targets)\n",
    "\n",
    "                losses += loss.item()\n",
    "                if step % 100 == 0 :\n",
    "                    print(f'{epoch}epoch loss: {losses/(step+1)}') # Accumulation할 경우 주석처리\n",
    "\n",
    "                loss.backward()\n",
    "                #################ACCUMULATION###############################\n",
    "                # loss_value += loss\n",
    "                # if (step+1) % args.gradient_accumulation_steps == 0 :\n",
    "                #     optimizer.step()\n",
    "                #     scheduler.step()\n",
    "                #     self.q_encoder.zero_grad()\n",
    "                #     self.p_encoder.zero_grad()\n",
    "                #     global_step += 1\n",
    "                #     print(loss_value/args.gradient_accumulation_steps)\n",
    "                #     loss_value = 0\n",
    "                ############################################################\n",
    "                optimizer.step()\n",
    "                # scheduler.step()\n",
    "                self.q_encoder.zero_grad()\n",
    "                self.p_encoder.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                del p_inputs, q_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[\n",
    "            \"pooler_output\"\n",
    "        ]  # [CLS] token's hidden featrues(hidden state)\n",
    "\n",
    "        # pooled_output = outputs[1] # TODO: check if it is \n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/data/train_dataset')\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5, # recommended learning rate is 1e-5\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c269a9b0359439e96cae9e5ac354560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=247.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0epoch loss: 45.3947868347168\n",
      "0epoch loss: 2.176331270301696\n",
      "0epoch loss: 1.360393577945692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [05:57<00:00, 357.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retriever는 아래와 같이 사용할 수 있도록 코드를 짜봅시다.\n",
    "retriever = DenseRetrieval(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder\n",
    ")\n",
    "retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "corpus = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ")  # set 은 매번 순서가 바뀌므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fb5c771c784c9c9e88428afbec9db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p_encoder = retriever.p_encoder\n",
    "q_encoder = retriever.q_encoder\n",
    "with torch.no_grad() :\n",
    "    p_encoder.eval()\n",
    "\n",
    "    p_embs = []\n",
    "    for p in tqdm(corpus[:100]) :\n",
    "        p = tokenizer([p], padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "        p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "        p_embs.append(p_emb)\n",
    "p_embs = torch.Tensor(p_embs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get cosine similarity between two arrays\n",
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# p_embs[0][0] == p_embs[1][0]\n",
    "\n",
    "cos_sim(p_embs[5][0], p_embs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7851726 , -0.48778328,  0.82468414, -0.9994512 ,  0.66243756,\n",
       "         0.56024635, -0.99997944,  0.07688553,  0.9984877 , -0.9993106 ,\n",
       "         0.01102075,  0.9790746 , -0.53845143, -0.3455528 ,  0.44703203,\n",
       "        -0.99937475, -0.9993723 , -0.81930596,  0.7370982 ,  0.63658166,\n",
       "         0.4322106 , -0.99978834, -0.7025193 ,  0.889094  , -0.85656047,\n",
       "        -0.7376723 , -0.99999547, -0.99673414, -0.97281915, -0.99313813,\n",
       "        -0.9802016 , -0.9999815 , -0.39314282,  0.99996483,  0.29816154,\n",
       "        -0.88221055, -0.9993746 , -0.9999418 ,  0.86694795, -0.69002855,\n",
       "         0.8707746 , -0.7348956 , -0.9999805 ,  0.9999823 ,  0.99774224,\n",
       "        -0.9352857 , -0.99852043, -0.40910897,  0.8847097 , -0.48227432,\n",
       "         0.9998008 , -0.39056444,  0.87851715,  0.503014  ,  0.88798594,\n",
       "         0.35612586,  0.9643091 , -0.99999905, -0.01133974, -0.97378886,\n",
       "        -0.7819594 , -0.06617206,  0.98293704, -0.94270927, -0.7929846 ,\n",
       "        -0.15542118, -0.44325835, -0.87392616,  0.99999917,  0.20320116,\n",
       "        -0.8617273 ,  0.6514844 , -0.44081095, -0.6983104 ,  0.63489175,\n",
       "        -0.9236891 ,  0.946073  ,  0.99924654,  0.3659683 ,  0.9561413 ,\n",
       "         0.9168216 , -0.2435571 , -0.92090523,  0.89760584, -0.99230707,\n",
       "        -0.9999825 ,  0.8935505 ,  0.9999988 , -0.8353169 ,  0.998575  ,\n",
       "        -0.9998456 , -0.88488716, -0.90301925,  0.5969039 , -0.96243876,\n",
       "        -0.6529742 ,  0.68466973,  0.9992789 ,  0.999748  ,  0.9208325 ,\n",
       "        -0.8213775 , -0.93430436, -0.9336945 , -0.5584358 , -0.819348  ,\n",
       "         0.8974032 ,  0.9999993 ,  0.23579526, -0.4073066 ,  0.08050998,\n",
       "        -0.46452412, -0.92526615, -0.6129174 , -0.5565939 ,  0.9977268 ,\n",
       "        -0.6456061 , -0.99541634, -0.35581622,  0.42637694, -0.03772092,\n",
       "        -0.9633899 , -0.9999493 , -0.99997735, -0.7317497 , -0.23160341,\n",
       "        -0.7748618 ,  0.93983984,  0.99882895, -0.97078496, -0.5698359 ,\n",
       "         0.69422394,  0.28749076, -0.9998914 , -0.9999992 ,  0.6504617 ,\n",
       "         0.71046805, -0.88965523,  0.9998496 ,  0.998449  , -0.7905421 ,\n",
       "         0.9993574 ,  0.9890598 , -0.24499933,  0.8818592 , -0.66293997,\n",
       "         0.94319785, -0.20715536,  0.5838899 , -0.96649736,  0.93552446,\n",
       "        -0.99998987, -0.68810534,  0.7094059 ,  0.9998242 , -0.6589418 ,\n",
       "         0.8281401 ,  0.6160947 ,  0.15684924, -0.9376056 , -0.64783245,\n",
       "         0.6966919 , -0.99986845,  0.7099118 ,  0.46532688,  0.6756276 ,\n",
       "         0.9998959 ,  0.8188011 , -0.7353265 ,  0.8635426 ,  0.6985699 ,\n",
       "        -0.9891897 ,  0.9869938 , -0.89298725,  0.82750374,  0.9295433 ,\n",
       "        -0.9999443 , -0.93436223,  0.53761894,  0.55796957, -0.2895236 ,\n",
       "        -0.7919208 ,  0.85837126, -0.99082035,  0.9387776 , -0.6243964 ,\n",
       "         0.84154904,  0.56186783, -0.9968604 , -0.44997987,  0.8546604 ,\n",
       "         0.99834865, -0.7671621 ,  0.97997165, -0.99998623,  0.9638749 ,\n",
       "         0.7587526 ,  0.99971116, -0.95380414, -0.13738853, -0.9208782 ,\n",
       "         0.25660273, -0.40004504,  0.9974662 ,  0.99923927,  0.98976684,\n",
       "         0.99865836,  0.99999434,  0.21384895,  0.79243135, -0.7041829 ,\n",
       "         0.8970136 , -0.9998091 , -0.82218224, -0.8845689 , -0.9392202 ,\n",
       "         0.7113412 , -0.17707299,  0.99999845,  0.87761116, -0.99828404,\n",
       "        -0.99999106, -0.92548484, -0.6142845 , -0.7342162 , -0.54996747,\n",
       "         0.9041133 ,  0.72561115, -0.5493321 , -0.9978464 , -0.888661  ,\n",
       "         0.63544786, -0.99197066, -0.810079  , -0.68211913, -0.9999996 ,\n",
       "        -0.978163  , -0.38966864,  0.8810483 , -0.8759535 , -0.99957544,\n",
       "         0.6380005 ,  0.32580185, -0.9998287 , -0.8926232 , -0.99426514,\n",
       "        -0.75091654,  0.1739987 ,  0.9313638 ,  0.18644881,  0.37869585,\n",
       "        -0.99996084, -0.997142  ,  0.6943055 , -0.65148705,  0.9999992 ,\n",
       "         0.5268227 ,  0.56546986,  0.99966615,  0.32176566,  0.95580333,\n",
       "        -0.32296258,  0.69564986, -0.9999327 , -0.80318445,  0.611079  ,\n",
       "        -0.9150789 ,  0.7485583 ,  0.89996284, -0.9982414 , -0.99987954,\n",
       "         0.83157617,  0.81070894, -0.67816687, -0.6833421 ,  0.291561  ,\n",
       "        -0.9051686 ,  0.19951133, -0.9999959 ,  0.88665193,  0.6961247 ,\n",
       "         0.67545116,  0.94252735, -0.10638047, -0.91703355, -0.7410895 ,\n",
       "         0.8420048 , -0.5359323 , -0.9026302 , -0.99985474, -0.99822927,\n",
       "         0.96094465,  0.99776936,  0.9088019 , -0.50696445, -0.7147651 ,\n",
       "        -0.62536824,  0.9388449 , -0.70859313, -0.60692716,  0.8840278 ,\n",
       "         0.68454593, -0.8163706 ,  0.9999965 , -0.91044784, -0.95680594,\n",
       "        -0.4127099 , -0.9854375 , -0.99593014, -0.09330096,  0.90684867,\n",
       "         0.44321203, -0.57058585,  0.9999944 ,  0.9645992 ,  0.77936745,\n",
       "        -0.6592349 , -0.9999878 ,  0.7010747 ,  0.39583433, -0.9136329 ,\n",
       "        -0.37679476,  0.99942476,  0.9989594 , -0.9703538 ,  0.02691686,\n",
       "        -0.99999577,  0.9859238 ,  0.9999806 ,  0.09109101,  0.22751276,\n",
       "         0.93677014,  0.72012377,  0.81882256,  0.20971686, -0.8752592 ,\n",
       "         0.9611304 , -0.67826486,  0.33007142, -0.9999997 , -0.58371234,\n",
       "         0.83451515, -0.22783306, -0.04455653,  0.71882474,  0.88729525,\n",
       "        -0.9999352 ,  0.55793047, -0.22464989, -0.2942222 , -0.9184101 ,\n",
       "        -0.99977684,  0.95715994, -0.15127939,  0.9999546 , -0.92833555,\n",
       "         0.2549073 , -0.4371733 , -0.9969367 , -0.9998061 , -0.8329995 ,\n",
       "         0.8401893 , -0.9536476 ,  0.16708769, -0.43587098, -0.63183147,\n",
       "        -0.97654253,  0.94629294, -0.8976641 , -0.98637396,  0.08952866,\n",
       "        -0.47445077, -0.8118547 , -0.9616445 ,  0.8909842 , -0.33526966,\n",
       "         0.92520905, -0.9966717 , -0.9999948 , -0.99976367, -0.67801   ,\n",
       "         0.9997686 , -0.72393656,  0.00941291, -0.7688765 , -0.4770523 ,\n",
       "        -0.8146454 ,  0.07297435,  0.85675144,  0.9635414 , -0.5565269 ,\n",
       "         0.6110054 , -0.7645979 ,  0.6941127 , -0.8604288 ,  0.85125107,\n",
       "         0.80131245,  0.4020215 , -0.9999977 , -0.79123366,  0.9914059 ,\n",
       "        -0.9184562 , -0.70985734,  0.28609857, -0.7867724 , -0.99899316,\n",
       "         0.91012585,  0.9999991 , -0.8731847 ,  0.44420335, -0.6079235 ,\n",
       "        -0.99916   ,  0.88405186, -0.6074472 ,  0.19008002, -0.97574615,\n",
       "        -0.02935634,  0.74507916, -0.788479  , -0.39490235,  0.9998928 ,\n",
       "        -0.8053302 , -0.99996907, -0.7796168 , -0.99980587,  0.9999992 ,\n",
       "         0.9541892 , -0.57620245,  0.94240606, -0.8334964 ,  0.42625555,\n",
       "        -0.9504423 , -0.99998415,  0.9998535 , -0.78535867,  0.247849  ,\n",
       "         0.98593086,  0.9892172 , -0.21958514, -0.76675904, -0.11128621,\n",
       "        -0.36619312,  0.39776388, -0.8803244 , -0.41752353, -0.07679035,\n",
       "        -0.99673235,  0.61797094, -0.83114326, -0.5085628 ,  0.9830829 ,\n",
       "        -0.99976724,  0.9508848 , -0.9788997 ,  0.93035036, -0.91445774,\n",
       "         0.90827185,  0.79693484, -0.884991  , -0.2702782 ,  0.12385403,\n",
       "         0.01390005, -0.961799  , -0.9998619 ,  0.8411514 , -0.23228598,\n",
       "         0.99287134, -0.9999541 ,  0.98428845,  0.672269  ,  0.9991919 ,\n",
       "         0.4333686 ,  0.95275915,  0.76580036, -0.5423058 , -0.9578858 ,\n",
       "        -0.5181132 , -0.6906678 , -0.999999  , -0.85002893,  0.97121066,\n",
       "         0.8983918 , -0.8336784 , -0.21403337,  0.35970142,  0.95073265,\n",
       "         0.95864403,  0.43642604, -0.8672305 ,  0.81972235,  0.8237792 ,\n",
       "        -0.30732337,  0.7376238 , -0.6085082 , -0.8153346 , -0.9401805 ,\n",
       "        -0.7758902 , -0.7193838 ,  0.4860393 , -0.36543596, -0.9498462 ,\n",
       "        -0.51140577, -0.8455787 , -0.9430685 , -0.739365  ,  0.91048336,\n",
       "        -0.1058805 ,  0.9275382 ,  0.42056572, -0.6609839 , -0.5282337 ,\n",
       "         0.9150968 , -0.9999183 , -0.99529326,  0.73336065, -0.9806728 ,\n",
       "        -0.73131144, -0.06322557,  0.88802874, -0.9991764 , -0.51240885,\n",
       "        -0.8252618 ,  0.1717154 ,  0.07527237, -0.7638853 , -0.99999547,\n",
       "        -0.21787061,  0.8605888 ,  0.6067829 ,  0.99999875,  0.9991469 ,\n",
       "         0.98586303, -0.38366926, -0.71643686, -0.40092865,  0.97085744,\n",
       "         0.6045237 , -0.9998821 , -0.7809758 ,  0.84473854,  0.7101075 ,\n",
       "        -0.08972096,  0.63736296,  0.9627662 ,  0.9470259 ,  0.96694237,\n",
       "        -0.9012301 , -0.9999814 , -0.8486767 ,  0.99921   ,  0.94542885,\n",
       "         0.73325276, -0.8350904 ,  0.04962721,  0.90289426,  0.9997626 ,\n",
       "         0.983113  ,  0.61359787, -0.5576824 , -0.15881859,  0.9997423 ,\n",
       "        -0.20930795, -0.34232748, -0.9502813 ,  0.9998411 ,  0.8649632 ,\n",
       "         0.50749004,  0.9791551 ,  0.2354404 , -0.9999752 ,  0.902705  ,\n",
       "         0.43062222, -0.94591415,  0.10732172,  0.29356474, -0.9879759 ,\n",
       "        -0.9923914 , -0.99997556, -0.8598403 ,  0.61365616, -0.7801042 ,\n",
       "        -0.9744327 , -0.1908041 ,  0.91285414,  0.57437503,  0.1112273 ,\n",
       "         0.10564231, -0.44347784, -0.31852216,  0.23914501, -0.7010697 ,\n",
       "         0.16459411, -0.596174  , -0.82366073, -0.9430858 , -0.9999754 ,\n",
       "        -0.6108675 , -0.5590675 ,  0.99883115,  0.6283611 ,  0.06089667,\n",
       "         0.90346354,  0.82584643, -0.20279112, -0.3421873 ,  0.15466781,\n",
       "        -0.6198044 ,  0.851523  , -0.11250128,  0.94827867, -0.9998556 ,\n",
       "        -0.97681737,  0.33921018,  0.5751831 , -0.01472591, -0.9699449 ,\n",
       "         0.89025086, -0.7128557 ,  0.03165732, -0.7063872 ,  0.6297971 ,\n",
       "         0.94315857, -0.5623835 , -0.99939233, -0.9871232 ,  0.883807  ,\n",
       "         0.9124913 ,  0.9999857 ,  0.9987454 , -0.57046336,  0.43656623,\n",
       "        -0.88116556,  0.99999315,  0.9559371 , -0.6252369 , -0.9929718 ,\n",
       "        -0.999897  , -0.68460095,  0.9548343 ,  0.9999445 , -0.9202367 ,\n",
       "        -0.9888461 ,  0.71985596, -0.667628  , -0.5536417 , -0.68485296,\n",
       "         0.9696095 ,  0.9966135 ,  0.9999854 ,  0.29818186,  0.2132955 ,\n",
       "         0.8727604 , -0.9725437 , -0.999994  , -0.99344844, -0.99999803,\n",
       "         0.9999989 , -0.7761629 ,  0.99955523, -0.9264967 ,  0.09916598,\n",
       "        -0.7742772 , -0.9998305 , -0.12887552, -0.8555009 , -0.01686588,\n",
       "         0.39646345, -0.9963415 , -0.21501862, -0.17453654, -0.5738604 ,\n",
       "         0.05256027, -0.9997993 ,  0.9998129 , -0.7037362 , -0.7142981 ,\n",
       "         0.9949154 ,  0.8262957 ,  0.9911495 ,  0.99998575,  0.9999203 ,\n",
       "         0.33895487,  0.99999917, -0.7559322 ,  0.7779451 , -0.7839156 ,\n",
       "        -0.19767311,  0.99577034,  0.15432478,  0.6444205 , -0.9597081 ,\n",
       "         0.2461934 , -0.5248537 , -0.2857403 , -0.5527047 , -0.99803656,\n",
       "        -0.8103072 , -0.72785896,  0.66373503, -0.9999133 ,  0.9999885 ,\n",
       "         0.6280385 ,  0.40685287, -0.76210046,  0.44010028, -0.915642  ,\n",
       "         0.9954059 ,  0.7723381 ,  0.07016578,  0.99954647, -0.5852214 ,\n",
       "         0.9999667 ,  0.8701352 ,  0.17557548, -0.04389553, -0.99824363,\n",
       "        -0.99999595,  0.93252933, -0.9802473 , -0.7312949 , -0.83570516,\n",
       "        -0.18656738,  0.9999961 ,  0.7018796 ,  0.99998176, -0.90491164,\n",
       "        -0.15521851,  0.9767159 ,  0.7644248 ,  0.8565381 , -0.99990207,\n",
       "        -0.9623931 ,  0.72732365,  0.83776534,  0.8405562 ,  0.93018   ,\n",
       "        -0.9974139 ,  0.9931088 ,  0.07324785,  0.5680444 ,  0.3827817 ,\n",
       "        -0.80021226, -0.80699253, -0.45147175,  0.64812344,  0.9990776 ,\n",
       "         0.7948134 ,  0.2598515 , -0.25192282, -0.9777649 , -0.28500357,\n",
       "         0.9096495 , -0.9984302 , -0.37892425,  0.43678164, -0.9553443 ,\n",
       "         0.851359  ,  0.9932278 , -0.9999998 , -0.9950846 ,  0.6425495 ,\n",
       "         0.9806341 ,  0.5147226 , -0.04615894,  0.90227133, -0.95482904,\n",
       "        -0.30388743,  0.8824214 ,  0.98086643]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer([corpus[3]], padding = 'max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "tmp_emb = p_encoder(**tmp).to('cpu').detach().numpy()\n",
    "tmp_emb\n",
    "\n",
    "tmp2 = tokenizer([corpus[3]], padding = 'max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "tmp2_emb = p_encoder(**tmp2).to('cpu').detach().numpy()\n",
    "tmp2_emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개요 형태로 나열하고 있다.\\n\\n이 목록은 명료화를 위해 두 부분으로 나뉘어 있다.\\n\\n# 첫 번째 부분은 바티칸 시국과 팔레스타인을 포함하여 유엔 등 국제 기구에 가입되어 국제적인 승인을 널리 받았다고 여기는 195개 나라를 나열하고 있다.\\n# 두 번째 부분은 일부 지역의 주권을 사실상 (데 팍토) 행사하고 있지만, 아직 국제적인 승인을 널리 받지 않았다고 여기는 11개 나라를 나열하고 있다.\\n\\n두 목록은 모두 가나다 순이다.\\n\\n일부 국가의 경우 국가로서의 자격에 논쟁의 여부가 있으며, 이 때문에 이러한 목록을 엮는 것은 매우 어렵고 논란이 생길 수 있는 과정이다. 이 목록을 구성하고 있는 국가를 선정하는 기준에 대한 정보는 \"포함 기준\" 단락을 통해 설명하였다. 나라에 대한 일반적인 정보는 \"국가\" 문서에서 설명하고 있다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9144,  0.0094,  0.6034,  ..., -0.4013,  0.8149, -0.9985],\n",
       "        [ 0.9143,  0.0093,  0.6033,  ..., -0.4012,  0.8148, -0.9985],\n",
       "        [ 0.9143,  0.0094,  0.6033,  ..., -0.4011,  0.8148, -0.9985],\n",
       "        ...,\n",
       "        [ 0.9144,  0.0094,  0.6035,  ..., -0.4010,  0.8148, -0.9985],\n",
       "        [ 0.9144,  0.0091,  0.6034,  ..., -0.4011,  0.8149, -0.9985],\n",
       "        [ 0.9144,  0.0093,  0.6036,  ..., -0.4013,  0.8148, -0.9985]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = []\n",
    "for i, aa in enumerate(corpus) :\n",
    "    a = tokenizer(aa, padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "    check.append(a)\n",
    "    if i == 2 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1a0935536de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'check' is not defined"
     ]
    }
   ],
   "source": [
    "check[0]['input_ids'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,  1504, 10188,  2170, 11381,  3728,  3872,  2073, 20998,  2440],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check[1]['input_ids'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(query, q_encoder, p_embs, k=1) :\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        \n",
    "        q_seqs_val = tokenizer(\n",
    "                    [query],\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "        ).to(args.device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to(\"cpu\")  # (num_query=1, emb_dim)\n",
    "\n",
    "    dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "    rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "\n",
    "    return dot_prod_scores, rank[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9113, -0.4006,  0.8705,  ..., -0.2941,  0.8215,  0.9828],\n",
       "        [ 0.8962, -0.5467,  0.9159,  ..., -0.1864,  0.7883,  0.9975],\n",
       "        [ 0.8993, -0.4746,  0.7946,  ..., -0.2417,  0.8237,  0.9943],\n",
       "        ...,\n",
       "        [ 0.8058, -0.2533,  0.8524,  ..., -0.2027,  0.8292,  0.9790],\n",
       "        [ 0.8413, -0.2148,  0.8774,  ..., -0.1120,  0.7612,  0.9868],\n",
       "        [ 0.7264, -0.1229,  0.8096,  ..., -0.1845,  0.8155,  0.9918]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'][3], q_encoder, p_embs, k = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([98, 97])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries: List, q_encoder, p_embs, k=1) :\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_embs = []\n",
    "        for q in queries :\n",
    "            q = tokenizer([q], padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "            q_emb = q_encoder(**q).to('cpu').numpy()\n",
    "            q_embs.append(q_emb)\n",
    "    q_embs = torch.Tensor(q_embs).squeeze()\n",
    "\n",
    "    result = torch.matmul(q_embs, torch.transpose(p_embs, 0, 1))\n",
    "    if not isinstance(result, np.ndarray) :\n",
    "        result = result.cpu().detach().numpy()\n",
    "\n",
    "    doc_scores = []\n",
    "    doc_indices = []\n",
    "    for i in range(result.shape[0]) :\n",
    "        sorted_result = np.argsort(result[i, :][::-1])\n",
    "        doc_scores.append(result[i, :][sorted_result].tolist()[:k])\n",
    "        doc_indices.append(sorted_result.tolist()[:k])\n",
    "\n",
    "    return result, doc_scores, doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "result, doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [35624],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [29703],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [35624],\n",
       " [52442],\n",
       " [29711],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [48782],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [29711],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [35624],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [29703],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [27014],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [43194],\n",
       " [52442],\n",
       " [52442],\n",
       " [39196],\n",
       " [52442],\n",
       " [52442],\n",
       " [27014],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [43194],\n",
       " [52442],\n",
       " [29706],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [29697],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [30217],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [43194],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_relavant_doc() missing 2 required positional arguments: 'q_encoder' and 'p_embs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-db2a2b67c075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relavant_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m for idx, example in enumerate(\n\u001b[1;32m      5\u001b[0m     \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Sparse retrieval: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_relavant_doc() missing 2 required positional arguments: 'q_encoder' and 'p_embs'"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, example in enumerate(\n",
    "    tqdm(dataset['validation'], desc=\"Sparse retrieval: \")\n",
    "):\n",
    "    tmp = {\n",
    "        # Query와 해당 id를 반환합니다.\n",
    "        \"question\": example[\"question\"],\n",
    "        \"id\": example[\"id\"],\n",
    "        # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "        \"context_id\": doc_indices[idx],\n",
    "        \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "            [corpus[pid] for pid in doc_indices[idx]]\n",
    "        ),\n",
    "    }\n",
    "    if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "        # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "        tmp[\"original_context\"] = example[\"context\"]\n",
    "        tmp[\"answers\"] = example[\"answers\"]\n",
    "    total.append(tmp)\n",
    "cqas = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    Sequence,\n",
    "    Value,\n",
    "    Features,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    ")\n",
    "f = Features(\n",
    "    {\n",
    "        \"answers\": Sequence(\n",
    "            feature={\n",
    "                \"text\": Value(dtype=\"string\", id=None),\n",
    "                \"answer_start\": Value(dtype=\"int32\", id=None),\n",
    "            },\n",
    "            length=-1,\n",
    "            id=None,\n",
    "        ),\n",
    "        \"context\": Value(dtype=\"string\", id=None),\n",
    "        \"id\": Value(dtype=\"string\", id=None),\n",
    "        \"question\": Value(dtype=\"string\", id=None),\n",
    "    }\n",
    ")\n",
    "datasets = DatasetDict({\"validation\": Dataset.from_pandas(df, features=f)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
