{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, RobertaModel,\n",
    "    BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.6.0].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwer\n",
    "class DenseRetrieval:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        p_encoder,\n",
    "        q_encoder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        학습과 추론에 사용될 여러 셋업을 마쳐봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "\n",
    "    def train(self, args=None, tokenizer = None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        if tokenizer is None :\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        q_seqs = tokenizer(self.dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        p_seqs = tokenizer(self.dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "        train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=args.per_device_train_batch_size)\n",
    "\n",
    "        no_decay = [\"bias\" ,\"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            # eps=args.adam_epsilon\n",
    "        )\n",
    "\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "        \n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "        for epoch, _ in enumerate(train_iterator):\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            # loss_value=0 # Accumulation할 때 진행\n",
    "            losses = 0\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.q_encoder.train()\n",
    "                self.p_encoder.train()\n",
    "            \n",
    "                if torch.cuda.is_available():\n",
    "                    batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "                p_inputs = {'input_ids': batch[0],\n",
    "                            'attention_mask': batch[1],\n",
    "                            'token_type_ids': batch[2]\n",
    "                            }\n",
    "                \n",
    "                q_inputs = {'input_ids': batch[3],\n",
    "                            'attention_mask': batch[4],\n",
    "                            'token_type_ids': batch[5]}\n",
    "            \n",
    "                p_outputs = self.p_encoder(**p_inputs)  # (batch_size, emb_dim)\n",
    "                q_outputs = self.q_encoder(**q_inputs)  # (batch_size, emb_dim)\n",
    "\n",
    "                # Calculate similarity score & loss\n",
    "                sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "\n",
    "                # target: position of positive samples = diagonal element \n",
    "                targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "                if torch.cuda.is_available():\n",
    "                    targets = targets.to('cuda')\n",
    "\n",
    "                sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                loss = F.nll_loss(sim_scores, targets)\n",
    "                losses += loss.item()\n",
    "                if step % 100 == 0 :\n",
    "                    print(f'{epoch}epoch loss: {losses/(step+1)}') # Accumulation할 경우 주석처리\n",
    "\n",
    "                loss.backward()\n",
    "                #################ACCUMULATION###############################\n",
    "                # loss_value += loss\n",
    "                # if (step+1) % args.gradient_accumulation_steps == 0 :\n",
    "                #     optimizer.step()\n",
    "                #     scheduler.step()\n",
    "                #     self.q_encoder.zero_grad()\n",
    "                #     self.p_encoder.zero_grad()\n",
    "                #     global_step += 1\n",
    "                #     print(loss_value/args.gradient_accumulation_steps)\n",
    "                #     loss_value = 0\n",
    "                ############################################################\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                self.q_encoder.zero_grad()\n",
    "                self.p_encoder.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                del p_inputs, q_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/data/train_dataset')\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6fdd9837f040ff9734314825e3fb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=247.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0epoch loss: 45.39480209350586\n",
      "0epoch loss: 7.887749570431096\n",
      "0epoch loss: 6.570167403909104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [05:30<00:00, 330.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retriever는 아래와 같이 사용할 수 있도록 코드를 짜봅시다.\n",
    "retriever = DenseRetrieval(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder\n",
    ")\n",
    "retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "corpus = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ")  # set 은 매번 순서가 바뀌므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7253bfca4143a0b408b2a8b31e34bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56737.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p_encoder = retriever.p_encoder\n",
    "q_encoder = retriever.q_encoder\n",
    "with torch.no_grad() :\n",
    "    p_encoder.eval()\n",
    "\n",
    "    p_embs = []\n",
    "    for p in tqdm(corpus) :\n",
    "        p = tokenizer([p], padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "        p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "        p_embs.append(p_emb)\n",
    "p_embs = torch.Tensor(p_embs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9143,  0.0093,  0.6034,  1.0000,  0.7705, -0.1520,  1.0000,  0.5420,\n",
       "        -0.9997,  0.9789, -0.7481, -1.0000, -0.6572, -0.9847,  0.0947,  1.0000,\n",
       "         0.9999, -0.9969, -1.0000,  0.6772,  0.9442,  1.0000, -0.8610,  0.9957,\n",
       "        -0.7710, -0.9560,  1.0000,  1.0000,  1.0000,  1.0000,  0.8914,  1.0000,\n",
       "        -0.9265, -1.0000,  0.9758, -0.9938,  1.0000,  1.0000,  0.7100, -0.2801,\n",
       "         0.7355, -0.4112,  1.0000, -1.0000, -1.0000,  0.5649,  1.0000,  1.0000,\n",
       "         0.9698, -0.8235, -1.0000, -0.9866, -0.9917,  0.8870,  0.8346,  0.8470,\n",
       "        -1.0000,  1.0000, -0.6939, -0.9981,  0.9993, -0.2256, -0.9755, -0.7413,\n",
       "        -0.9865, -0.8644,  0.1050, -0.9537, -1.0000, -0.7533, -0.9895,  0.9980,\n",
       "        -0.8461, -0.9776, -0.7893,  0.3740,  0.9431, -0.9999,  0.4345,  0.9897,\n",
       "        -0.9549,  0.7058, -0.9390,  0.9786,  0.9999,  1.0000,  0.9969, -1.0000,\n",
       "        -0.7620, -0.5176,  1.0000, -0.9876, -0.8812, -0.9652, -0.5312, -0.9905,\n",
       "         0.9923, -1.0000, -1.0000,  0.7790, -0.9420, -0.3340,  0.9997, -0.9609,\n",
       "        -0.9959, -0.0087, -1.0000,  0.6131,  0.9518,  0.9385, -0.8922, -0.9160,\n",
       "        -0.8139, -0.7764, -0.9984, -0.9654,  0.9978, -0.6473,  0.9686, -0.9597,\n",
       "        -0.9262,  1.0000,  1.0000, -0.9725, -0.5860, -0.2844,  0.9631,  0.9576,\n",
       "        -0.5605, -0.6854,  0.8858, -0.0873,  1.0000,  1.0000,  0.7918,  0.9794,\n",
       "        -0.9877, -1.0000, -0.9999, -0.9414, -0.9997,  0.9637,  0.9318,  0.8809,\n",
       "        -0.6950,  0.9418, -0.8907,  0.9845,  0.9993,  0.9419,  1.0000,  0.9501,\n",
       "         0.6046, -0.9992, -0.8169, -0.6689,  0.9583,  0.9381,  0.9991, -0.9219,\n",
       "         0.9452,  1.0000,  0.9812,  0.9469,  0.9424, -1.0000,  0.9649,  0.7917,\n",
       "         0.9091,  0.9711,  1.0000,  0.8084, -0.9743,  0.8963,  0.9844,  1.0000,\n",
       "        -0.9501, -0.8909,  0.8689, -0.2275, -0.9796,  0.9847, -0.9960,  0.8858,\n",
       "        -0.2637, -0.3730, -0.3332,  1.0000, -0.9768,  0.0835, -1.0000, -0.9809,\n",
       "         0.9982,  1.0000,  0.9525,  0.9757, -1.0000, -0.9632, -0.3257, -0.0192,\n",
       "         0.4378,  0.7489, -1.0000, -1.0000, -0.9938, -1.0000, -1.0000,  0.7330,\n",
       "         0.6739, -0.9174,  0.9205,  1.0000, -0.9890, -0.8932, -0.9872, -0.6337,\n",
       "        -0.6767, -1.0000,  0.8361,  0.9999,  1.0000, -0.9245,  0.5061, -0.9851,\n",
       "        -0.7694,  0.6052,  0.9950,  0.6724,  0.9942, -0.9718, -0.8882,  0.9952,\n",
       "         0.2044, -0.4870,  1.0000, -0.9611, -0.9756,  0.9344, -0.8516,  1.0000,\n",
       "        -0.3950,  0.9386,  1.0000,  0.0119,  1.0000, -0.9125,  0.9940,  0.8337,\n",
       "         0.1103,  0.9851,  1.0000,  0.9999,  0.1806, -0.9000, -1.0000,  0.6194,\n",
       "        -0.0349, -1.0000,  0.9991,  0.9251, -0.8057,  0.6851,  1.0000,  0.9664,\n",
       "         0.8456,  0.9998,  0.9891,  0.9744,  1.0000,  1.0000,  0.9935,  0.9808,\n",
       "        -0.7236, -0.9132,  0.8237,  0.3713, -0.8478,  1.0000, -0.9938, -0.2940,\n",
       "         0.8635, -0.9995, -0.9575,  0.6775, -0.3219, -0.5372, -0.9987, -0.9966,\n",
       "         1.0000,  1.0000,  0.9384, -0.9999, -0.3283,  0.8880, -0.9461, -0.1812,\n",
       "         0.2701, -0.7731, -0.8471,  0.9271,  0.9492, -0.9176, -1.0000,  0.9996,\n",
       "         0.9963, -0.9463,  0.9881,  0.9991, -0.3681,  0.4806,  0.9697,  0.5946,\n",
       "        -1.0000, -0.9998,  0.9947,  0.5045,  1.0000,  0.1530,  0.9377,  0.6977,\n",
       "        -0.9326, -0.9994, -1.0000, -0.9970, -0.8230,  1.0000, -0.0128, -0.9996,\n",
       "        -0.9539, -0.9978,  0.9800, -0.8171,  0.9314,  0.9831, -0.9760, -1.0000,\n",
       "        -0.9698,  0.9992,  1.0000,  0.9969,  0.9732, -0.5063,  0.0890, -0.9570,\n",
       "         0.9409,  1.0000, -0.8750,  0.6682, -0.8627,  0.9998,  1.0000,  0.9624,\n",
       "        -0.8656, -1.0000,  0.9980,  0.3818, -0.8661,  0.9302,  1.0000, -0.9831,\n",
       "        -0.9936, -0.9590, -0.3617, -0.8067,  0.1231, -0.8475,  0.9161, -0.9946,\n",
       "         1.0000, -0.5365, -0.6160, -0.8397, -0.8426,  0.8629,  0.9545,  0.9640,\n",
       "         1.0000,  1.0000,  1.0000, -0.8816, -1.0000, -0.8209,  0.5724, -0.9815,\n",
       "        -0.9608, -0.9914, -0.9893, -0.9999, -0.9953, -0.8529, -0.6873, -0.8664,\n",
       "         0.6949, -0.9113,  0.5172,  0.9172,  0.9947,  1.0000,  0.5094,  0.9991,\n",
       "        -0.9842,  0.8210,  0.8390,  0.6419,  1.0000,  0.8778, -1.0000, -0.8195,\n",
       "         0.0906, -0.7275, -0.9847,  0.9122,  0.8294, -0.9508, -0.9945,  0.9282,\n",
       "         0.0949, -0.2613, -0.9033, -1.0000, -0.8397,  1.0000, -0.9961,  1.0000,\n",
       "        -1.0000,  0.9949, -0.8967,  0.5668, -0.5999,  0.4432, -0.9506,  1.0000,\n",
       "        -1.0000,  0.9962, -0.2409, -0.9756,  0.4403, -0.4170, -0.6921,  0.4288,\n",
       "        -0.8964, -0.5965, -0.6879,  0.3515,  0.9974,  1.0000,  0.9274, -0.6920,\n",
       "         0.9506,  0.9979,  1.0000, -0.8045, -0.8583,  0.9719, -0.9949,  0.9632,\n",
       "        -0.8007, -0.9685, -0.8217,  0.9988, -0.9321,  0.9990,  1.0000,  0.9104,\n",
       "        -0.9945, -1.0000,  1.0000, -1.0000,  0.8180, -1.0000, -0.2524,  0.9500,\n",
       "        -0.7606, -0.9244, -0.9365,  0.4656,  0.9975,  1.0000,  0.9741,  0.9925,\n",
       "         0.9690, -0.9279,  0.9908,  0.8992,  0.1784,  0.9802, -0.8943,  0.9488,\n",
       "         0.9779,  0.9976,  0.0629,  0.9920,  0.9501, -0.7145, -0.9434,  0.8711,\n",
       "        -0.9954, -0.9983, -0.7541, -0.9670, -0.5866, -0.9029, -0.5204, -0.8732,\n",
       "         0.9916, -0.9254,  0.9996, -0.8502,  0.3063,  0.9998,  0.9588,  1.0000,\n",
       "        -0.9930, -0.9935,  0.9975, -0.3302, -0.4989,  0.8682,  1.0000, -0.9021,\n",
       "        -0.9369,  0.5189, -0.4091, -0.9646,  1.0000, -0.5106,  0.9259, -0.5733,\n",
       "        -1.0000, -1.0000, -1.0000,  0.9995, -0.7419,  0.3893, -0.9940, -0.4681,\n",
       "         1.0000, -0.7595,  0.9498,  0.9762, -0.9701,  0.8029, -1.0000,  0.9160,\n",
       "        -0.9998,  0.9243,  1.0000, -0.9781,  0.9985, -0.9993,  0.3761,  0.0084,\n",
       "         0.6869,  0.9178,  0.9815, -0.9267,  0.7887,  0.1392,  0.5386, -1.0000,\n",
       "        -0.8521, -0.7788,  0.9980, -1.0000,  0.9964, -0.8101, -0.9999, -0.9351,\n",
       "         1.0000,  0.9685, -0.9988, -0.8945, -0.9765, -0.9636,  0.3673,  1.0000,\n",
       "         1.0000,  0.9982, -0.2546, -0.8590, -0.9525, -0.4444,  0.3892, -0.9999,\n",
       "        -0.9559, -0.9232, -0.8938,  0.0333,  0.9934,  0.8034,  0.5930,  0.9464,\n",
       "         0.9460, -0.9668,  1.0000, -0.8016, -0.6897, -0.9995,  0.9791,  0.5527,\n",
       "         0.9892, -0.1419, -0.5524,  0.9139,  0.9552, -0.6832,  0.9637, -0.7184,\n",
       "        -0.9962,  1.0000, -0.9174, -0.7172,  0.5322,  0.8502, -0.2779,  0.9582,\n",
       "        -0.7184,  0.9913, -0.9644,  0.6967,  0.9456,  0.3260,  1.0000, -0.7822,\n",
       "         0.8224, -0.9938, -1.0000, -1.0000,  0.2018, -0.9707, -0.9858, -1.0000,\n",
       "         0.9609, -0.8643, -0.9999,  1.0000, -0.9519,  0.5464, -1.0000,  0.9963,\n",
       "         0.9990, -1.0000, -0.6176,  0.2502, -0.6309,  0.9998, -0.9999, -1.0000,\n",
       "        -0.8929, -0.9483,  0.9845, -0.2482,  1.0000,  1.0000,  1.0000, -1.0000,\n",
       "        -0.8911, -1.0000, -0.6662,  0.1360, -0.9783,  1.0000, -0.6203, -0.7519,\n",
       "         0.9119,  0.8382,  1.0000, -0.7342, -0.4281, -0.9107,  0.9531,  0.9999,\n",
       "        -1.0000,  0.9997, -0.2074,  0.9436,  0.9213, -1.0000, -0.9999, -1.0000,\n",
       "         0.7316, -1.0000,  0.9153, -0.9839, -0.8647,  0.9014, -1.0000, -0.9114,\n",
       "        -0.1489, -0.8997, -0.9932, -0.9928, -0.8582, -0.9214,  1.0000, -0.9743,\n",
       "        -0.0631, -0.4007,  1.0000, -1.0000,  0.3777, -0.8852, -0.7639,  0.6609,\n",
       "         1.0000, -1.0000,  0.9957, -0.9843, -1.0000,  0.6637, -1.0000,  0.5499,\n",
       "         0.9887,  0.4318,  0.9998,  1.0000,  0.9917,  1.0000, -0.9623, -0.9837,\n",
       "        -0.9848, -1.0000, -0.3715, -1.0000, -0.9283,  0.8557,  0.4404,  0.9928,\n",
       "         0.3426,  1.0000, -0.9819, -0.7345,  0.7479,  0.9764,  0.9878,  1.0000,\n",
       "        -0.9999, -0.4548,  0.9936,  0.6385, -0.9968, -0.9973, -0.8639,  0.5435,\n",
       "         0.5294,  0.8004, -0.9984, -0.8514, -0.9684,  0.6620,  0.9123,  1.0000,\n",
       "        -0.9963,  0.9552, -0.8853, -0.0210, -1.0000,  1.0000,  0.9999, -0.1202,\n",
       "         0.6074, -0.2048,  0.9881,  0.9830, -0.9848, -0.4011,  0.8148, -0.9985])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_embs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer([corpus[3]], padding = 'max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "tmp_emb = p_encoder(**tmp).to('cpu').detach().numpy()\n",
    "tmp_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개요 형태로 나열하고 있다.\\n\\n이 목록은 명료화를 위해 두 부분으로 나뉘어 있다.\\n\\n# 첫 번째 부분은 바티칸 시국과 팔레스타인을 포함하여 유엔 등 국제 기구에 가입되어 국제적인 승인을 널리 받았다고 여기는 195개 나라를 나열하고 있다.\\n# 두 번째 부분은 일부 지역의 주권을 사실상 (데 팍토) 행사하고 있지만, 아직 국제적인 승인을 널리 받지 않았다고 여기는 11개 나라를 나열하고 있다.\\n\\n두 목록은 모두 가나다 순이다.\\n\\n일부 국가의 경우 국가로서의 자격에 논쟁의 여부가 있으며, 이 때문에 이러한 목록을 엮는 것은 매우 어렵고 논란이 생길 수 있는 과정이다. 이 목록을 구성하고 있는 국가를 선정하는 기준에 대한 정보는 \"포함 기준\" 단락을 통해 설명하였다. 나라에 대한 일반적인 정보는 \"국가\" 문서에서 설명하고 있다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9144,  0.0094,  0.6034,  ..., -0.4013,  0.8149, -0.9985],\n",
       "        [ 0.9143,  0.0093,  0.6033,  ..., -0.4012,  0.8148, -0.9985],\n",
       "        [ 0.9143,  0.0094,  0.6033,  ..., -0.4011,  0.8148, -0.9985],\n",
       "        ...,\n",
       "        [ 0.9144,  0.0094,  0.6035,  ..., -0.4010,  0.8148, -0.9985],\n",
       "        [ 0.9144,  0.0091,  0.6034,  ..., -0.4011,  0.8149, -0.9985],\n",
       "        [ 0.9144,  0.0093,  0.6036,  ..., -0.4013,  0.8148, -0.9985]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = []\n",
    "for i, aa in enumerate(corpus) :\n",
    "    a = tokenizer(aa, padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "    check.append(a)\n",
    "    if i == 2 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,  1504,  6953,  2259,  3779, 10188,  2052,  2307,    16,  1537],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check[0]['input_ids'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,  1504, 10188,  2170, 11381,  3728,  3872,  2073, 20998,  2440],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check[1]['input_ids'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(query, q_encoder, p_embs, k=1) :\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        \n",
    "        q_seqs_val = tokenizer(\n",
    "                    [query],\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "        ).to(args.device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to(\"cpu\")  # (num_query=1, emb_dim)\n",
    "\n",
    "    dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "    rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "\n",
    "    return dot_prod_scores, rank[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9920, -0.9073, -0.9912,  ...,  0.1764, -0.9692,  0.9948],\n",
       "        [-0.9920, -0.9073, -0.9912,  ...,  0.1764, -0.9692,  0.9948],\n",
       "        [-0.9920, -0.9073, -0.9912,  ...,  0.1764, -0.9692,  0.9948],\n",
       "        ...,\n",
       "        [-0.9920, -0.9073, -0.9912,  ...,  0.1764, -0.9692,  0.9947],\n",
       "        [-0.9920, -0.9073, -0.9912,  ...,  0.1763, -0.9692,  0.9948],\n",
       "        [-0.9920, -0.9073, -0.9912,  ...,  0.1763, -0.9692,  0.9948]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'스카버러 남쪽과 코보콘그 마을의 철도 노선이 처음 연장된 연도는?'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']['question'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'][0], q_encoder, p_embs, k = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([51633, 52323])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries: List, q_encoder, p_embs, k=1) :\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_embs = []\n",
    "        for q in queries :\n",
    "            q = tokenizer([q], padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "            q_emb = q_encoder(**q).to('cpu').numpy()\n",
    "            q_embs.append(q_emb)\n",
    "    q_embs = torch.Tensor(q_embs).squeeze()\n",
    "\n",
    "    result = torch.matmul(q_embs, torch.transpose(p_embs, 0, 1))\n",
    "    if not isinstance(result, np.ndarray) :\n",
    "        result = result.cpu().detach().numpy()\n",
    "\n",
    "    doc_scores = []\n",
    "    doc_indices = []\n",
    "    for i in range(result.shape[0]) :\n",
    "        sorted_result = np.argsort(result[i, :][::-1])\n",
    "        doc_scores.append(result[i, :][sorted_result].tolist()[:k])\n",
    "        doc_indices.append(sorted_result.tolist()[:k])\n",
    "\n",
    "    return result, doc_scores, doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "result, doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157],\n",
       " [52442, 36157]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [35624],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [29703],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [35624],\n",
       " [52442],\n",
       " [29711],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [48782],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [29711],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [35624],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [29703],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [27014],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [43194],\n",
       " [52442],\n",
       " [52442],\n",
       " [39196],\n",
       " [52442],\n",
       " [52442],\n",
       " [27014],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [43194],\n",
       " [52442],\n",
       " [29706],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [29697],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [30217],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [49808],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [43194],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442],\n",
       " [52442]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_relavant_doc() missing 2 required positional arguments: 'q_encoder' and 'p_embs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-db2a2b67c075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relavant_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m for idx, example in enumerate(\n\u001b[1;32m      5\u001b[0m     \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Sparse retrieval: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_relavant_doc() missing 2 required positional arguments: 'q_encoder' and 'p_embs'"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, example in enumerate(\n",
    "    tqdm(dataset['validation'], desc=\"Sparse retrieval: \")\n",
    "):\n",
    "    tmp = {\n",
    "        # Query와 해당 id를 반환합니다.\n",
    "        \"question\": example[\"question\"],\n",
    "        \"id\": example[\"id\"],\n",
    "        # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "        \"context_id\": doc_indices[idx],\n",
    "        \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "            [corpus[pid] for pid in doc_indices[idx]]\n",
    "        ),\n",
    "    }\n",
    "    if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "        # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "        tmp[\"original_context\"] = example[\"context\"]\n",
    "        tmp[\"answers\"] = example[\"answers\"]\n",
    "    total.append(tmp)\n",
    "cqas = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    Sequence,\n",
    "    Value,\n",
    "    Features,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    ")\n",
    "f = Features(\n",
    "    {\n",
    "        \"answers\": Sequence(\n",
    "            feature={\n",
    "                \"text\": Value(dtype=\"string\", id=None),\n",
    "                \"answer_start\": Value(dtype=\"int32\", id=None),\n",
    "            },\n",
    "            length=-1,\n",
    "            id=None,\n",
    "        ),\n",
    "        \"context\": Value(dtype=\"string\", id=None),\n",
    "        \"id\": Value(dtype=\"string\", id=None),\n",
    "        \"question\": Value(dtype=\"string\", id=None),\n",
    "    }\n",
    ")\n",
    "datasets = DatasetDict({\"validation\": Dataset.from_pandas(df, features=f)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
