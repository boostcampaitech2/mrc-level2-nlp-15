{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'klue/bert-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7484f97b-df40-4b4a-a818-aa061525e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "import string\n",
    "import pickle\n",
    "import os.path as p\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datasets import load_from_disk\n",
    "from transformers import AdamW, TrainingArguments\n",
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizerFast, BertConfig\n",
    "\n",
    "\n",
    "class QueryTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tok = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "\n",
    "        self.Q_marker_token, self.Q_marker_token_id = \"[Q]\", self.tok.convert_tokens_to_ids(\"[unused0]\")\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "        self.mask_token, self.mask_token_id = self.tok.mask_token, self.tok.mask_token_id\n",
    "        self.query_maxlen = self.tok.model_max_length\n",
    "\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.Q_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix + [self.mask_token] * (self.query_maxlen - (len(lst) + 3)) for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.Q_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix + [self.mask_token_id] * (self.query_maxlen - (len(lst) + 3)) for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        # add placehold for the [Q] marker\n",
    "        batch_text = [\". \" + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(\n",
    "            batch_text, padding=\"longest\", truncation=True, return_tensors=\"pt\", max_length=self.tok.model_max_length\n",
    "        )\n",
    "\n",
    "        ids, mask = obj[\"input_ids\"], obj[\"attention_mask\"]\n",
    "\n",
    "        # postprocess for the [Q] marker and the [MASK] augmentation\n",
    "        ids[:, 1] = self.Q_marker_token_id\n",
    "        ids[ids == 0] = self.mask_token_id\n",
    "\n",
    "        if bsize:\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "\n",
    "class DocTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tok = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "\n",
    "        self.D_marker_token, self.D_marker_token_id = \"[D]\", self.tok.convert_tokens_to_ids(\"[unused1]\")\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "\n",
    "        # assert self.D_marker_token_id == 1\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.D_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.D_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        # add placehold for the [D] marker\n",
    "        batch_text = [\". \" + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(\n",
    "            batch_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=self.tok.model_max_length\n",
    "        )\n",
    "\n",
    "        ids, mask = obj[\"input_ids\"], obj[\"attention_mask\"]\n",
    "\n",
    "        # postprocess for the [D] marker\n",
    "        ids[:, 1] = self.D_marker_token_id\n",
    "\n",
    "        if bsize:\n",
    "            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches, reverse_indices\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "\n",
    "def tensorize_triples(query_tokenizer, doc_tokenizer, queries, positives, negatives, bsize):\n",
    "    # assert len(queries) == len(positives) == len(negatives)\n",
    "    # assert bsize is None or len(queries) % bsize == 0\n",
    "\n",
    "    N = len(queries)\n",
    "    queries = queries.to_list()\n",
    "    Q_ids, Q_mask = query_tokenizer.tensorize(queries)\n",
    "\n",
    "    positives = positives.to_list()\n",
    "    negatives = negatives.to_list()\n",
    "\n",
    "    D_ids, D_mask = doc_tokenizer.tensorize(positives + negatives)\n",
    "    D_ids, D_mask = D_ids.view(2, N, -1), D_mask.view(2, N, -1)\n",
    "\n",
    "    # Compute max among {length of i^th positive, length of i^th negative} for i \\in N\n",
    "    maxlens = D_mask.sum(-1).max(0).values\n",
    "\n",
    "    # Sort by maxlens\n",
    "    indices = maxlens.sort().indices\n",
    "    Q_ids, Q_mask = Q_ids[indices], Q_mask[indices]\n",
    "    D_ids, D_mask = D_ids[:, indices], D_mask[:, indices]\n",
    "\n",
    "    (positive_ids, negative_ids), (positive_mask, negative_mask) = D_ids, D_mask\n",
    "\n",
    "    query_batches = _split_into_batches(Q_ids, Q_mask, bsize)\n",
    "    positive_batches = _split_into_batches(positive_ids, positive_mask, bsize)\n",
    "    negative_batches = _split_into_batches(negative_ids, negative_mask, bsize)\n",
    "\n",
    "    batches = []\n",
    "    for (q_ids, q_mask), (p_ids, p_mask), (n_ids, n_mask) in zip(query_batches, positive_batches, negative_batches):\n",
    "        Q = (torch.cat((q_ids, q_ids)), torch.cat((q_mask, q_mask)))\n",
    "        D = (torch.cat((p_ids, n_ids)), torch.cat((p_mask, n_mask)))\n",
    "        batches.append((Q, D))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def _sort_by_length(ids, mask, bsize):\n",
    "    if ids.size(0) <= bsize:\n",
    "        return ids, mask, torch.arange(ids.size(0))\n",
    "\n",
    "    indices = mask.sum(-1).sort().indices\n",
    "    reverse_indices = indices.sort().indices\n",
    "\n",
    "    return ids[indices], mask[indices], reverse_indices\n",
    "\n",
    "\n",
    "def _split_into_batches(ids, mask, bsize):\n",
    "    batches = []\n",
    "    for offset in range(0, ids.size(0), bsize):\n",
    "        batches.append((ids[offset : offset + bsize], mask[offset : offset + bsize]))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "\n",
    "def print_message(*s, condition=True):\n",
    "    s = ' '.join([str(x) for x in s])\n",
    "    msg = \"[{}] {}\".format(datetime.datetime.now().strftime(\"%b %d, %H:%M:%S\"), s)\n",
    "\n",
    "    if condition:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "    return msg\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    format_str = \"%Y-%m-%d_%H.%M.%S\"\n",
    "    result = datetime.datetime.now().strftime(format_str)\n",
    "    return result\n",
    "\n",
    "\n",
    "def file_tqdm(file):\n",
    "    print(f\"#> Reading {file.name}\")\n",
    "\n",
    "    with tqdm.tqdm(total=os.path.getsize(file.name) / 1024.0 / 1024.0, unit=\"MiB\") as pbar:\n",
    "        for line in file:\n",
    "            yield line\n",
    "            pbar.update(len(line) / 1024.0 / 1024.0)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "\n",
    "def save_checkpoint(path, epoch_idx, mb_idx, model, optimizer, arguments=None):\n",
    "    print(f\"#> Saving a checkpoint to {path} ..\")\n",
    "\n",
    "    if hasattr(model, 'module'):\n",
    "        model = model.module  # extract model from a distributed/data-parallel wrapper\n",
    "\n",
    "    checkpoint = {}\n",
    "    checkpoint['epoch'] = epoch_idx\n",
    "    checkpoint['batch'] = mb_idx\n",
    "    checkpoint['model_state_dict'] = model.state_dict()\n",
    "    checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
    "    checkpoint['arguments'] = arguments\n",
    "\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None, do_print=True):\n",
    "    if do_print:\n",
    "        print_message(\"#> Loading checkpoint\", path, \"..\")\n",
    "\n",
    "    if path.startswith(\"http:\") or path.startswith(\"https:\"):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(path, map_location='cpu')\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k\n",
    "        if k[:7] == 'module.':\n",
    "            name = k[7:]\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    checkpoint['model_state_dict'] = new_state_dict\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except:\n",
    "        print_message(\"[WARNING] Loading checkpoint with strict=False\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    if do_print:\n",
    "        print_message(\"#> checkpoint['epoch'] =\", checkpoint['epoch'])\n",
    "        print_message(\"#> checkpoint['batch'] =\", checkpoint['batch'])\n",
    "\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def create_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        print('\\n')\n",
    "        print_message(\"#> Note: Output directory\", path, 'already exists\\n\\n')\n",
    "    else:\n",
    "        print('\\n')\n",
    "        print_message(\"#> Creating directory\", path, '\\n\\n')\n",
    "        os.makedirs(path)\n",
    "\n",
    "# def batch(file, bsize):\n",
    "#     while True:\n",
    "#         L = [ujson.loads(file.readline()) for _ in range(bsize)]\n",
    "#         yield L\n",
    "#     return\n",
    "\n",
    "\n",
    "def f7(seq):\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/a/480227/1493011\n",
    "    \"\"\"\n",
    "\n",
    "    seen = set()\n",
    "    return [x for x in seq if not (x in seen or seen.add(x))]\n",
    "\n",
    "\n",
    "def batch(group, bsize, provide_offset=False):\n",
    "    offset = 0\n",
    "    while offset < len(group):\n",
    "        L = group[offset: offset + bsize]\n",
    "        yield ((offset, L) if provide_offset else L)\n",
    "        offset += len(L)\n",
    "    return\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"\n",
    "    dot.notation access to dictionary attributes\n",
    "    Credit: derek73 @ https://stackoverflow.com/questions/2352181\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "def flatten(L):\n",
    "    return [x for y in L for x in y]\n",
    "\n",
    "\n",
    "def zipstar(L, lazy=False):\n",
    "    \"\"\"\n",
    "    A much faster A, B, C = zip(*[(a, b, c), (a, b, c), ...])\n",
    "    May return lists or tuples.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(L) == 0:\n",
    "        return L\n",
    "\n",
    "    width = len(L[0])\n",
    "\n",
    "    if width < 100:\n",
    "        return [[elem[idx] for elem in L] for idx in range(width)]\n",
    "\n",
    "    L = zip(*L)\n",
    "\n",
    "    return L if lazy else list(L)\n",
    "\n",
    "\n",
    "def zip_first(L1, L2):\n",
    "    length = len(L1) if type(L1) in [tuple, list] else None\n",
    "\n",
    "    L3 = list(zip(L1, L2))\n",
    "\n",
    "    assert length in [None, len(L3)], \"zip_first() failure: length differs!\"\n",
    "\n",
    "    return L3\n",
    "\n",
    "\n",
    "def int_or_float(val):\n",
    "    if '.' in val:\n",
    "        return float(val)\n",
    "        \n",
    "    return int(val)\n",
    "\n",
    "def load_ranking(path, types=None, lazy=False):\n",
    "    print_message(f\"#> Loading the ranked lists from {path} ..\")\n",
    "\n",
    "    try:\n",
    "        lists = torch.load(path)\n",
    "        lists = zipstar([l.tolist() for l in tqdm.tqdm(lists)], lazy=lazy)\n",
    "    except:\n",
    "        if types is None:\n",
    "            types = itertools.cycle([int_or_float])\n",
    "\n",
    "        with open(path) as f:\n",
    "            lists = [[typ(x) for typ, x in zip_first(types, line.strip().split('\\t'))]\n",
    "                     for line in file_tqdm(f)]\n",
    "\n",
    "    return lists\n",
    "\n",
    "\n",
    "def save_ranking(ranking, path):\n",
    "    lists = zipstar(ranking)\n",
    "    lists = [torch.tensor(l) for l in lists]\n",
    "\n",
    "    torch.save(lists, path)\n",
    "\n",
    "    return lists\n",
    "\n",
    "\n",
    "def groupby_first_item(lst):\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    for first, *rest in lst:\n",
    "        rest = rest[0] if len(rest) == 1 else rest\n",
    "        groups[first].append(rest)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def process_grouped_by_first_item(lst):\n",
    "    \"\"\"\n",
    "        Requires items in list to already be grouped by first item.\n",
    "    \"\"\"\n",
    "\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    started = False\n",
    "    last_group = None\n",
    "\n",
    "    for first, *rest in lst:\n",
    "        rest = rest[0] if len(rest) == 1 else rest\n",
    "\n",
    "        if started and first != last_group:\n",
    "            yield (last_group, groups[last_group])\n",
    "            assert first not in groups, f\"{first} seen earlier --- violates precondition.\"\n",
    "\n",
    "        groups[first].append(rest)\n",
    "\n",
    "        last_group = first\n",
    "        started = True\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"\"\"\n",
    "    Collect data into fixed-length chunks or blocks\n",
    "        Example: grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "        Source: https://docs.python.org/3/library/itertools.html#itertools-recipes\n",
    "    \"\"\"\n",
    "\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "\n",
    "# see https://stackoverflow.com/a/45187287\n",
    "class NullContextManager(object):\n",
    "    def __init__(self, dummy_resource=None):\n",
    "        self.dummy_resource = dummy_resource\n",
    "    def __enter__(self):\n",
    "        return self.dummy_resource\n",
    "    def __exit__(self, *args):\n",
    "        pass\n",
    "\n",
    "\n",
    "def load_batch_backgrounds(args, qids):\n",
    "    if args.qid2backgrounds is None:\n",
    "        return None\n",
    "\n",
    "    qbackgrounds = []\n",
    "\n",
    "    for qid in qids:\n",
    "        back = args.qid2backgrounds[qid]\n",
    "\n",
    "        if len(back) and type(back[0]) == int:\n",
    "            x = [args.collection[pid] for pid in back]\n",
    "        else:\n",
    "            x = [args.collectionX.get(pid, '') for pid in back]\n",
    "\n",
    "        x = ' [SEP] '.join(x)\n",
    "        qbackgrounds.append(x)\n",
    "    \n",
    "    return qbackgrounds\n",
    "\n",
    "\n",
    "class ColBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, mask_punctuation=string.punctuation, dim=128, similarity_metric=\"cosine\"):\n",
    "        super(ColBERT, self).__init__(config)\n",
    "\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.dim = dim\n",
    "\n",
    "        self.mask_punctuation = mask_punctuation\n",
    "        self.skiplist = {}\n",
    "\n",
    "        if self.mask_punctuation:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "            self.skiplist = {\n",
    "                w: True\n",
    "                for symbol in string.punctuation\n",
    "                for w in [symbol, self.tokenizer.encode(symbol, add_special_tokens=False)[0]]\n",
    "            }\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.linear = nn.Linear(config.hidden_size, dim, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, Q=None, D=None):\n",
    "        # return self.query(**Q), self.doc(**D)\n",
    "        \n",
    "        return self.score(self.query(**Q), self.doc(**D))\n",
    "\n",
    "    def query(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # input_ids, attention_mask = input_ids.to(\"cuda\"), attention_mask.to(\"cuda\")\n",
    "        Q = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        # Q_pooled_outputs = Q_outputs[1]\n",
    "        Q = self.linear(Q)\n",
    "        # return Q\n",
    "        return torch.nn.functional.normalize(Q, p=2, dim=2)\n",
    "\n",
    "    def doc(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # input_ids, attention_mask = input_ids.to(\"cuda\"), attention_mask.to(\"cuda\")\n",
    "        D = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        D = self.linear(D)\n",
    "\n",
    "        mask = torch.tensor(self.mask(input_ids), device=\"cuda\").unsqueeze(2).float()\n",
    "        D = D * mask\n",
    "\n",
    "        D = torch.nn.functional.normalize(D, p=2, dim=2)\n",
    "\n",
    "        # if not keep_dims:\n",
    "        #     D, mask = D.cpu().to(dtype=torch.float16), mask.cpu().bool().squeeze(-1)\n",
    "        #     D = [d[mask[idx]] for idx, d in enumerate(D)]\n",
    "\n",
    "        return D\n",
    "\n",
    "    def score(self, Q, D):\n",
    "        \n",
    "        # print(f'score shape ------------- {Q.shape} {D.shape}')\n",
    "\n",
    "        if self.similarity_metric == \"cosine\":\n",
    "            return (Q @ D.permute(0, 2, 1)).max(2).values.sum(1)\n",
    "\n",
    "        assert self.similarity_metric == \"l2\"\n",
    "        return (-1.0 * ((Q.unsqueeze(2) - D.unsqueeze(1)) ** 2).sum(-1)).max(-1).values.sum(-1)\n",
    "\n",
    "    def mask(self, input_ids):\n",
    "        mask = [[(x not in self.skiplist) and (x != 0) for x in d] for d in input_ids.cpu().tolist()]\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce35e8-4c2c-4e6d-882f-ae3bdcc83985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelInference():\n",
    "#     def __init__(self, colbert: ColBERT, amp=False):\n",
    "#         assert colbert.training is False\n",
    "\n",
    "#         self.colbert = colbert\n",
    "#         self.query_tokenizer = QueryTokenizer(colbert.query_maxlen)\n",
    "#         self.doc_tokenizer = DocTokenizer(colbert.doc_maxlen)\n",
    "\n",
    "#         self.amp_manager = MixedPrecisionManager(amp)\n",
    "\n",
    "#     def query(self, *args, to_cpu=False, **kw_args):\n",
    "#         with torch.no_grad():\n",
    "#             with self.amp_manager.context():\n",
    "#                 Q = self.colbert.query(*args, **kw_args)\n",
    "#                 return Q.cpu() if to_cpu else Q\n",
    "\n",
    "#     def doc(self, *args, to_cpu=False, **kw_args):\n",
    "#         with torch.no_grad():\n",
    "#             with self.amp_manager.context():\n",
    "#                 D = self.colbert.doc(*args, **kw_args)\n",
    "#                 return D.cpu() if to_cpu else D\n",
    "\n",
    "#     def queryFromText(self, queries, bsize=None, to_cpu=False):\n",
    "#         if bsize:\n",
    "#             batches = self.query_tokenizer.tensorize(queries, bsize=bsize)\n",
    "#             batches = [self.query(input_ids, attention_mask, to_cpu=to_cpu) for input_ids, attention_mask in batches]\n",
    "#             return torch.cat(batches)\n",
    "\n",
    "#         input_ids, attention_mask = self.query_tokenizer.tensorize(queries)\n",
    "#         return self.query(input_ids, attention_mask)\n",
    "\n",
    "#     def docFromText(self, docs, bsize=None, keep_dims=True, to_cpu=False):\n",
    "#         if bsize:\n",
    "#             batches, reverse_indices = self.doc_tokenizer.tensorize(docs, bsize=bsize)\n",
    "\n",
    "#             batches = [self.doc(input_ids, attention_mask, keep_dims=keep_dims, to_cpu=to_cpu)\n",
    "#                        for input_ids, attention_mask in batches]\n",
    "\n",
    "#             if keep_dims:\n",
    "#                 D = _stack_3D_tensors(batches)\n",
    "#                 return D[reverse_indices]\n",
    "\n",
    "#             D = [d for batch in batches for d in batch]\n",
    "#             return [D[idx] for idx in reverse_indices.tolist()]\n",
    "\n",
    "#         input_ids, attention_mask = self.doc_tokenizer.tensorize(docs)\n",
    "#         return self.doc(input_ids, attention_mask, keep_dims=keep_dims)\n",
    "\n",
    "#     def score(self, Q, D, mask=None, lengths=None, explain=False):\n",
    "#         if lengths is not None:\n",
    "#             assert mask is None, \"don't supply both mask and lengths\"\n",
    "\n",
    "#             mask = torch.arange(D.size(1), device=DEVICE) + 1\n",
    "#             mask = mask.unsqueeze(0) <= lengths.to(DEVICE).unsqueeze(-1)\n",
    "\n",
    "#         scores = (D @ Q)\n",
    "#         scores = scores if mask is None else scores * mask.unsqueeze(-1)\n",
    "#         scores = scores.max(1)\n",
    "\n",
    "#         if explain:\n",
    "#             assert False, \"TODO\"\n",
    "\n",
    "#         return scores.values.sum(-1).cpu()\n",
    "\n",
    "\n",
    "# def _stack_3D_tensors(groups):\n",
    "#     bsize = sum([x.size(0) for x in groups])\n",
    "#     maxlen = max([x.size(1) for x in groups])\n",
    "#     hdim = groups[0].size(2)\n",
    "\n",
    "#     output = torch.zeros(bsize, maxlen, hdim, device=groups[0].device, dtype=groups[0].dtype)\n",
    "\n",
    "#     offset = 0\n",
    "#     for x in groups:\n",
    "#         endpos = offset + x.size(0)\n",
    "#         output[offset:endpos, :x.size(1)] = x\n",
    "#         offset = endpos\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cbd2e-3965-478c-bed9-0d5bcbd566b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_retrieve(args, queries):\n",
    "#     assert args.retrieve_only, \"TODO: Combine batch (multi-qu+ery) retrieval with batch re-ranking\"\n",
    "\n",
    "#     # faiss_index = FaissIndex(args.index_path, args.faiss_index_path, args.nprobe, args.part_range)\n",
    "#     inference = ModelInference(args.colbert, amp=args.amp)\n",
    "\n",
    "#     # ranking_logger = RankingLogger(Run.path, qrels=None)\n",
    "\n",
    "#     # with ranking_logger.context('unordered.tsv', also_save_annotations=False) as rlogger:\n",
    "#     with torch.no_grad() :\n",
    "#         # queries = args.queries\n",
    "#         qids_in_order = queries #list(queries.keys())\n",
    "\n",
    "#         for qoffset, qbatch in batch(qids_in_order, 100_000, provide_offset=True):\n",
    "#             qbatch_text = [queries[qid] for qid in qbatch]\n",
    "\n",
    "#             print_message(f\"#> Embedding {len(qbatch_text)} queries in parallel...\")\n",
    "#             Q = inference.queryFromText(qbatch_text, bsize=16)\n",
    "\n",
    "#             print_message(\"#> Starting batch retrieval...\")\n",
    "#             all_pids = faiss_index.retrieve(args.faiss_depth, Q, verbose=True)\n",
    "\n",
    "#             # Log the PIDs with rank -1 for all\n",
    "#             for query_idx, (qid, ranking) in enumerate(zip(qbatch, all_pids)):\n",
    "#                 query_idx = qoffset + query_idx\n",
    "\n",
    "#                 if query_idx % 1000 == 0:\n",
    "#                     print_message(f\"#> Logging query #{query_idx} (qid {qid}) now...\")\n",
    "\n",
    "#                 ranking = [(None, pid, None) for pid in ranking]\n",
    "#                 rlogger.log(qid, ranking, is_ranked=False)\n",
    "\n",
    "#     print('\\n\\n')\n",
    "#     print(ranking_logger.filename)\n",
    "#     print(\"#> Done.\")\n",
    "#     print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e666e1e-3768-4b18-8578-ac15d102edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tokenizer = QueryTokenizer()\n",
    "d_tokenizer = DocTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ee9cec-83b6-4277-86cf-14aad71c8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def query(self, input_ids, attention_mask=None, token_type_ids=None): -> Q\n",
    "# def doc(self, input_ids, attention_mask=None, token_type_ids=None): -> D\n",
    "# def score(self, Q, D):\n",
    "\n",
    "def get_p_embs(corpus, colbert_encoder):\n",
    "    with torch.no_grad() :\n",
    "        colbert_encoder.eval()\n",
    "\n",
    "        p_embs = []\n",
    "        for p in tqdm(corpus) :\n",
    "            # p = tokenizer(p, padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "            # p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "            p_emb = colbert_encoder.docs(d_tokenizer.tensorize(p))\n",
    "            p_embs.append(p_emb)\n",
    "    p_embs = torch.Tensor(p_embs).squeeze()  \n",
    "    return p_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67640e-4329-4877-830a-f73c9687642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# file_path = '/opt/ml/custom/passage_embedding.bin'\n",
    "# with open(file_path, 'wb') as file :\n",
    "#     pickle.dump(p_embs, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98075315-3d39-4cd8-8f4a-75c5ae5c2522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "corpus = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ")  # set 은 매번 순서가 바뀌므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f4353-5946-4d54-aba7-3af5487f355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('/opt/ml/custom/passage_embedding.bin', 'rb') as file :\n",
    "#     p_embs = pickle.load(file)\n",
    "# p_embs = p_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b5ef17-f615-450c-a1d4-d7c0cbddc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "colbert_encoder = torch.load('/opt/ml/models/colbert_encoder.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c3507-1dd4-4674-a38b-ccf4f03a70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_embs = get_p_embs(corpus, colbert_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996c46c-e038-484c-bc9c-32c6eb28665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def query(self, input_ids, attention_mask=None, token_type_ids=None): -> Q\n",
    "# def doc(self, input_ids, attention_mask=None, token_type_ids=None): -> D\n",
    "# def score(self, Q, D):\n",
    "    \n",
    "def get_relavant_doc(queries, colbert_encoder, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        colbert_encoder.eval()\n",
    "        # query (input_id, attention_mask) -> Q emb\n",
    "        q_emb = colbert_encoder.query(q_tokenizer.tensorize(queries))\n",
    "        \n",
    "        # q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device) \n",
    "        # q_emb = q_encoder(**q_seqs_val).to('cpu') # \n",
    "        \n",
    "    # dot_prod_scores = colbert_encoder.score.mm(q_emb, p_embs.T)\n",
    "    dot_prod_scores = colbert_encoder.score(q_emb, p_embs)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i][:k])\n",
    "        result_indices.append(ranks[i][:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb3166-ee1e-43af-b377-fd0fb07242f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/data/train_dataset')\n",
    "# dataset = load_from_disk('/opt/ml/data/test_dataset')\n",
    "\n",
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], colbert_encoder, k = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec08139-c839-4e68-8c44-a3353cca0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde18b2a-828d-4695-b5e0-8ec8e6614c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_length = []\n",
    "for i in range(len(cqas)) :\n",
    "    if cqas['original_context'][i] in cqas['context'][i] :\n",
    "        correct_length.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb941e2-4093-4e1b-b588-0d4b1863eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(correct_length) / len(dataset['validation']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6370f-39a6-488f-b66c-da8492324b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86211c-ead6-4c32-81cf-5126c41de2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
