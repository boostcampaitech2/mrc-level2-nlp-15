{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mimicking ChrisMcCormick's Finetuning BERT on SQUAD v1.1 dataset. Please refer to original notebook here: \n",
    "https://colab.research.google.com/drive/16VjEulbATgok4mELTSaq7GTQdh3JGhGy#scrollTo=Xm1wTn09RAR7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from Baseline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from typing import List, Callable, NoReturn, NewType, Any\n",
    "import dataclasses\n",
    "from datasets import load_metric, load_from_disk, Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "from baseline.utils_qa import postprocess_qa_predictions, check_no_error\n",
    "from baseline.trainer_qa import QuestionAnsweringTrainer\n",
    "from baseline.retrieval import SparseRetrieval\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Read config.yaml file\n",
    "with open(\"config.yaml\") as infile:\n",
    "    SAVED_CFG = yaml.load(infile, Loader=yaml.FullLoader)\n",
    "    SAVED_CFG = dotdict(SAVED_CFG)\n",
    "\n",
    "# arguments setting\n",
    "data_args = dotdict(SAVED_CFG.data)\n",
    "model_args = dotdict(SAVED_CFG.custom_model)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # output directory\n",
    "    save_total_limit=5,  # number of total save model.\n",
    "    save_steps=model_args.save_steps,  # model saving step.\n",
    "    num_train_epochs=model_args.num_train_epochs,  # total number of training epochs\n",
    "    learning_rate=model_args.learning_rate,  # learning_rate\n",
    "    per_device_train_batch_size=model_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=model_args.batch_size,  # batch size for evaluation\n",
    "    warmup_steps=model_args.warmup_steps,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=model_args.weight_decay,  # strength of weight decay\n",
    "    logging_dir=\"./logs\",  # directory for storing logs\n",
    "    logging_steps=100,  # log saving step.\n",
    "    evaluation_strategy=\"steps\",  # evaluation strategy to adopt during training\n",
    "    # `no`: No evaluation during training.\n",
    "    # `steps`: Evaluate every `eval_steps`.\n",
    "    # `epoch`: Evaluate every end of epoch.\n",
    "    eval_steps=500,  # evaluation step.\n",
    "    load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW,AutoTokenizer, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from datasets import load_metric, load_from_disk, Dataset, DatasetDict\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "datasets = load_from_disk(data_args.dataset_name)\n",
    "train_dataset = datasets['train']\n",
    "valid_dataset = datasets['validation']\n",
    "#train dataset, train dataloader\n",
    "q_seqs = tokenizer(\n",
    "    train_dataset['question'], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,  # for RoBERTa\n",
    "    )\n",
    "p_seqs = tokenizer(\n",
    "    train_dataset['context'], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,  # for RoBERTa\n",
    "    )\n",
    "# print(q_seqs[0])\n",
    "train_dataset = TensorDataset(\n",
    "    p_seqs['input_ids'], \n",
    "    p_seqs['attention_mask'], \n",
    "    # p_seqs['token_type_ids'],\n",
    "    q_seqs['input_ids'], \n",
    "    q_seqs['attention_mask'], \n",
    "    # q_seqs['token_type_ids']\n",
    "    )\n",
    "train_loader = DataLoader(train_dataset,batch_size=model_args.batch_size)\n",
    "\n",
    "#valid dataset, valid dataloader\n",
    "q_seqs = tokenizer(\n",
    "    valid_dataset['question'], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,  # for RoBERTa\n",
    "    )\n",
    "p_seqs = tokenizer(\n",
    "    valid_dataset['context'], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,  # for RoBERTa\n",
    "    )\n",
    "# print(q_seqs[0])\n",
    "valid_dataset = TensorDataset(\n",
    "    p_seqs['input_ids'], \n",
    "    p_seqs['attention_mask'], \n",
    "    # p_seqs['token_type_ids'],\n",
    "    q_seqs['input_ids'], \n",
    "    q_seqs['attention_mask'], \n",
    "    # q_seqs['token_type_ids']\n",
    "    )\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=model_args.batch_size\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from Fine_Tune_BERT_on_SQuAD_v1_1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
