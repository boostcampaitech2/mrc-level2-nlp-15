{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2021)\n",
    "torch.cuda.manual_seed(2021)\n",
    "np.random.seed(2021)\n",
    "random.seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=load_from_disk(\"../data/train_dataset/\")\n",
    "train_dataset,valid_dataset=datasets['train'],datasets['validation']\n",
    "\n",
    "model_name=\"klue/bert-base\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_seqs = tokenizer(train_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(train_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_seqs = tokenizer(valid_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(valid_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "valid_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'],\n",
    "                                q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BertEncoder, self).__init__(config)\n",
    "\n",
    "    self.bert = BertModel(config)\n",
    "    self.init_weights()\n",
    "      \n",
    "  def forward(self, input_ids, \n",
    "              attention_mask=None, token_type_ids=None): \n",
    "  \n",
    "      outputs = self.bert(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids)\n",
    "      \n",
    "      pooled_output = outputs[1]\n",
    "\n",
    "      return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAvgEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertAvgEncoder,self).__init__(config)\n",
    "        self.bert=BertModel(config)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs=self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        valid_length=torch.sum(attention_mask,dim=-1)\n",
    "        valid_length=valid_length.unsqueeze(dim=-1)\n",
    "\n",
    "        sum_hidden=torch.sum(outputs[0],dim=1)\n",
    "        #sum_hidden=torch.sum(outputs.last_hidden_state,dim=1)\n",
    "        avg_output=sum_hidden/valid_length\n",
    "\n",
    "        return avg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertAvgEncoder: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertAvgEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertAvgEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertAvgEncoder: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertAvgEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertAvgEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "p_encoder=BertAvgEncoder.from_pretrained(model_name)\n",
    "q_encoder=BertAvgEncoder.from_pretrained(model_name)\n",
    "if torch.cuda.is_available():\n",
    "    p_encoder.cuda()\n",
    "    q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\" ,\"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
    "    {\"params\": [p for n, p in p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    {\"params\": [p for n, p in q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
    "    {\"params\": [p for n, p in q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "]\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=5e-5,\n",
    "    # eps=args.adam_epsilon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=20)\n",
    "valid_loader=DataLoader(valid_dataset,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proprecessing(text):\n",
    "    new_text = text.replace(r'\\n\\n','')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_embedding(dataset):\n",
    "    \"\"\"문맥의 임베딩 값을 구하고리턴합니다.\"\"\"\n",
    "    q_seqs = tokenizer(dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "    p_seqs = tokenizer(dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "    # print(q_seqs[0])\n",
    "    dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'],\n",
    "                                    q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])\n",
    "    dataloader = DataLoader(dataset,batch_size=20)\n",
    "    print(\"Build passage embedding\")\n",
    "    for idx, data in enumerate(tqdm(dataloader)):\n",
    "        p_inputs = {'input_ids': data[0].to('cuda'),\n",
    "                    'attention_mask': data[1].to('cuda'),\n",
    "                    'token_type_ids': data[2].to('cuda')\n",
    "                    }\n",
    "        with torch.no_grad():\n",
    "            p_inputs = {k: v for k, v in p_inputs.items()}\n",
    "            output = p_encoder(**p_inputs)\n",
    "            if idx == 0:\n",
    "                p_embedding = output\n",
    "            else:\n",
    "                p_embedding = torch.cat((p_embedding, output), 0)\n",
    "    return p_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 100/198 [02:00<01:58,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0epoch loss: 143.64504294395448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [03:59<00:00,  1.21s/it]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build passage embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.40it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  5.02it/s]\n",
      "  0%|          | 0/198 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc :0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 100/198 [02:00<01:58,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1epoch loss: 7.396877980741895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [03:59<00:00,  1.21s/it]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build passage embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.40it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  5.01it/s]\n",
      "  0%|          | 0/198 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc :0.12083333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 100/198 [02:01<01:58,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2epoch loss: 4.549121269368567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [03:59<00:00,  1.21s/it]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build passage embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.40it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  5.01it/s]\n",
      "  0%|          | 0/198 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc :0.10833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 100/198 [02:01<01:58,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3epoch loss: 4.4408785168706775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [03:59<00:00,  1.21s/it]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build passage embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.40it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  5.01it/s]\n",
      "  0%|          | 0/198 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc :0.13333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 100/198 [02:01<01:58,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4epoch loss: 3.845984483085878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [03:59<00:00,  1.21s/it]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build passage embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.39it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc :0.10416666666666667\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/198 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build passage embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:39<00:00,  5.00it/s]\n"
     ]
    }
   ],
   "source": [
    "best = 0\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "train_dataset,valid_dataset=datasets['train'],datasets['validation']\n",
    "\n",
    "train_datasets = {}\n",
    "train_datasets['context'] = [proprecessing(string) for string in train_dataset['context']]\n",
    "train_datasets['question'] = train_dataset['question']\n",
    "valid_datasets = {}\n",
    "valid_datasets['context'] = [proprecessing(string) for string in valid_dataset['context']]\n",
    "valid_datasets['question'] = valid_dataset['question']\n",
    "for epoch in range(5):\n",
    "    step = 0\n",
    "    losses = 0\n",
    "    for idx,data in enumerate(tqdm(train_loader)):\n",
    "        step += 1\n",
    "        p_inputs = {'input_ids': data[0].to(device),\n",
    "                    'attention_mask': data[1].to(device),\n",
    "                    'token_type_ids': data[2].to(device)\n",
    "                    }\n",
    "        q_inputs = {'input_ids': data[3].to(device),\n",
    "                    'attention_mask': data[4].to(device),\n",
    "                    'token_type_ids': data[5].to(device)}\n",
    "        targets = torch.arange(0, len(p_inputs['input_ids'])).long().to(device)\n",
    "        q_output = q_encoder(**q_inputs)\n",
    "        p_output = p_encoder(**p_inputs)\n",
    "        retrieval = torch.matmul(q_output,p_output.T)\n",
    "        retrieval_scores = F.log_softmax(retrieval, dim=1)\n",
    "\n",
    "        loss = F.nll_loss(retrieval_scores, targets)\n",
    "        losses += loss.item()\n",
    "        # q_encoder.zero_grad()\n",
    "        p_encoder.zero_grad()\n",
    "        q_encoder.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        if step % 100 == 0:\n",
    "            print(f'{epoch}epoch loss: {losses/(step)}')\n",
    "\n",
    "            losses = 0\n",
    "            correct = 0\n",
    "            step = 0\n",
    "    optimizer.lr =5e-6\n",
    "    valid_embedding = dense_embedding(valid_datasets)\n",
    "    valid_ans = []\n",
    "    for idx,data in enumerate(tqdm(valid_loader)):\n",
    "        with torch.no_grad():\n",
    "            q_inputs = {'input_ids': data[3].to(device),\n",
    "                        'attention_mask': data[4].to(device),\n",
    "                        'token_type_ids': data[5].to(device)}\n",
    "            output = p_encoder(**q_inputs)\n",
    "            if idx == 0:\n",
    "                query_vec = output\n",
    "            else:\n",
    "                query_vec = torch.cat((query_vec, output), 0)\n",
    "\n",
    "    result = torch.mm(query_vec, valid_embedding.T)\n",
    "    result = result.cpu().detach().numpy()\n",
    "    doc_indices = []\n",
    "    for i in range(result.shape[0]):\n",
    "        sorted_result = np.argsort(result[i, :])[::-1]\n",
    "        doc_indices.append(sorted_result.tolist()[:3])\n",
    "    correct = 0\n",
    "    for idx,i in enumerate(doc_indices):\n",
    "        if idx in i:\n",
    "            correct += 1\n",
    "    print(f'acc :{correct/len(valid_dataset)}')\n",
    "print('save')\n",
    "torch.save(p_encoder.state_dict(), 'gold/p_encoder.pt')\n",
    "torch.save(q_encoder.state_dict(), 'gold/q_encoder.pt')\n",
    "p_embedding = dense_embedding(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['처음으로 부실 경영인에 대한 보상 선고를 받은 회사는?',\n",
       " '스카버러 남쪽과 코보콘그 마을의 철도 노선이 처음 연장된 연도는?',\n",
       " '촌락에서 운영 위원 후보자 이름을 쓰기위해 사용된 것은?',\n",
       " '로타이르가 백조를 구하기 위해 사용한 것은?',\n",
       " '의견을 자유롭게 나누는 것은 조직 내 어떤 관계에서 가능한가?',\n",
       " '1945년 쇼와천황의 항복 선언이 발표된 라디오 방송은?',\n",
       " '징금수는 서양 자수의 어떤 기법과 같은 기술을 사용하는가?',\n",
       " '다른 과 의사들은 감염내과 전문의들로부터 어떤 것에 대해 조언을 받는가?',\n",
       " '루이 14세의 왕비 마리아 테래사는 어느 나라 공주인가?',\n",
       " '헤자즈 왕국이 실존했던 것은 언제까지인가?',\n",
       " '버드 교장이 5월의 여왕의 대안으로 제시한 것은?',\n",
       " \"인형사'를 만들어낸 것으로 추측되는 사업의 이름은?\",\n",
       " '멘데스가 요원들을 구하기 위해 간 도시는 어디인가?',\n",
       " '교과부의 행동에 화가나 여러명이 사직한 기구의 이름은?',\n",
       " '반대동맹이 공산당과 갈라서겠다고 얘기한 날은 언제인가?',\n",
       " '피어슨이 다시 의회를 해산했던 년도는?',\n",
       " '몽케가 죽은 뒤 쿠릴타이에서 대칸의 지위를 얻은 사람의 이름은?',\n",
       " '이흥구의 사법시험 이야기를 기사로 작성한 곳은?',\n",
       " '남북조 시대에서 이이 씨가 전쟁이 발생했을 때, 생활했던 장소는?',\n",
       " '박지훈은 1라운드에서 몇 순위를 차지했는가?',\n",
       " '데메카론에는 무엇을 풍자하는 이야기가 들어있나요?',\n",
       " '병에 걸려 죽을 확률이 약 25~50%에 달하는 유형의 질병은?',\n",
       " '설리반이 불만을 표시한 대상은 누구인가?',\n",
       " '베소스는 어디서 추방당했는가?',\n",
       " '진전사의 명칭이 드러나는 데 영향을 준 물건은?',\n",
       " '자신의 이상적인 국가관이 스파르타와 닮아 있다고 생각하는 플라톤의 저서는?',\n",
       " '박제된 북극곰이 사망한 날짜는?',\n",
       " '문법 측면에서 더 보수적인 포르투갈어 표준은?',\n",
       " '로스 수장이 살해한 사람은 어느 당 회원인가?',\n",
       " '조경숙왕의 아들인 요자의 친어머니는 누구인가?',\n",
       " '오래플린과 부스의 마지막 계획에 따르면 그들은 어디서 링컨을 납치하려고 했는가?',\n",
       " '김득황이 친일파로 취급되었던 것은 무슨 경력 때문인가?',\n",
       " '레닌이 출간한 책 중 농민의 자발적 참여에 대한 내용이 포함되어있는 것은?',\n",
       " '신란의 동반자가 죽었다고 전해지는 지역은?',\n",
       " '칭자오의 머리가 엄청난 위력을 발휘할 수 없게 된 것은 누구 때문인가?',\n",
       " '해초나 조류 표면에서 자라는 유기체 중 가장 비율이 높은 것은?',\n",
       " '류한욱이 두 번째 뇌출혈로 쓰러진 공간은?',\n",
       " '로마의 공성무기에 대한 기록을 남긴 사람은?',\n",
       " '동반자 등록제를 최초로 실시한 중국의 도시는?',\n",
       " '장대호가 사용한 흉기는?',\n",
       " '스뮈츠에게 학비를 지원해 준 사람은?',\n",
       " '브루투스가 세운도시의 현재 이름은?',\n",
       " '일본의 대학 입시는 며칠간 진행되는가?',\n",
       " '국내 화엄종의 선구자는 누구인가?',\n",
       " '다케다 노부히로가 통치한 지역은 어디인가?',\n",
       " '둥근 해자를 건너는 다리 난간에는 어떤 신화의 내용이 새겨져 있나?',\n",
       " '우나동의 주요 식재료는?',\n",
       " '제2차 세계 대전 이후 동부 갈리치아 지방은 누구에게 지배를 받았는가?',\n",
       " '치환과 결합되어 파이스텔 암호 사용을 가능케 하는 것은?',\n",
       " '적색육을 지칭하는 또 다른 이름은?',\n",
       " '정민과 이별한 이후 옥림의 매력에 마음을 빼앗겨버린 인물은?',\n",
       " '슈트레제만이 이끌었던 당은 무엇인가?',\n",
       " '닛폰 제지 시라오이 공장과 가까운 역은?',\n",
       " '벽에 천녀를 그리기 전에 하는 밑작업은?',\n",
       " '합천에서 나루터 역할을 대신하고 있는 것은?',\n",
       " '독일의 취업자들이 주로 기술을 습득한 방법은?',\n",
       " '김준연은 김구가 누구와 몰래 교류하고 있다고 주장했나요?',\n",
       " '당산제를 시행하던 날은 언제였는가?',\n",
       " '육조혜능의 이름을 모방한 불교의 종파는?',\n",
       " '더우징퉁의 부모가 결혼한 곳은 어디인가?',\n",
       " '벨기에가 리에주 전투에서 맞서 싸운 상대방은?',\n",
       " '아킬레 라티가 추기경으로 추대된 해는?',\n",
       " '제나라가 쇠약해진 시기는?',\n",
       " '갈라파고스 제도의 9월 날씨에 무엇이 계속 오는가?',\n",
       " '진행 천궁도의 가치는 무엇의 형태에 영향을 받나요?',\n",
       " '서울 주변에서 자주 볼 수 없는 불상은?',\n",
       " '교황 비오 11세가 사망한 연도는?',\n",
       " '닭의 관절 재생을 유도할 수 있는 재생 기술은?',\n",
       " '엘디 서쪽 리틀강에 위차하는 마을은?',\n",
       " '몽양 여운형의 아버지 이름은 무엇인가요?',\n",
       " '부처의 몸에서 뿜어져 나오는 빛을 형상화한 것은 무엇인가?',\n",
       " '2012년 당시 곽태휘의 소속팀은?',\n",
       " '교황의 문장에서 교차한 금빛 열쇠와 은빛 열쇠가 뜻하는 바는?',\n",
       " '돌을 캐고 난 빈 채석장은 어떠한 용도로 이용하였나?',\n",
       " '러셀의 여자 친구의 종교는?',\n",
       " '쿠데타를 위해 초대된 나폴레옹의 수하는?',\n",
       " '엠넷미디어와 YG엔터테인먼트는 각자 상대방의 무엇을 얻어 협력관계를 맺었는가?',\n",
       " '할더가 히틀러의 공격안을 반대한 결과는?',\n",
       " '대한민국 민법에서 채권의 권리가 누구에게 있으면 채권이 사라지지 않는다고 명시하는가?',\n",
       " '이베이의 계산 방법으로 아디옌이 많이 보급될수록 필요성이 없어지는 결제 방법은?',\n",
       " '일본 프로 야구에서 처음으로 가쿠사다마를 사용한 선수는?',\n",
       " '수를 놓을 때에 사용하는 기법 중 아주 작은 점으로 표현하는 방식은?',\n",
       " '각룡류 중에서 가장 짧은 프릴을 가지고 있는 공룡은?',\n",
       " '러시앤캐시는 광고를 통해 어떤 회사라는 이미지에서 탈피하고 싶어하는가?',\n",
       " '1954년 샤넬이 프랑스로 돌아오기 전 머무르던 국가는?',\n",
       " '경제학의 어떤 개념을 통해 개방경지제의 공동 방목지가 능률이 떨어지는 이유를 이해할 수 있는가?',\n",
       " '쿠에바가 생각하는 자본주의 체제는?',\n",
       " '와그너가 국내 건강보험과 민생을 위해 입법하는 것에 열정적으로 지지했던 인물은?',\n",
       " '무위의 오랑캐들의 횡포로 길이 끊긴 사실을 소칙에게 알린 사람의 관직은?',\n",
       " '요미우리가 먼저 에가와를 지명할 수 없게 만든 구단은?',\n",
       " '충주 조동리유적의 존재 사실을 언제 알게 되었는가?',\n",
       " '뱌쿠롄과 레이무 일행은 어떤 철학 때문에 싸우게 되었나?',\n",
       " '견훤에게 살해당한 인물과 함께 태조를 대접한 왕은?',\n",
       " '인도 총리에게 말라리아와 루푸스 치료제를 보내달라고 요구한 인물은?',\n",
       " '1831년 조선 대목구의 첫 주교 직위를 맡은 사람은?',\n",
       " '탱화는 어느 종교와 관련이 있는가?',\n",
       " '천체 관측과 동시에 고도와 방위각을 알 수 있는 망원경은?',\n",
       " '철도보다 자가용 이용이 많아지면서, 히사쓰 오렌지 철도 노선의 대항마로 부상한 것은?',\n",
       " '태양이 대부분 철로 이루어지지 않음을 발견한 인물은?',\n",
       " '클라이언트이자 서버인 경우가 많아지는 상황에서 변화가 생기고 있는 환경은?',\n",
       " '캐서린 스윈포드의 언니의 직업은?',\n",
       " '메테스키가 최초로 폭탄을 설치하였던 지역은?',\n",
       " '시노하라에 의하면 아인슈타인은 무엇으로 원자폭탄 제작에 도움을 주었는가?',\n",
       " '주이로부터 문흠의 가짜 항복 소식을 들은 인물은?',\n",
       " '콘스탄스를 죽인 것은 누구인가?',\n",
       " '댈러스 이전에 법무장관 직을 거부한 인물은 어디 출신인가요?',\n",
       " '로얄 살롱에 부착된 후면 번호판의 기존 위치는 어디인가?',\n",
       " '카르가 반대입장으로 돌아서자 반란세력이 진행한 것은?',\n",
       " '팔라바국이 9세기 초까지 텔랑가나와 타밀나두 북쪽 지역을 다스린 기간은?',\n",
       " '어느정도 규모 이상의 대회에서 깃털 셔틀콕만을 사용하는 이유는?',\n",
       " '한일생명 축구단의 감독에게 감사패를 준 축제가 열린 해는?',\n",
       " '교황에 의해 새롭게 토지 개발된 최초의 관리지역은?',\n",
       " '2003년 세계 태권도 선수권 대회가 개최되었던 장소는?',\n",
       " '슘페터는 경제적 이윤도 전혀 발생시키지 않고 아무 발전이 없는 사회를 무엇이라 정의했나?',\n",
       " '트레이 파커가 목소리를 맡은 등장인물의 자식은 누구인가?',\n",
       " '뒤러는 어디에 코뿔소를 그렸는가?',\n",
       " '받침 굽에 새겨져 있는 글자의 수는?',\n",
       " '상사병으로 사망한 소녀의 장례식이 열린 곳은 어디인가?',\n",
       " '러시아에 있는 다량의 천연자원을 약탈하려했던 계획은?',\n",
       " '냉대 동계 소우 기후의 w는 무엇의 약자인가?',\n",
       " '이인국 박사가 석방되는 결과를 가져온 병은?',\n",
       " '생산물 중 다른 물건을 만들기 위해 사용되는 것은?',\n",
       " '동기부여적 설명이 효과가 있는지 알아내기 위해 현재 연구되고 있는 것은 무엇인가?',\n",
       " '코로나를 발생시키는 것은?',\n",
       " '애니미즘만을 믿던 세부아노족들에게 전파된 외부 종교는?',\n",
       " '아이오와주에 수많은 산업체들이 들어오기 시작한 것은 언제인가?',\n",
       " '제10군단의 아퀼리페르가 아퀼라를 든 채 배 밖으로 뛰어들었을 당시, 카이사르군대가 두려워한 대상은?',\n",
       " '《악마는 프라다를 입는다》은 어느 국가의 영화인가요?',\n",
       " '161개의 집터 근처에서 발견된 석실묘의 개수는?',\n",
       " '송나라가 돈을 지불하여 피하고자 했던 것은?',\n",
       " '마이어의 사인은?',\n",
       " '장면을 가톨릭 성자로 추앙하려 했던 사람은?',\n",
       " '김준연은 김옥성과 몇 년간 함께 살았나요?',\n",
       " '석탄기 다음 시대는?',\n",
       " '돈대에서 무너져내린 포좌부분이 있는 위치는?',\n",
       " '기렌 총수가 살해당한 직후에 파괴된 군함은?',\n",
       " '크레센티우스 2세가 받은 형벌은?',\n",
       " '왕필과 함께 대표적인 현학자로 불리며 《장자》에 주석을 단 사람은?',\n",
       " '요시카와 하루오의 어린시절 역을 맡은 성우는?',\n",
       " '반바리사이파의 반발을 불러일으킨 원인은 누구의 사망이었나요?',\n",
       " '외국어영화상 위원회에서 최종 후보 다섯 편을 추리는 방법은?',\n",
       " '쿠데타가 있었던 회차는?',\n",
       " '스즈키 증장이 전사할 당시에 타고 있던 것은?',\n",
       " '전자부품에 특화된 종합물류서비스를 제공하는 기업의 모회사는?',\n",
       " '네이딘의 하나뿐인 친구는?',\n",
       " '바이마르 정부가 화폐를 수없이 찍어낸 이유는?',\n",
       " '본국 의회가 제정한 선언법은 식민지와 관련하여 무엇을 의미하는가?',\n",
       " '효율적인 운영을 도와주는 열차선 배치 방법은?',\n",
       " '쓰치야 히로시가 마지막으로 활동한 해는?',\n",
       " '어피인장함의 뒷면에 붙은 경첩은 무엇의 형태인가?',\n",
       " '예루살렘의 공방전이 지속된 기간은?',\n",
       " '길버트 J. 체크 중령이 7월 23일 저녁에 공격을 막기 위해 준비한 곳은 어디 주변이었나?',\n",
       " '조선의 민족대표들이 조선이 독립국임을 선언한 곳은?',\n",
       " 'Mr.9은 라분을 무엇으로 사용하려고 했는가?',\n",
       " '1881년 리비에르의 목적지는?',\n",
       " '거북이가 더 나쁘다고 하는 사람들이 문제삼는 것은?',\n",
       " '비슷한 양의 골편이 두종류가 발견되는것으로 알 수 있는 A.수페르스테스의 몸의 형태는?',\n",
       " '성당의 서쪽과 남쪽 부분에 난방이 가능해진 시기는?',\n",
       " '빔비사라가 왕위에 오른 뒤 마가다가 지배권을 얻은 도시는 어디인가?',\n",
       " '1835년 선출된 텍사스 지원군 지휘관이 제임스 보이에게 임명한 직책은?',\n",
       " '2011년이후 세력이 약해진 조직은?',\n",
       " '확증편향이라는 단어를 처음으로 인용한 것은 누구인가?',\n",
       " '입자들의 속도가 줄 때, 방출되는 에너지가 만들어내는 것은?',\n",
       " '10월 1일 북진으로 압록강 근처까지 전쟁 지역을 이르게 한 부대는?',\n",
       " '캠벨에게 원주민 성직자 양성의 중요성을 일깨워준 사람은?',\n",
       " '학생들이 선생님과 같은 열정과 에너지를 가지게 되는 것은 무엇을 통해서인가?',\n",
       " '토큰링이 빠른 속도로 퇴보하는데 영향을 준 기술은?',\n",
       " '중고령자의 채취업 활성화로 경제 주체마저 옮겨가는 바람에 생기는 부작용 중 하나는?',\n",
       " '1901년에 연기로 인해 타격을 받아 황폐한 마을이 되어버린 곳은?',\n",
       " '조현병은 무엇으로 오해하기 쉬운가?',\n",
       " '탄수화물의 최종적 분해가 이루어지는 곳은?',\n",
       " '수행에 대해 근본적으로 의문을 제기함으로써 영적 수행자의 수행 의지를 갉아먹는 것은 무엇인가?',\n",
       " '알베르가 에마뉘엘을 \"성 에마뉘엘\"로 부른 것은 무엇 때문인가?',\n",
       " '네거티브 광고를 할 때 기업은 법적인 문제 외에 또 무엇을 생각해야 하는가?',\n",
       " '아길루프의 요구에 의해 보니파시오 4세에게 서신을 쓴 사람은?',\n",
       " '나오치카는 주살의 위험을 피해 누구의 영지로 도망갔나요?',\n",
       " '기타 케이스에 기타 외의 물건이 있는 인물은 어떤 역할 인가요?',\n",
       " '《탄이초》의 작자에 가장 적합한 인물로 알려진 것은?',\n",
       " '베르니츠 강과 라슈트 강이 만나는 곳 부근에서 코네티컷 연대와 만나 전투를 벌였던 프랑스 부대는 어디인가?',\n",
       " '면역력이 약해져서 아시클로버의 약효가 발휘되지 않을 때 쓰는 것은 무엇인가?',\n",
       " '동맹항에서의 교역에서 세금을 내지 않으려면 지참해야 하는 것은 무엇인가?',\n",
       " '동궁전 화재에서 인종을 구한 사람은 누구인가?',\n",
       " \"굿보이'에 대한 비판글이 올라온 곳은?\",\n",
       " '봉은사 영산전 석가불상의 색을 다시 칠한 시기는?',\n",
       " '코페르니쿠스의 지동설에 대한 내용으로 그의 사후에 발간된 책은?',\n",
       " '요나미네가 야쿠자 팬으로부터 협박을 받았던 도시는 어디였는가?',\n",
       " '박정희 전 대통령이 활주로가 있던 자리에 새로 지은 것은?',\n",
       " '행정청사 옥상으로 피신했던 직원들이 알고 있었던 쓰나미의 높이는?',\n",
       " '윤진학이 동아찬영회에 들어가기 전에 속해있었던 곳은?',\n",
       " '태양풍을 멈추게 하는 것과 말단 충격의 틈을 무엇이라 부르는가?',\n",
       " '여자가 안드레우치오의 디 피에트로에 대한 정보를 얻었던 인물은?',\n",
       " '<석탄형성에 관한 관찰 기록>은 무엇에 관한 내용인가?',\n",
       " '아이작 웨인의 아버지가 세상을 떠난 것은 몇 년도인가?',\n",
       " '타코벨과 달리 대한민국에서 많은 성과를 달성했던 업체는?',\n",
       " '8월 18일 대구역 근처에 박격포를 쏜 적군은 어디에서 침투했나?',\n",
       " '1988년에 영국에 있는 파커의 공장을 찾아온 정치인은?',\n",
       " '소프트웨어 공학이 이전과는 다른 분야로 나아가게 된 계기는?',\n",
       " '아카키우스 분열은 얼마나 유지됐는가?',\n",
       " '브라니미르의 뒤를 이어 크로아티아 공작의 직위를 승계받은 인물은?',\n",
       " '볼드윈이 \"당신들은 나를 야유합니까?\"라는 말을 한 연도는?',\n",
       " '대형 변압기 수송에 사용되는 전용선은?',\n",
       " '타지마할의 실제 구조를 밝히는데 도움을 준 것은?',\n",
       " '자기장의 강도가 높아짐에 따라 페르미 면 외부로 옮겨지는 것의 명칭은?',\n",
       " '나가시마의 은퇴경기는 무엇 때문에 미뤄졌나?',\n",
       " '그녀의 첫 작품이 데뷔한 곳은?',\n",
       " '송이 금에게 바친 공물은 은 30만량과 비단 몇 필인가?',\n",
       " '미래의 군사 능력은 무엇의 능력에 의해 결정되는가?',\n",
       " '카를로 말라테스타가 포로로 잡혀간 전쟁은?',\n",
       " '푸에르토리코의 미카엘 곤살레스 선수가 참가하고자 했던 동계올림픽의 종목은?',\n",
       " '마오를 직접적으로 죽인 사람은?',\n",
       " '워너가 자신의 회사를 매도했던 해는?',\n",
       " '조르댕의 딸은 누구를 좋아하는가?',\n",
       " '동도서기라는 구호를 사용한 나라에서 처음 서구화와 접한 시기는?',\n",
       " '극처럼 점단위 공격이 가능한 무기는?',\n",
       " '미국 남부 지방에서 급격하게 영국산 제품이 많이 팔린 계기가 된 조치는?',\n",
       " '샤이닝 폼을 무엇이라고 칭하기도 하나요?',\n",
       " '악단이 재정적으로 힘들 때 단원들은 어디서 돈을 벌었는가?',\n",
       " '정부의 성향이 진보에 가까울 때 사법적극주의를 띠는 법원은 어떤 성향을 갖나?',\n",
       " '블랙워치의 지도자가 게임에서 불리는 이름은 무엇인가?',\n",
       " '통합진보당 해산 청구에 관한 안건을 의결한 기구는?',\n",
       " '화순 춘산영당에 소장된 최익현의 초상화는 무엇에다 그린 것인가?',\n",
       " '제6회 전조선축구대회가 진행된 기간은?',\n",
       " '대제학을 지낼 수 있는 유일한 사람은?',\n",
       " '센니치마에 선의 관리권은 어디에 있나?',\n",
       " '배현경과 신숭겸 등이 일을 꾀하여 찾아간 인물의 부인은 누구인가?',\n",
       " '십자군 설교를 통해 집한한 자들은?',\n",
       " \"한국을 '형제의 나라'라고 칭한 나라는 어디인가요?\",\n",
       " '정상적인 조건에서 아라키돈 산이 염증을 일으키는데 필요한 물질은?',\n",
       " '페루치가 피사에서 한달을 지체한 이유는 무엇인가?',\n",
       " '갑훈에 의해 1천만여 전의 뇌물을 받았다는 사실이 밝혀진 인물은?',\n",
       " '성직자들의 실체에 대해 말한 용병의 기사는 누가 작성하였나요?',\n",
       " '러셀은 누구의 가족과 함께 1889년 파리 박람회에 갔는가?',\n",
       " '1표로 간주하기 위한 의견 수렴 방식은?',\n",
       " '데이비드 스튜어트는 누구의 해독방법을 다른 지협 문자 자료에 적용해보았나요?',\n",
       " '인적 성과와 더불어 기업의 성과를 평가하는 요소는?',\n",
       " '전단이 연나라와의 전쟁에서 승리했을 당시 제나라의 왕은 누구인가?',\n",
       " '공놀이 경기장 중 일부는 어디에 위치하고 있나?',\n",
       " '창씨개명령의 시행일을 미루는 것을 수락한 인물은?',\n",
       " '망코 잉카가 쿠스코를 되찾기 위해 마련한 군사는 총 몇 명인가?',\n",
       " '마르크스주의자들의 사상은?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['validation']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
