{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Training and Validation\n",
    "- For Performance and Hyperparameters Comparison, Please refer to [ðŸ”— wandb](https://wandb.ai/happyface-boostcamp/KLUE-QA?workspace=user-snoop2head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes, as dict.key_name, not as dict[\"key_name\"] \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'klue/roberta-large',\n",
       " 'save_steps': 100,\n",
       " 'num_train_epochs': 3,\n",
       " 'learning_rate': 2e-05,\n",
       " 'batch_size': 10,\n",
       " 'warmup_steps': 300,\n",
       " 'weight_decay': 0.01,\n",
       " 'validation': False,\n",
       " 'max_length': 512,\n",
       " 'DEBUG': True,\n",
       " 'num_rnn_layers': 2,\n",
       " 'num_folds': 4,\n",
       " 'gamma': 1.0,\n",
       " 'smoothing': 0.2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Read config.yaml file\n",
    "with open(\"config.yaml\") as infile:\n",
    "    SAVED_CFG = yaml.load(infile, Loader=yaml.FullLoader)\n",
    "    SAVED_CFG = dotdict(SAVED_CFG)\n",
    "\n",
    "# arguments setting\n",
    "data_args = dotdict(SAVED_CFG.data)\n",
    "model_args = dotdict(SAVED_CFG.custom_model)\n",
    "\n",
    "# adding additional arguments\n",
    "model_args.batch_size = 10\n",
    "model_args.num_rnn_layers = 2\n",
    "model_args.learning_rate = 2e-5\n",
    "model_args.num_folds = 4\n",
    "model_args.gamma = 1.0\n",
    "model_args.smoothing = 0.2\n",
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm-trainer-AdamW-CE-num_rnn_layers-2-no-clip\n"
     ]
    }
   ],
   "source": [
    "name_of_experiment = f'lstm-trainer-AdamW-CE-num_rnn_layers-{model_args.num_rnn_layers}-no-clip'\n",
    "print(name_of_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Custom Dataset \n",
    "**Remove Passages where Answers located > 512 token length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "# load dataset from the huggingface format dataset\n",
    "datasets = load_from_disk(data_args.dataset_name)\n",
    "train_dataset_from_huggingface = datasets['train']\n",
    "valid_dataset_from_huggingface = datasets['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ë¯¸êµ­ ìƒì›',\n",
       " 'context': 'ë¯¸êµ­ ìƒì˜ì› ë˜ëŠ” ë¯¸êµ­ ìƒì›(United States Senate)ì€ ì–‘ì›ì œì¸ ë¯¸êµ­ ì˜íšŒì˜ ìƒì›ì´ë‹¤.\\\\n\\\\në¯¸êµ­ ë¶€í†µë ¹ì´ ìƒì›ì˜ìž¥ì´ ëœë‹¤. ê° ì£¼ë‹¹ 2ëª…ì˜ ìƒì›ì˜ì›ì´ ì„ ì¶œë˜ì–´ 100ëª…ì˜ ìƒì›ì˜ì›ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìžˆë‹¤. ìž„ê¸°ëŠ” 6ë…„ì´ë©°, 2ë…„ë§ˆë‹¤ 50ê°œì£¼ ì¤‘ 1/3ì”© ìƒì›ì˜ì›ì„ ìƒˆë¡œ ì„ ì¶œí•˜ì—¬ ì—°ë°©ì— ë³´ë‚¸ë‹¤.\\\\n\\\\në¯¸êµ­ ìƒì›ì€ ë¯¸êµ­ í•˜ì›ê³¼ëŠ” ë‹¤ë¥´ê²Œ ë¯¸êµ­ ëŒ€í†µë ¹ì„ ìˆ˜ë°˜ìœ¼ë¡œ í•˜ëŠ” ë¯¸êµ­ ì—°ë°© í–‰ì •ë¶€ì— ê°ì¢… ë™ì˜ë¥¼ í•˜ëŠ” ê¸°ê´€ì´ë‹¤. í•˜ì›ì´ ì„¸ê¸ˆê³¼ ê²½ì œì— ëŒ€í•œ ê¶Œí•œ, ëŒ€í†µë ¹ì„ í¬í•¨í•œ ëŒ€ë‹¤ìˆ˜ì˜ ê³µë¬´ì›ì„ íŒŒë©´í•  ê¶Œí•œì„ ê°–ê³  ìžˆëŠ” êµ­ë¯¼ì„ ëŒ€í‘œí•˜ëŠ” ê¸°ê´€ì¸ ë°˜ë©´ ìƒì›ì€ ë¯¸êµ­ì˜ ì£¼ë¥¼ ëŒ€í‘œí•œë‹¤. ì¦‰ ìº˜ë¦¬í¬ë‹ˆì•„ì£¼, ì¼ë¦¬ë…¸ì´ì£¼ ê°™ì´ ì£¼ ì •ë¶€ì™€ ì£¼ ì˜íšŒë¥¼ ëŒ€í‘œí•˜ëŠ” ê¸°ê´€ì´ë‹¤. ê·¸ë¡œ ì¸í•˜ì—¬ êµ°ëŒ€ì˜ íŒŒë³‘, ê´€ë£Œì˜ ìž„ëª…ì— ëŒ€í•œ ë™ì˜, ì™¸êµ­ ì¡°ì•½ì— ëŒ€í•œ ìŠ¹ì¸ ë“± ì‹ ì†ì„ ìš”í•˜ëŠ” ê¶Œí•œì€ ëª¨ë‘ ìƒì›ì—ê²Œë§Œ ìžˆë‹¤. ê·¸ë¦¬ê³  í•˜ì›ì— ëŒ€í•œ ê²¬ì œ ì—­í• (í•˜ì›ì˜ ë²•ì•ˆì„ ê±°ë¶€í•  ê¶Œí•œ ë“±)ì„ ë‹´ë‹¹í•œë‹¤. 2ë…„ì˜ ìž„ê¸°ë¡œ ì¸í•˜ì—¬ ê¸‰ì§„ì ì¼ ìˆ˜ë°–ì— ì—†ëŠ” í•˜ì›ì€ ì§€ë‚˜ì¹˜ê²Œ ê¸‰ì§„ì ì¸ ë²•ì•ˆì„ ë§Œë“¤ê¸° ì‰½ë‹¤. ëŒ€í‘œì ì¸ ì˜ˆë¡œ ê±´ê°•ë³´í—˜ ê°œí˜ ë‹¹ì‹œ í•˜ì›ì´ ë¯¸êµ­ ì—°ë°© í–‰ì •ë¶€ì—ê²Œ í¼ë¸”ë¦­ ì˜µì…˜(ê³µê³µê±´ê°•ë³´í—˜ê¸°ê´€)ì˜ ì¡°í•­ì´ ìžˆëŠ” ë°˜ë©´ ìƒì›ì˜ ê²½ìš° í•˜ì›ì•ˆì´ ì§€ë‚˜ì¹˜ê²Œ ì„¸ê¸ˆì´ ë§Žì´ ë“ ë‹¤ëŠ” ì´ìœ ë¡œ í¼ë¸”ë¦­ ì˜µì…˜ ì¡°í•­ì„ ì œì™¸í•˜ê³  ë¹„ì˜ë¦¬ê±´ê°•ë³´í—˜ê¸°ê´€ì´ë‚˜ ë³´í—˜íšŒì‚¬ê°€ ë‹´ë‹¹í•˜ë„ë¡ í•œ ê²ƒì´ë‹¤. ì´ ê²½ìš°ì²˜ëŸ¼ ìƒì›ì€ í•˜ì›ì´ë‚˜ ë‚´ê°ì±…ìž„ì œê°€ ë¹ ì§€ê¸° ì‰¬ìš´ êµ­ê°€ë“¤ì˜ êµ­íšŒì²˜ëŸ¼ ê±¸í•í•˜ë©´ ë°œìƒí•˜ëŠ” ì˜íšŒì˜ ë¹„ì •ìƒì ì¸ ì‚¬íƒœë¥¼ ë°©ì§€í•˜ëŠ” ê¸°ê´€ì´ë‹¤. ìƒì›ì€ ê¸‰ë°•í•œ ì²˜ë¦¬ì‚¬í•­ì˜ ê²½ìš°ê°€ ì•„ë‹ˆë©´ ë²•ì•ˆì„ ë¨¼ì € ë‚´ëŠ” ê²½ìš°ê°€ ë“œë¬¼ê³  í•˜ì›ì´ ë§Œë“  ë²•ì•ˆì„ ìˆ˜ì •í•˜ì—¬ ë‹¤ì‹œ í•˜ì›ì— ë˜ëŒë ¤ë³´ë‚¸ë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ë‹¨ì›ì œê°€ ë¹ ì§€ê¸° ì‰¬ìš´ í•¨ì •ì„ ë¯¸ë¦¬ ë°©ì§€í•˜ëŠ” ê²ƒì´ë‹¤.ë‚ ì§œ=2017-02-05',\n",
       " 'question': 'ëŒ€í†µë ¹ì„ í¬í•¨í•œ ë¯¸êµ­ì˜ í–‰ì •ë¶€ ê²¬ì œê¶Œì„ ê°–ëŠ” êµ­ê°€ ê¸°ê´€ì€?',\n",
       " 'id': 'mrc-1-000067',\n",
       " 'answers': {'answer_start': [235], 'text': ['í•˜ì›']},\n",
       " 'document_id': 18293,\n",
       " '__index_level_0__': 42}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_from_huggingface[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def pull_out_dictionary(df_input: pd.DataFrame):\n",
    "    \"\"\"pull out str `{}` values from the pandas dataframe and shape it as a new column\"\"\"\n",
    "\n",
    "    df = df_input.copy()\n",
    "\n",
    "    # assign subject_entity and object_entity column values type as dictionary\n",
    "    # df[\"answers\"] = df[\"answers\"].apply(lambda x: eval(x))\n",
    "    \n",
    "    df = df.assign(\n",
    "        # subject_entity\n",
    "        answer_start=lambda x: x[\"answers\"].apply(lambda x: x[\"answer_start\"]),\n",
    "        text=lambda x: x[\"answers\"].apply(lambda x: x[\"text\"]),\n",
    "    )\n",
    "\n",
    "    # drop subject_entity and object_entity column\n",
    "    df = df.drop([\"answers\"], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def pull_out_list(df_input: pd.DataFrame):\n",
    "    \"\"\" pull out single item out of the list \"\"\"\n",
    "    \n",
    "    df = df_input.copy()\n",
    "\n",
    "    df[\"answer_start\"] = df[\"answer_start\"].apply(lambda x: int(x[0]))\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: x[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3952, 8)\n",
      "(240, 8)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Converting train and validation dataset to Pandas dataframe for convenience \"\"\"\n",
    "\n",
    "train_df = pull_out_dictionary(pd.DataFrame.from_records(datasets['train']))\n",
    "val_df = pull_out_dictionary(pd.DataFrame.from_records(datasets['validation']))\n",
    "\n",
    "train_df = pull_out_list(train_df)\n",
    "val_df = pull_out_list(val_df)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'context', 'question', 'id', 'document_id',\n",
      "       '__index_level_0__', 'answer_start', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "Fixed: roberta not receiving sequence ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence id not used: klue/roberta-large\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "# load tokenizer and configuration according to the model (ex: klue/roberta-large)\n",
    "if \"roberta\" in model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, \n",
    "        model_input_names = [\"input_ids\", \"attention_mask\"],\n",
    "        use_fast=True # use rust based tokenizer    \n",
    "    )\n",
    "    print(\"sequence id not used:\", model_args.model_name_or_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "    print(model_args.model_name_or_path)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'í˜„ëŒ€ ##ì  ì¸ì‚¬ ##ì¡° ##ì§ ##ê´€ë¦¬ ##ì˜ ì‹œë°œì  ##ì´ ëœ ì±… ##ì€ ?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample tokenization\n",
    "tokens = tokenizer.tokenize(train_dataset_from_huggingface[1]['question'])\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['í¬ë¦¬ìŠ¤í† ', '##í¬', 'ì•Œ', '##í•˜ìš°ìŠ¤']\n",
      "Wrong Example: tensor([[21533,     7,     7,  1862,  1381,     7,     7,  6634]])\n",
      "Correctly Encoded: tensor([[21533,  2208,  1381, 17975]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# test batch tokenization\n",
    "# https://github.com/huggingface/transformers/issues/10297#issuecomment-783464293\n",
    "sample_answer_token = ['í¬ë¦¬ìŠ¤í† ', '##í¬', 'ì•Œ', '##í•˜ìš°ìŠ¤']\n",
    "print(sample_answer_token)\n",
    "\n",
    "print(\"Wrong Example:\", tokenizer.encode(sample_answer_token, add_special_tokens=False, return_tensors='pt', is_split_into_words=True))\n",
    "print(\"Correctly Encoded:\" ,torch.IntTensor([tokenizer.convert_tokens_to_ids(sample_answer_token)])) # apply int for torch Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Creation and Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3952, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'ë¯¸êµ­ ìƒì›', 'context': 'ë¯¸êµ­ ìƒì˜ì› ë˜ëŠ” ë¯¸êµ­ ìƒì›(United States Senate)ì€ ì–‘ì›ì œì¸ ë¯¸êµ­ ì˜íšŒì˜ ìƒì›ì´ë‹¤.\\\\n\\\\në¯¸êµ­ ë¶€í†µë ¹ì´ ìƒì›ì˜ìž¥ì´ ëœë‹¤. ê° ì£¼ë‹¹ 2ëª…ì˜ ìƒì›ì˜ì›ì´ ì„ ì¶œë˜ì–´ 100ëª…ì˜ ìƒì›ì˜ì›ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìžˆë‹¤. ìž„ê¸°ëŠ” 6ë…„ì´ë©°, 2ë…„ë§ˆë‹¤ 50ê°œì£¼ ì¤‘ 1/3ì”© ìƒì›ì˜ì›ì„ ìƒˆë¡œ ì„ ì¶œí•˜ì—¬ ì—°ë°©ì— ë³´ë‚¸ë‹¤.\\\\n\\\\në¯¸êµ­ ìƒì›ì€ ë¯¸êµ­ í•˜ì›ê³¼ëŠ” ë‹¤ë¥´ê²Œ ë¯¸êµ­ ëŒ€í†µë ¹ì„ ìˆ˜ë°˜ìœ¼ë¡œ í•˜ëŠ” ë¯¸êµ­ ì—°ë°© í–‰ì •ë¶€ì— ê°ì¢… ë™ì˜ë¥¼ í•˜ëŠ” ê¸°ê´€ì´ë‹¤. í•˜ì›ì´ ì„¸ê¸ˆê³¼ ê²½ì œì— ëŒ€í•œ ê¶Œí•œ, ëŒ€í†µë ¹ì„ í¬í•¨í•œ ëŒ€ë‹¤ìˆ˜ì˜ ê³µë¬´ì›ì„ íŒŒë©´í•  ê¶Œí•œì„ ê°–ê³  ìžˆëŠ” êµ­ë¯¼ì„ ëŒ€í‘œí•˜ëŠ” ê¸°ê´€ì¸ ë°˜ë©´ ìƒì›ì€ ë¯¸êµ­ì˜ ì£¼ë¥¼ ëŒ€í‘œí•œë‹¤. ì¦‰ ìº˜ë¦¬í¬ë‹ˆì•„ì£¼, ì¼ë¦¬ë…¸ì´ì£¼ ê°™ì´ ì£¼ ì •ë¶€ì™€ ì£¼ ì˜íšŒë¥¼ ëŒ€í‘œí•˜ëŠ” ê¸°ê´€ì´ë‹¤. ê·¸ë¡œ ì¸í•˜ì—¬ êµ°ëŒ€ì˜ íŒŒë³‘, ê´€ë£Œì˜ ìž„ëª…ì— ëŒ€í•œ ë™ì˜, ì™¸êµ­ ì¡°ì•½ì— ëŒ€í•œ ìŠ¹ì¸ ë“± ì‹ ì†ì„ ìš”í•˜ëŠ” ê¶Œí•œì€ ëª¨ë‘ ìƒì›ì—ê²Œë§Œ ìžˆë‹¤. ê·¸ë¦¬ê³  í•˜ì›ì— ëŒ€í•œ ê²¬ì œ ì—­í• (í•˜ì›ì˜ ë²•ì•ˆì„ ê±°ë¶€í•  ê¶Œí•œ ë“±)ì„ ë‹´ë‹¹í•œë‹¤. 2ë…„ì˜ ìž„ê¸°ë¡œ ì¸í•˜ì—¬ ê¸‰ì§„ì ì¼ ìˆ˜ë°–ì— ì—†ëŠ” í•˜ì›ì€ ì§€ë‚˜ì¹˜ê²Œ ê¸‰ì§„ì ì¸ ë²•ì•ˆì„ ë§Œë“¤ê¸° ì‰½ë‹¤. ëŒ€í‘œì ì¸ ì˜ˆë¡œ ê±´ê°•ë³´í—˜ ê°œí˜ ë‹¹ì‹œ í•˜ì›ì´ ë¯¸êµ­ ì—°ë°© í–‰ì •ë¶€ì—ê²Œ í¼ë¸”ë¦­ ì˜µì…˜(ê³µê³µê±´ê°•ë³´í—˜ê¸°ê´€)ì˜ ì¡°í•­ì´ ìžˆëŠ” ë°˜ë©´ ìƒì›ì˜ ê²½ìš° í•˜ì›ì•ˆì´ ì§€ë‚˜ì¹˜ê²Œ ì„¸ê¸ˆì´ ë§Žì´ ë“ ë‹¤ëŠ” ì´ìœ ë¡œ í¼ë¸”ë¦­ ì˜µì…˜ ì¡°í•­ì„ ì œì™¸í•˜ê³  ë¹„ì˜ë¦¬ê±´ê°•ë³´í—˜ê¸°ê´€ì´ë‚˜ ë³´í—˜íšŒì‚¬ê°€ ë‹´ë‹¹í•˜ë„ë¡ í•œ ê²ƒì´ë‹¤. ì´ ê²½ìš°ì²˜ëŸ¼ ìƒì›ì€ í•˜ì›ì´ë‚˜ ë‚´ê°ì±…ìž„ì œê°€ ë¹ ì§€ê¸° ì‰¬ìš´ êµ­ê°€ë“¤ì˜ êµ­íšŒì²˜ëŸ¼ ê±¸í•í•˜ë©´ ë°œìƒí•˜ëŠ” ì˜íšŒì˜ ë¹„ì •ìƒì ì¸ ì‚¬íƒœë¥¼ ë°©ì§€í•˜ëŠ” ê¸°ê´€ì´ë‹¤. ìƒì›ì€ ê¸‰ë°•í•œ ì²˜ë¦¬ì‚¬í•­ì˜ ê²½ìš°ê°€ ì•„ë‹ˆë©´ ë²•ì•ˆì„ ë¨¼ì € ë‚´ëŠ” ê²½ìš°ê°€ ë“œë¬¼ê³  í•˜ì›ì´ ë§Œë“  ë²•ì•ˆì„ ìˆ˜ì •í•˜ì—¬ ë‹¤ì‹œ í•˜ì›ì— ë˜ëŒë ¤ë³´ë‚¸ë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ë‹¨ì›ì œê°€ ë¹ ì§€ê¸° ì‰¬ìš´ í•¨ì •ì„ ë¯¸ë¦¬ ë°©ì§€í•˜ëŠ” ê²ƒì´ë‹¤.ë‚ ì§œ=2017-02-05', 'question': 'ëŒ€í†µë ¹ì„ í¬í•¨í•œ ë¯¸êµ­ì˜ í–‰ì •ë¶€ ê²¬ì œê¶Œì„ ê°–ëŠ” êµ­ê°€ ê¸°ê´€ì€?', 'id': 'mrc-1-000067', 'document_id': 18293, '__index_level_0__': 42, 'answer_start': 235, 'text': 'í•˜ì›'}\n",
      "dict_keys(['title', 'context', 'question', 'id', 'document_id', '__index_level_0__', 'answer_start', 'text'])\n",
      "3952\n"
     ]
    }
   ],
   "source": [
    "# change dataframe to dictionary\n",
    "train_df_dict = train_df.to_dict('records')\n",
    "valid_df_dict = val_df.to_dict('records')\n",
    "print(train_df_dict[0]) \n",
    "print(train_df_dict[0].keys())\n",
    "print(len(train_df_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DEBUG_MODE = True\n",
    "\n",
    "def drop_truncated_data(dict_df):\n",
    "    \"\"\" dataset creation reference: https://mccormickml.com/2021/05/27/question-answering-system-tf-idf/ \"\"\"\n",
    "    # Lists to store the encoded samples.\n",
    "    all_input_ids = []\n",
    "    attention_masks = []\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    num_dropped = 0\n",
    "\n",
    "    for num, item in enumerate(dict_df):\n",
    "        answer_tokens = tokenizer.tokenize(item['text'], add_special_tokens=False)\n",
    "        sentinel_str = \" \".join([tokenizer.mask_token]*len(answer_tokens))\n",
    "        start_char_i = item['answer_start']\n",
    "        end_char_i = start_char_i + len(item['text'])\n",
    "\n",
    "        context_w_sentinel = \\\n",
    "            item['context'][:start_char_i] \\\n",
    "            + sentinel_str \\\n",
    "            + item['context'][end_char_i:]\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            item['question'],\n",
    "            context_w_sentinel,\n",
    "            add_special_tokens=True,\n",
    "            max_length = model_args.max_seq_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_dict['input_ids']\n",
    "        is_mask_token = (input_ids[0] == tokenizer.mask_token_id)\n",
    "        mask_token_indeces = is_mask_token.nonzero(as_tuple=False)[:, 0]\n",
    "        if not len(mask_token_indeces) == len(answer_tokens):\n",
    "            num_dropped += 1\n",
    "            continue\n",
    "\n",
    "        start_index = mask_token_indeces[0]\n",
    "        end_index = mask_token_indeces[-1]\n",
    "\n",
    "        # change start_index tensor and end_index tensor into integer type\n",
    "        start_index = start_index.item()\n",
    "        end_index = end_index.item()\n",
    "        answer_token_ids = tokenizer.convert_tokens_to_ids(answer_tokens)\n",
    "        input_ids[0, start_index : end_index + 1] = torch.tensor(answer_token_ids)\n",
    "        \n",
    "        all_input_ids.append(input_ids)\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        start_positions.append(start_index)\n",
    "        end_positions.append(end_index)\n",
    "\n",
    "\n",
    "    # Convert the lists of tensors into 2D tensors.\n",
    "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    # Convert the \"labels\" (the start and end indeces) into tensors.\n",
    "    start_positions = torch.tensor(start_positions)\n",
    "    end_positions = torch.tensor(end_positions)\n",
    "\n",
    "    return all_input_ids, attention_masks, start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_truncated = drop_truncated_data(train_df_dict)\n",
    "train_dataset = TensorDataset(train_truncated[0], train_truncated[1], train_truncated[2], train_truncated[3])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=model_args.batch_size, shuffle=True)\n",
    "\n",
    "valid_truncated = drop_truncated_data(valid_df_dict)\n",
    "valid_dataset = TensorDataset(valid_truncated[0], valid_truncated[1], valid_truncated[2], valid_truncated[3])\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=model_args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3952, 8)\n",
      "3706\n",
      "(240, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(len(train_dataset)) # number of clipped samples where answers are located out of the maximum sequence length\n",
    "\n",
    "print(val_df.shape)\n",
    "len(valid_dataset) # number of clipped samples where answers are located out of the maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Losses to use\n",
    "- CrossEntropyLoss provided by torch.nn was the best\n",
    "- For Performance Comparison, Please refer to [ðŸ”— wandb](https://wandb.ai/happyface-boostcamp/KLUE-QA?workspace=user-snoop2head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    # https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\n",
    "    def __init__(self, gamma=0.5, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "\n",
    "class LabelSmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    # https://github.com/NingAnMe/Label-Smoothing-for-CrossEntropyLoss-PyTorch/blob/main/label_smothing_cross_entropy_loss.py\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth_one_hot(targets: torch.Tensor, n_classes: int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = torch.empty(size=(targets.size(0), n_classes),\n",
    "                                  device=targets.device) \\\n",
    "                .fill_(smoothing / (n_classes - 1)) \\\n",
    "                .scatter_(1, targets.data.unsqueeze(1), 1. - smoothing)\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = LabelSmoothCrossEntropyLoss._smooth_one_hot(targets, inputs.size(-1),\n",
    "                                                              self.smoothing)\n",
    "        lsm = F.log_softmax(inputs, -1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            lsm = lsm * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = -(targets * lsm).sum(-1)\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cvqluu/Angular-Penalty-Softmax-Losses-Pytorch -> doesn't work, full of bugs\n",
    "# https://github.com/ronghuaiyang/arcface-pytorch/blob/master/models/metrics.py -> fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/CoinCheung/pytorch-loss/blob/master/dice_loss.py -> Not working\n",
    "# https://github.com/ShannonAI/dice_loss_for_NLP/blob/master/loss/dice_loss.py -> Loss too big: 300 or more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Tue Nov  2 07:16:46 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   34C    P0    36W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "model_args.DEBUG = False\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() and model_args.DEBUG == False else 'cpu')\n",
    "print(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoConfig\n",
    "\n",
    "\"\"\" default model, but not using this one \"\"\"\n",
    "if model_args.DEBUG == True:\n",
    "    model_config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        config=model_config,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://discuss.pytorch.org/uploads/default/original/2X/e/e7496a33d835f085d800ee17c0ade05895a89551.png)\n",
    "\n",
    "- t: length of passage (or input text)\n",
    "- depth: number of rnn layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForQuestionAnswering\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\"\"\" Reference: https://github.com/snexus/nlp-question-answering-system/blob/main/model.py \"\"\"\n",
    "\n",
    "class QAModel(nn.Module):\n",
    "    \"\"\" Custom Question Answering Model with Bidirectionaal LSTM head attached \"\"\"\n",
    "    def __init__(self, MODEL_NAME, dropout_proba=0.1):\n",
    "        super().__init__()        \n",
    "        self.model_config= AutoConfig.from_pretrained(MODEL_NAME)\n",
    "        self.transformer = AutoModel.from_pretrained(MODEL_NAME, config= self.model_config)\n",
    "        self.embed_dim = self.model_config.hidden_size # roberta hidden dim = 1024\n",
    "        #self.device=\"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "        # We replace the head with lstm layer\n",
    "        self.lstm= nn.LSTM(\n",
    "            input_size= self.embed_dim, \n",
    "            hidden_size= self.embed_dim, \n",
    "            num_layers= model_args.num_rnn_layers,\n",
    "            dropout= 0.2,\n",
    "            batch_first= True, \n",
    "            bidirectional= True\n",
    "            )\n",
    "        self.qa_head = nn.Linear(in_features=self.embed_dim*2, out_features=2, bias=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_proba)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, start_positions, end_positions):\n",
    "        \"\"\"\n",
    "        Forward step for the question-answering model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_enc - encoding dictionary from the tokenizer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        start_logits - logit corresponding to the start position of the answer (batch_size, sentence_size, 1)\n",
    "        start_positions - true start position (batch_size, 1) or None\n",
    "        end_logits - logit corresponding to the end position of the answer (batch_size, sentence_size, 1)\n",
    "        end_positions - ture end position (batch_size, 1) or None\n",
    "        \"\"\"\n",
    "\n",
    "        trans_out = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = trans_out.last_hidden_state  # (batch_size, len_sentence, embed_dim)\n",
    "        hidden_out, (last_hidden, last_cell) = self.lstm(hidden_state)\n",
    "\n",
    "        # Pass through the linear layer, we need to learn it's parameters\n",
    "        final_output = self.qa_head(hidden_out)  # (batch_size, len_sentence, 2)\n",
    "        start_logits, end_logits = final_output.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)  # (bs, max_query_len)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return {\n",
    "            \"start_logits\": start_logits, \n",
    "            \"start_positions\": start_positions,\n",
    "            \"end_logits\": end_logits,\n",
    "            \"end_positions\": end_positions\n",
    "            }\n",
    "\n",
    "\n",
    "    def compute_loss(self, start_logits, start_positions, end_logits, end_positions, return_outputs=False):\n",
    "        if len(start_positions.size()) > 1:\n",
    "            start_positions = start_positions.squeeze(-1)\n",
    "        if len(end_positions.size()) > 1:\n",
    "            end_positions = end_positions.squeeze(-1)\n",
    "        \n",
    "        start_loss_fct, end_loss_fct = nn.CrossEntropyLoss(), nn.CrossEntropyLoss() # -> so far the best performance\n",
    "        # start_loss_fct, end_loss_fct = FocalLoss(gamma=model_args.gamma), FocalLoss(gamma=model_args.gamma)\n",
    "        # start_loss_fct, end_loss_fct = LabelSmoothCrossEntropyLoss(smoothing=model_args.smoothing), LabelSmoothCrossEntropyLoss(smoothing=model_args.smoothing)\n",
    "        \n",
    "        start_loss = start_loss_fct(start_logits, start_positions)\n",
    "        end_loss = end_loss_fct(end_logits, end_positions)\n",
    "        total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "model = QAModel(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  2 07:16:58 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   34C    P0    36W / 250W |   2931MiB / 32510MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# from adamp import AdamP\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = model_args.learning_rate, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "epochs = model_args.num_train_epochs\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lstm-trainer-AdamW-CE-num_rnn_layers-2-no-clip'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_of_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm-trainer-AdamW-CE-num_rnn_layers-2-no-clip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msnoop2head\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/happyface-boostcamp/KLUE-QA/runs/v9a69h0d\" target=\"_blank\">lstm-trainer-AdamW-CE-num_rnn_layers-2-no-clip</a></strong> to <a href=\"https://wandb.ai/happyface-boostcamp/KLUE-QA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training 371 batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b674a7c8a946508658583a9d4578a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=371.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 Step: 100 Train Loss: 3.1070\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266326813775498ba37546c304776139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/3 Step: 100 Dev Loss: 1.6518 Dev Acc: 0.4455\n",
      "Saved model with highest EM accuracy: 0.4455\n",
      "Epoch: 1/3 Step: 200 Train Loss: 0.8847\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ae46801b97427b9d790d6108892cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/3 Step: 200 Dev Loss: 0.8757 Dev Acc: 0.7364\n",
      "Saved model with highest EM accuracy: 0.7364\n",
      "Epoch: 1/3 Step: 300 Train Loss: 0.6086\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e490c85a65204cc1afdc772f23c28807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/3 Step: 300 Dev Loss: 0.6843 Dev Acc: 0.7591\n",
      "Saved model with highest EM accuracy: 0.7591\n",
      "\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training 371 batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2803abfe56eb441796270ca0092d43d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=371.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/3 Step: 100 Train Loss: 0.3378\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cef8cc996b4341ab8caa45766c2845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/3 Step: 100 Dev Loss: 0.5934 Dev Acc: 0.7864\n",
      "Saved model with highest EM accuracy: 0.7864\n",
      "Epoch: 2/3 Step: 200 Train Loss: 0.2142\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa567f116b8d4aa8826615f68a9bb89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/3 Step: 200 Dev Loss: 0.6438 Dev Acc: 0.7773\n",
      "Epoch: 2/3 Step: 300 Train Loss: 0.2250\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e568754afa8447f9f961d67d6140f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/3 Step: 300 Dev Loss: 0.5673 Dev Acc: 0.7909\n",
      "Saved model with highest EM accuracy: 0.7909\n",
      "\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training 371 batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062c67a454b846f2aa0656bf7505d973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=371.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/3 Step: 100 Train Loss: 0.1450\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bac33f5359544268eb1f7aeca993789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3/3 Step: 100 Dev Loss: 0.4982 Dev Acc: 0.8409\n",
      "Saved model with highest EM accuracy: 0.8409\n",
      "Epoch: 3/3 Step: 200 Train Loss: 0.0798\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888b685b91d646768600ded2146dfe60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3/3 Step: 200 Dev Loss: 0.5151 Dev Acc: 0.8227\n",
      "Epoch: 3/3 Step: 300 Train Loss: 0.0685\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9deaaa1164e94adeaa86ae5f819f50f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3/3 Step: 300 Dev Loss: 0.5888 Dev Acc: 0.7955\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import wandb \n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "# https://wandb.ai/happyface-boostcamp/KLUE-QA\n",
    "print(name_of_experiment)\n",
    "wandb.init(\n",
    "    project='KLUE-QA', \n",
    "    entity='happyface-boostcamp', \n",
    "    name=name_of_experiment,\n",
    "    config=model_args\n",
    "    )\n",
    "\n",
    "\n",
    "fold_num = 0\n",
    "best_acc = 0.0\n",
    "\n",
    "# Initialize using Metrics \n",
    "train_acc, train_loss = Metrics(), Metrics()\n",
    "dev_acc, dev_loss = Metrics(), Metrics()\n",
    "dev_start_acc, dev_end_acc = Metrics(), Metrics()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))    \n",
    "    print('Training {:,} batches...'.format(len(train_dataloader)))\n",
    "\n",
    "    # change the model to train mode\n",
    "    model.train()\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        # get info from batch\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_start_pos = batch[2].to(device)\n",
    "        b_end_pos = batch[3].to(device)\n",
    "\n",
    "        # get outputs from the model\n",
    "        model.zero_grad()        \n",
    "        outputs = model(\n",
    "            b_input_ids, \n",
    "            attention_mask=b_input_mask, \n",
    "            start_positions=b_start_pos,\n",
    "            end_positions=b_end_pos,\n",
    "        )\n",
    "\n",
    "        # get logits(probability) and loss\n",
    "        start_logits = outputs['start_logits']\n",
    "        end_logits = outputs['end_logits']\n",
    "        loss = model.compute_loss(\n",
    "            start_logits = start_logits,\n",
    "            start_positions = outputs['start_positions'],\n",
    "            end_logits = end_logits,\n",
    "            end_positions = outputs['end_positions'],\n",
    "        )\n",
    "        \n",
    "        # record train loss\n",
    "        train_loss.update(loss.item(), len(b_input_ids))\n",
    "\n",
    "        # backward propagation & optimizer stepping\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # EVALUATE every 100 steps\n",
    "        if step != 0 and step % 100 == 0:\n",
    "            print('Epoch: {}/{}'.format(epoch_i+1, model_args.num_train_epochs), 'Step: {}'.format(step), 'Train Loss: {:.4f}'.format(train_loss.avg))\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            # initialize prediction and labels for future accuracy calculation\n",
    "            pred_start, pred_end, true_start, true_end = [], [], [], []\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in tqdm(valid_dataloader):\n",
    "                \n",
    "                # get info from batch\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_input_mask = batch[1].to(device)\n",
    "                b_start_pos = batch[2].to(device)\n",
    "                b_end_pos = batch[3].to(device)\n",
    "\n",
    "                # Evaluate and get outputs\n",
    "                # model.eval() # lstm does not accept model.eval()\n",
    "                with torch.no_grad():        \n",
    "                    outputs = model(\n",
    "                        b_input_ids, \n",
    "                        # token_type_ids=b_seg_ids, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        start_positions=b_start_pos,\n",
    "                        end_positions=b_end_pos,\n",
    "                    )\n",
    "\n",
    "                # get logits(probability) and loss\n",
    "                start_logits = outputs['start_logits']\n",
    "                end_logits = outputs['end_logits']        \n",
    "                loss = model.compute_loss(\n",
    "                    start_logits = start_logits,\n",
    "                    start_positions = outputs['start_positions'],\n",
    "                    end_logits = end_logits,\n",
    "                    end_positions = outputs['end_positions'],\n",
    "                )\n",
    "\n",
    "                # record evaluation loss\n",
    "                dev_loss.update(loss.item(), len(b_input_ids))\n",
    "\n",
    "                # get logits and predictions for evaluation accuracy\n",
    "                start_logits = start_logits.detach().cpu().numpy()\n",
    "                end_logits = end_logits.detach().cpu().numpy()                \n",
    "                b_start_pos = b_start_pos.to('cpu').numpy()\n",
    "                b_end_pos = b_end_pos.to('cpu').numpy()\n",
    "                answer_start = np.argmax(start_logits, axis=1)\n",
    "                answer_end = np.argmax(end_logits, axis=1)\n",
    "\n",
    "                # append the prediction and ground truth to the list for comparison\n",
    "                pred_start.append(answer_start)\n",
    "                pred_end.append(answer_end)\n",
    "                true_start.append(b_start_pos)\n",
    "                true_end.append(b_end_pos)\n",
    "\n",
    "            # compare start for accuracy calculation\n",
    "            pred_start = np.concatenate(pred_start, axis=0)\n",
    "            true_start = np.concatenate(true_start, axis=0)\n",
    "            num_start_correct = np.sum(pred_start == true_start)\n",
    "            dev_start_acc.update((pred_start == true_start).mean(), len(true_start))\n",
    "            \n",
    "            # compare end for accuracy calculation\n",
    "            pred_end = np.concatenate(pred_end, axis=0)\n",
    "            true_end = np.concatenate(true_end, axis=0)\n",
    "            num_end_correct = np.sum(pred_end == true_end)\n",
    "            dev_end_acc.update((pred_end == true_end).mean(), len(true_end))\n",
    "            \n",
    "            # compare both start and end for EM accuracy calculation\n",
    "            total_correct = num_start_correct + num_end_correct\n",
    "            total_indeces = len(true_start) + len(true_end)\n",
    "            \n",
    "            # get both cases where pred_start == true_start and pred_end == true_end\n",
    "            both_correct = np.mean(np.logical_and(pred_start == true_start, pred_end == true_end))\n",
    "            dev_acc.update(both_correct, len(b_input_ids))\n",
    "\n",
    "            # Report the final accuracy for this validation run.\n",
    "            print('Epoch: {}/{}'.format(epoch_i+1, epochs), 'Step: {}'.format(step), 'Dev Loss: {:.4f}'.format(dev_loss.avg), 'Dev Acc: {:.4f}'.format(dev_acc.avg))\n",
    "            \n",
    "            # log on wandb\n",
    "            wandb.log(\n",
    "                    {\n",
    "                        'train/loss': train_loss.avg,\n",
    "                        'train/learning_rate':optimizer.param_groups[0]['lr'], \n",
    "                        'eval/loss': dev_loss.avg,\n",
    "                        'eval/accuracy':dev_acc.avg,\n",
    "                        'eval/start_accuracy':dev_start_acc.avg,\n",
    "                        'eval/end_accuracy':dev_end_acc.avg,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # save the best model based on EM accuracy\n",
    "            if best_acc < dev_acc.avg:\n",
    "                best_acc = dev_acc.avg\n",
    "                torch.save(model.state_dict(), './results/{}-fold-{}-best-acc-model.pt'.format(fold_num+1, model_args.num_folds))\n",
    "                print('Saved model with highest EM accuracy: {:.4f}'.format(best_acc))\n",
    "                wandb.log({'best_acc':best_acc})\n",
    "\n",
    "            # reset metrics\n",
    "            train_loss.reset()\n",
    "            dev_loss.reset()\n",
    "            dev_acc.reset()\n",
    "            dev_start_acc.reset()\n",
    "            dev_end_acc.reset()\n",
    "            \n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  2 07:44:26 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   57C    P0    50W / 250W |  29163MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
