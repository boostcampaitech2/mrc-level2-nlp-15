{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, RobertaModel,\n",
    "    BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "from torch.utils.data import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.6.0].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'klue/bert-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# num_added_toks = tokenizer.add_tokens(['\\\\n']) # 32000번째로 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/data/train_dataset')\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "unk_count = []\n",
    "for i in tqdm(range(len(train_dataset))) :\n",
    "    unk_count.append(tokenizer(train_dataset['context'][i], max_length = True)['input_ids'].count(1)) # 1 == UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(unk_count))\n",
    "print(np.mean(unk_count))\n",
    "print(np.max(unk_count))\n",
    "print(np.min(unk_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/data/train_dataset')\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSampler(Sampler) :\n",
    "    def __init__(self, data_source, batch_size) :\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self) :\n",
    "        n = len(self.data_source)\n",
    "        index_list = []\n",
    "        while True :\n",
    "            out = True\n",
    "            for i in range(self.batch_size) :\n",
    "                tmp_data = random.randint(0, n-1)\n",
    "                index_list.append(tmp_data)\n",
    "            for f, s in zip(index_list, index_list[1:]) :\n",
    "                if abs(s-f) <= 2 :\n",
    "                    out = False\n",
    "            if out == True :\n",
    "                break\n",
    "\n",
    "        while True : # 추가 삽입\n",
    "            tmp_data = random.randint(0, n-1)\n",
    "            if (tmp_data not in index_list) and \\\n",
    "                (abs(tmp_data-index_list[-i]) > 2 for i in range(1,self.batch_size+1)) \\\n",
    "            : \n",
    "                index_list.append(tmp_data)\n",
    "            if len(index_list) == n :\n",
    "                break\n",
    "        return iter(index_list)\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwer\n",
    "class DenseRetrieval:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        p_encoder,\n",
    "        q_encoder,\n",
    "        sampler\n",
    "    ):\n",
    "        \"\"\"\n",
    "        학습과 추론에 사용될 여러 셋업을 마쳐봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def train(self, args=None, tokenizer = None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        if tokenizer is None :\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        for i in (range(len(self.dataset))) :\n",
    "            if i == 0 :\n",
    "                q_seqs = tokenizer(\n",
    "                    self.dataset[i]['question'],\n",
    "                    padding='max_length',\n",
    "                    truncation = True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                p_seqs = tokenizer(\n",
    "                    self.dataset[i]['context'],\n",
    "                    truncation = True,\n",
    "                    stride = 128,\n",
    "                    padding='max_length',\n",
    "                    return_overflowing_tokens=True,\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "\n",
    "                p_seqs.pop('overflow_to_sample_mapping')\n",
    "                p_seqs.pop('offset_mapping')\n",
    "\n",
    "                for k in q_seqs.keys() :\n",
    "                    q_seqs[k] = q_seqs[k].tolist()\n",
    "                    p_seqs[k] = p_seqs[k].tolist()\n",
    "            else :\n",
    "                tmp_q_seq = tokenizer(\n",
    "                    self.dataset[i]['question'],\n",
    "                    padding='max_length',\n",
    "                    truncation = True,\n",
    "                    return_tensors='pt')\n",
    "                tmp_p_seq = tokenizer(\n",
    "                    self.dataset[i]['context'],\n",
    "                    truncation = True,\n",
    "                    stride = 128,\n",
    "                    padding='max_length',\n",
    "                    return_overflowing_tokens=True,\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "\n",
    "                tmp_p_seq.pop('overflow_to_sample_mapping')\n",
    "                tmp_p_seq.pop('offset_mapping')\n",
    "\n",
    "                for k in tmp_p_seq.keys() :\n",
    "                    tmp_p_seq[k] = tmp_p_seq[k].tolist()\n",
    "                    tmp_q_seq[k] = tmp_q_seq[k].tolist()\n",
    "\n",
    "\n",
    "                for j in range(len(tmp_p_seq['input_ids'])) :\n",
    "                    q_seqs['input_ids'].append(tmp_q_seq['input_ids'][0])\n",
    "                    q_seqs['token_type_ids'].append(tmp_q_seq['token_type_ids'][0])\n",
    "                    q_seqs['attention_mask'].append(tmp_q_seq['attention_mask'][0])\n",
    "                    p_seqs['input_ids'].append(tmp_p_seq['input_ids'][j])\n",
    "                    p_seqs['token_type_ids'].append(tmp_p_seq['token_type_ids'][j])\n",
    "                    p_seqs['attention_mask'].append(tmp_p_seq['attention_mask'][j])\n",
    "\n",
    "        for k in q_seqs.keys() :\n",
    "            q_seqs[k] = torch.tensor(q_seqs[k])\n",
    "            p_seqs[k] = torch.tensor(p_seqs[k])\n",
    "\n",
    "        train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])\n",
    "        sampler = self.sampler(train_dataset, args.per_device_train_batch_size)\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                      batch_size=args.per_device_train_batch_size,\n",
    "                                      sampler = sampler)\n",
    "\n",
    "        no_decay = [\"bias\" ,\"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            # eps=args.adam_epsilon\n",
    "        )\n",
    "\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "        \n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "        self.q_encoder.train()\n",
    "        self.p_encoder.train()\n",
    "        for epoch, _ in enumerate(train_iterator):\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            loss_value=0 # Accumulation할 때 진행\n",
    "            losses = 0\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                if torch.cuda.is_available():\n",
    "                    batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "                p_inputs = {'input_ids': batch[0],\n",
    "                            'attention_mask': batch[1],\n",
    "                            'token_type_ids': batch[2]\n",
    "                            }\n",
    "                \n",
    "                q_inputs = {'input_ids': batch[3],\n",
    "                            'attention_mask': batch[4],\n",
    "                            'token_type_ids': batch[5]}\n",
    "\n",
    "                p_outputs = self.p_encoder(**p_inputs)  # (batch_size, emb_dim)\n",
    "                q_outputs = self.q_encoder(**q_inputs)  # (batch_size, emb_dim)\n",
    "\n",
    "                # Calculate similarity score & loss\n",
    "                sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "\n",
    "                # target: position of positive samples = diagonal element \n",
    "                # targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "                targets = torch.arange(0, len(p_inputs['input_ids'])).long()\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    targets = targets.to('cuda')\n",
    "                    \n",
    "                sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                loss = F.nll_loss(sim_scores, targets)\n",
    "                losses += loss.item()\n",
    "                if step % 100 == 0 :\n",
    "                    print(f'{epoch}epoch loss: {losses/(step+1)}') # Accumulation할 경우 주석처리\n",
    "\n",
    "                # ################ACCUMULATION###############################\n",
    "                # if (step+1) % args.gradient_accumulation_steps == 0 :\n",
    "                #     optimizer.step()\n",
    "                #     scheduler.step()\n",
    "                #     self.q_encoder.zero_grad()\n",
    "                #     self.p_encoder.zero_grad()\n",
    "\n",
    "                # losses += loss.item()\n",
    "                # if (step+1) % 64 == 0:\n",
    "                #     train_loss = losses / 64\n",
    "                #     print(f'training loss: {train_loss:4.4}')\n",
    "                #     losses = 0\n",
    "                # ###########################################################\n",
    "                self.q_encoder.zero_grad()\n",
    "                self.p_encoder.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                # global_step += 1\n",
    "                \n",
    "                #torch.cuda.empty_cache()\n",
    "                del p_inputs, q_inputs\n",
    "\n",
    "        return self.p_encoder, self.q_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# num_added_toks = tokenizer.add_tokens(['\\\\n']) # 32000번째로 삽입\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4644ac01aced4270a66202192854f432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0epoch loss: 27.258563995361328\n",
      "0epoch loss: 3.188428820009426\n",
      "0epoch loss: 2.15615082128131\n",
      "0epoch loss: 1.6615046049341362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [05:59<53:56, 359.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b0c9a0eec643c2bec3c89f6255c4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1epoch loss: 0.7229509353637695\n",
      "1epoch loss: 0.33619247033897015\n",
      "1epoch loss: 0.3605005782959399\n",
      "1epoch loss: 0.3809681247666082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [11:59<47:57, 359.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083ffe7a88324382ad5dabd74cd12438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2epoch loss: 0.400591641664505\n",
      "2epoch loss: 0.26086950214524374\n",
      "2epoch loss: 0.2360156178845102\n",
      "2epoch loss: 0.23930015467282112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [17:59<41:57, 359.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e809889fe474a59a953276d3c0edfc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3epoch loss: 0.002421972109004855\n",
      "3epoch loss: 0.19476143344038047\n",
      "3epoch loss: 0.1759268373971462\n",
      "3epoch loss: 0.17089595330841964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [23:59<35:59, 359.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5eda2995e94e9fb2a4249497a10081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4epoch loss: 0.04584382101893425\n",
      "4epoch loss: 0.1344734929988419\n",
      "4epoch loss: 0.1484213233838525\n",
      "4epoch loss: 0.13362091015080652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [29:58<29:58, 359.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adcc85dc06948e098e82ba6b9806473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5epoch loss: 0.0014048183802515268\n",
      "5epoch loss: 0.10812467914322474\n",
      "5epoch loss: 0.11312140181990206\n",
      "5epoch loss: 0.11231223831322211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [35:57<23:57, 359.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c25dc22b3641e7b4d97c68b0d6d1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6epoch loss: 0.009419458918273449\n",
      "6epoch loss: 0.08964838462493885\n",
      "6epoch loss: 0.09739828461209782\n",
      "6epoch loss: 0.09690767208800188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 7/10 [41:56<17:57, 359.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c6cb274bca46e69fa6a1a39450a8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7epoch loss: 0.002462411765009165\n",
      "7epoch loss: 0.07503278178982967\n",
      "7epoch loss: 0.08865643773617139\n",
      "7epoch loss: 0.08171159889440917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 8/10 [47:55<11:58, 359.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711bcee64b254a33af6945e4ba070182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8epoch loss: 0.0057458411902189255\n",
      "8epoch loss: 0.06557883964954514\n",
      "8epoch loss: 0.06918674565960502\n",
      "8epoch loss: 0.06761777068392721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 9/10 [53:55<05:59, 359.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63855af0ace143ab830934e95b1a3d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9epoch loss: 0.23215711116790771\n",
      "9epoch loss: 0.062069881366540465\n",
      "9epoch loss: 0.054815869627156136\n",
      "9epoch loss: 0.056187619689004605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [59:54<00:00, 359.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# sampler = CustomSampler()\n",
    "# Retriever는 아래와 같이 사용할 수 있도록 코드를 짜봅시다.\n",
    "retriever = DenseRetrieval(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder,\n",
    "    sampler = CustomSampler\n",
    ")\n",
    "p_encoder, q_encoder = retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "corpus = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ")  # set 은 매번 순서가 바뀌므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37491fda54e4480883796bc178522386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56737.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# p_encoder = retriever.p_encoder\n",
    "# q_encoder = retriever.q_encoder\n",
    "with torch.no_grad() :\n",
    "    p_encoder.eval()\n",
    "\n",
    "    p_embs = []\n",
    "    for p in tqdm(corpus) :\n",
    "        p = tokenizer(p, padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "        p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "        p_embs.append(p_emb)\n",
    "p_embs = torch.Tensor(p_embs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = '/opt/ml/custom/passage_embedding_special_customsample_10.bin'\n",
    "with open(file_path, 'wb') as file :\n",
    "    pickle.dump(p_embs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder.cpu()\n",
    "del p_encoder\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(q_encoder, '/opt/ml/custom/q_encoder_special_customsample_10.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Relavant Documnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "corpus = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ")  # set 은 매번 순서가 바뀌므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/opt/ml/custom/passage_embedding_special_shuffle_100.bin', 'rb') as file :\n",
    "    p_embs = pickle.load(file)\n",
    "p_embs = p_embs\n",
    "\n",
    "q_encoder = torch.load('/opt/ml/custom/q_encoder_special_shuffle_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size 16/epoch:10/CustomSampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3ea31c36f146a6be92851635d740cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 50)\n",
    "\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_100 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6916666666666667\n"
     ]
    }
   ],
   "source": [
    "correct_length_100 = []\n",
    "for i in range(len(cqas_100)) :\n",
    "    if cqas_100['original_context'][i] in cqas_100['context'][i] :\n",
    "        correct_length_100.append(i)\n",
    "print(len(correct_length_100) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqas_100.to_csv('/opt/ml/custom/valid_dpr_b_16_speical_customsampling_e10_t50.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
