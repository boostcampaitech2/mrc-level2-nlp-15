{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 30 09:45:49 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   41C    P0    38W / 250W |      0MiB / 32510MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, RobertaModel,\n",
    "    BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dpr + sparse embedding (wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwer\n",
    "from retrieval import SparseRetrieval \n",
    "\n",
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output\n",
    "    \n",
    "def proprecessing(text):\n",
    "    new_text = text.replace(r\"\\n\\n\", \"\")\n",
    "    return new_text\n",
    "\n",
    "\n",
    "class DenseRetrieval:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        p_encoder,\n",
    "        q_encoder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        학습과 추론에 사용될 여러 셋업을 마쳐봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.train_dataset = dataset\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "        self.num_neg = 1\n",
    "        self.prepare_in_batch_negative()\n",
    "        \n",
    "    def prepare_in_batch_negative(self,\n",
    "        train_dataset=None,\n",
    "        num_neg=1,\n",
    "        tokenizer=None\n",
    "    ):\n",
    "        self.num_neg = num_neg\n",
    "        \n",
    "        if train_dataset is None:\n",
    "            train_dataset = self.train_dataset\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        batch_size = args.per_device_train_batch_size\n",
    "        \n",
    "        # 1. hard-Negative 추가하기\n",
    "        # sparse embedding -> top1 neg passage\n",
    "        sparse_retriever =   SparseRetrieval(\n",
    "                tokenize_fn=tokenizer\n",
    "            )\n",
    "        full_ds = concatenate_datasets(\n",
    "            [\n",
    "                train_dataset.flatten_indices(),\n",
    "                # dataset[\"validation\"].flatten_indices(),\n",
    "            ]\n",
    "        ) \n",
    "\n",
    "        sparse_retriever.get_sparse_embedding()\n",
    "        df = sparse_retriever.retrieve(full_ds, topk=args.per_device_train_batch_size) # sparse-retriever에서 뽑은 top_k\n",
    "\n",
    "        p_with_neg = []\n",
    "        step=0\n",
    "        \n",
    "        # CORPUS를 np.array로 변환해줍니다.\n",
    "        corpus = np.array([proprecessing(example) for example in train_dataset[\"context\"]])\n",
    "        batch_size = batch_size\n",
    "        for idx, c in enumerate(train_dataset[\"context\"]):\n",
    "            if idx % batch_size==0:\n",
    "                # [step, step+1, .. , step+batch_size]. \n",
    "                batch_idxs = np.arange(step*batch_size, min((step+1)*batch_size, len(train_dataset)))\n",
    "                step +=1\n",
    "                \n",
    "            p_neg = list(corpus[batch_idxs])\n",
    "            \n",
    "            # sparse-retriever에서 뽑은 top_k indices\n",
    "            hard_negs = df['context_id'][idx]\n",
    "            for h in hard_negs:\n",
    "                if len(p_neg)==batch_size + num_neg: break # 한 배치당 hard_neg num_neg(1)만큼 추가\n",
    "                \n",
    "                hard_neg = proprecessing(sparse_retriever.contexts[h]) # sparse-retriever top_k indices -> context\n",
    "                if hard_neg in p_neg: continue\n",
    "                p_neg.append(hard_neg)\n",
    "                \n",
    "            p_with_neg.extend(p_neg)\n",
    "                \n",
    "                \n",
    "        # 2. (Question, Passage) 데이터셋 만들어주기\n",
    "        q_seqs = tokenizer(\n",
    "            train_dataset[\"question\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        p_seqs = tokenizer(\n",
    "            p_with_neg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        max_len = p_seqs[\"input_ids\"].size(-1)\n",
    "        p_seqs[\"input_ids\"] = p_seqs[\"input_ids\"].view(-1, batch_size+num_neg, max_len)\n",
    "        p_seqs[\"attention_mask\"] = p_seqs[\"attention_mask\"].view(-1, batch_size+num_neg, max_len)\n",
    "        p_seqs[\"token_type_ids\"] = p_seqs[\"token_type_ids\"].view(-1, batch_size+num_neg, max_len)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            p_seqs[\"input_ids\"], p_seqs[\"attention_mask\"], p_seqs[\"token_type_ids\"], \n",
    "            q_seqs[\"input_ids\"], q_seqs[\"attention_mask\"], q_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "    def train(self, args=None, tokenizer = None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        if tokenizer is None :\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        train_dataloader = self.train_dataloader\n",
    "\n",
    "        no_decay = [\"bias\" ,\"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            # eps=args.adam_epsilon\n",
    "        )\n",
    "\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "        \n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "        self.q_encoder.train()\n",
    "        self.p_encoder.train()\n",
    "        for epoch, _ in enumerate(train_iterator):\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            # loss_value=0 # Accumulation할 때 진행\n",
    "            losses = 0\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                if torch.cuda.is_available():\n",
    "                    batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "                p_inputs = {\n",
    "                    \"input_ids\": batch[0].view(args.per_device_train_batch_size * (args.per_device_train_batch_size + self.num_neg), -1),\n",
    "                    \"attention_mask\": batch[1].view(args.per_device_train_batch_size * (args.per_device_train_batch_size + self.num_neg), -1),\n",
    "                    \"token_type_ids\": batch[2].view(args.per_device_train_batch_size * (args.per_device_train_batch_size + self.num_neg), -1)\n",
    "                }       \n",
    "                \n",
    "                q_inputs = {'input_ids': batch[3],\n",
    "                            'attention_mask': batch[4],\n",
    "                            'token_type_ids': batch[5]}\n",
    "\n",
    "                p_outputs = self.p_encoder(**p_inputs)  # (batch_size+1, emb_dim)\n",
    "                # p_outputs = torch.transpose(p_outputs.view(batch_size, self.num_neg+1,-1), 1, 2)\n",
    "                \n",
    "                q_outputs = self.q_encoder(**q_inputs)  # (batch_size, emb_dim)\n",
    "                # print(f'p_outputs {p_outputs.shape}')\n",
    "                # print(f'q_outputs {q_outputs.shape}')\n",
    "                \n",
    "                # Calculate similarity score & loss\n",
    "                sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size+num_neg) = (batch_size, batch_size+num_neg)\n",
    "                # print(f'sim_scores {sim_scores.shape}')\n",
    "                \n",
    "                # target: position of positive samples = diagonal element \n",
    "                targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    targets = targets.to('cuda')\n",
    "\n",
    "                sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                loss = F.nll_loss(sim_scores, targets)\n",
    "                losses += loss.item()\n",
    "                if step % 100 == 0 :\n",
    "                    print(f'{epoch}epoch loss: {losses/(step+1)}') # Accumulation할 경우 주석처리\n",
    "\n",
    "                \n",
    "                #################ACCUMULATION###############################\n",
    "                # loss_value += loss\n",
    "                # if (step+1) % args.gradient_accumulation_steps == 0 :\n",
    "                #     optimizer.step()\n",
    "                #     scheduler.step()\n",
    "                #     self.q_encoder.zero_grad()\n",
    "                #     self.p_encoder.zero_grad()\n",
    "                #     global_step += 1\n",
    "                #     print(loss_value/args.gradient_accumulation_steps)\n",
    "                #     loss_value = 0\n",
    "                ############################################################\n",
    "                self.q_encoder.zero_grad()\n",
    "                self.p_encoder.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                global_step += 1\n",
    "                \n",
    "                #torch.cuda.empty_cache()\n",
    "                del p_inputs, q_inputs\n",
    "\n",
    "        return self.p_encoder, self.q_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "dataset = load_from_disk('/opt/ml/data/train_dataset')\n",
    "train_dataset = dataset['train']\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /opt/ml/data/train_dataset/train/cache-fbc57aa6e699fb0c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n",
      "Embedding pickle load.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval:  24%|██▍       | 940/3952 [00:00<00:00, 9393.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[query exhaustive search] done in 41.005 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval: 100%|██████████| 3952/3952 [00:00<00:00, 9277.03it/s]\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9d868ec2c04c45ab71fec931435f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=988.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0epoch loss: 64.46937561035156\n",
      "0epoch loss: 8.713131009942234\n",
      "0epoch loss: 6.130762319659713\n",
      "0epoch loss: 5.19589794593\n",
      "0epoch loss: 4.687264641026903\n",
      "0epoch loss: 4.384292032904254\n",
      "0epoch loss: 4.174694070006765\n",
      "0epoch loss: 4.01329259015354\n",
      "0epoch loss: 3.895762870671895\n",
      "0epoch loss: 3.801125058587992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [12:54<1:56:07, 774.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdca022b9a046b3972d336a8099aa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=988.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1epoch loss: 2.7169995307922363\n",
      "1epoch loss: 3.0098952869377515\n",
      "1epoch loss: 3.025026746057159\n",
      "1epoch loss: 3.025579998263489\n",
      "1epoch loss: 3.0181673922740906\n",
      "1epoch loss: 3.0139089939361083\n",
      "1epoch loss: 3.0099677301682966\n",
      "1epoch loss: 3.0050437270149524\n",
      "1epoch loss: 3.0017942578605052\n",
      "1epoch loss: 3.000652249460083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [25:47<1:43:11, 773.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd381f5eadac4b78a46e4c4f9303d0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=988.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2epoch loss: 2.918689250946045\n",
      "2epoch loss: 2.9625078475121223\n",
      "2epoch loss: 2.9789974452251227\n",
      "2epoch loss: 2.9853754360414424\n",
      "2epoch loss: 2.9793721095582195\n",
      "2epoch loss: 2.9775493316307755\n",
      "2epoch loss: 2.9755210602739686\n",
      "2epoch loss: 2.973076890436627\n",
      "2epoch loss: 2.9709773798858032\n",
      "2epoch loss: 2.967497061942182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [38:39<1:30:13, 773.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a49942990c4aa7b88d8331a4c88890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=988.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3epoch loss: 3.1869192123413086\n",
      "3epoch loss: 2.94145410603816\n",
      "3epoch loss: 2.9549368910528533\n",
      "3epoch loss: 2.952835509151319\n",
      "3epoch loss: 2.9561024764529487\n",
      "3epoch loss: 2.9558922292705545\n",
      "3epoch loss: 2.9519385688515154\n",
      "3epoch loss: 2.948497179061302\n",
      "3epoch loss: 2.947234131721373\n",
      "3epoch loss: 2.9461469253345283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [51:32<1:17:19, 773.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a14a82f80c543429cbd89d9ecd10156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=988.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4epoch loss: 3.032198905944824\n",
      "4epoch loss: 2.9408191973620124\n",
      "4epoch loss: 2.9315267593706427\n",
      "4epoch loss: 2.93070309898782\n",
      "4epoch loss: 2.9260703274734\n",
      "4epoch loss: 2.9266225468374776\n",
      "4epoch loss: 2.930595846223752\n",
      "4epoch loss: 2.9261756382042945\n",
      "4epoch loss: 2.927038513616974\n",
      "4epoch loss: 2.925711704014938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [1:04:25<1:04:25, 773.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8433bec9e0bd4ce8813ab45b5a8251e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=988.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5epoch loss: 2.5492897033691406\n",
      "5epoch loss: 2.904755717456931\n",
      "5epoch loss: 2.9007016485603296\n",
      "5epoch loss: 2.911125889648235\n",
      "5epoch loss: 2.9096349867204774\n",
      "5epoch loss: 2.910969939774382\n",
      "5epoch loss: 2.91470556727265\n",
      "5epoch loss: 2.9141915585957308\n",
      "5epoch loss: 2.916206397069676\n",
      "5epoch loss: 2.9171998162116117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [1:17:17<51:31, 772.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09887b1431e14976b2196c81c8663265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=988.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6epoch loss: 3.044895648956299\n",
      "6epoch loss: 2.907477570052194\n",
      "6epoch loss: 2.915490242972303\n",
      "6epoch loss: 2.909752833882836\n",
      "6epoch loss: 2.9103252816378626\n",
      "6epoch loss: 2.9106368280932338\n",
      "6epoch loss: 2.9081584229048794\n",
      "6epoch loss: 2.908284685921227\n",
      "6epoch loss: 2.9079762284972994\n",
      "6epoch loss: 2.90667826565733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 7/10 [1:30:09<38:38, 772.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b8a78e8506461eb4babb4c5809eb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=988.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7epoch loss: 2.8771088123321533\n",
      "7epoch loss: 2.9034086147157274\n",
      "7epoch loss: 2.9121520021068514\n",
      "7epoch loss: 2.9078104979176063\n",
      "7epoch loss: 2.9052108005989816\n",
      "7epoch loss: 2.904405474424838\n",
      "7epoch loss: 2.9054545618333356\n",
      "7epoch loss: 2.905327877542602\n",
      "7epoch loss: 2.9054859247100486\n",
      "7epoch loss: 2.9046683176508488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 8/10 [1:43:01<25:44, 772.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b5f244e5174fe9a9bdaf12ff874f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=988.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8epoch loss: 2.8463616371154785\n",
      "8epoch loss: 2.899140546817591\n",
      "8epoch loss: 2.901031770516391\n",
      "8epoch loss: 2.90061831553513\n",
      "8epoch loss: 2.8992427590481955\n",
      "8epoch loss: 2.8991127061748694\n",
      "8epoch loss: 2.899466374947902\n",
      "8epoch loss: 2.9007860226569946\n",
      "8epoch loss: 2.899254776565323\n",
      "8epoch loss: 2.8990515458597064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 9/10 [1:55:53<12:52, 772.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e916341c1ab046b49109d74b6830f468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=988.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9epoch loss: 2.7852673530578613\n",
      "9epoch loss: 2.8585264871616176\n",
      "9epoch loss: 2.875302117855395\n",
      "9epoch loss: 2.8783825925022266\n",
      "9epoch loss: 2.8839872406605176\n",
      "9epoch loss: 2.8866678788038547\n",
      "9epoch loss: 2.8888835323828825\n",
      "9epoch loss: 2.890118754029104\n",
      "9epoch loss: 2.890539919690098\n",
      "9epoch loss: 2.891009535032689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [2:08:45<00:00, 772.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retriever는 아래와 같이 사용할 수 있도록 코드를 짜봅시다.\n",
    "retriever = DenseRetrieval(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder\n",
    ")\n",
    "\n",
    "p_encoder, q_encoder = retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(q_encoder, '/opt/ml/models/q_encoder.pt')\n",
    "torch.save(p_encoder, '/opt/ml/models/p_encoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
