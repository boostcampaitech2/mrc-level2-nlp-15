{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, RobertaModel,\n",
    "    BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "from adamp import AdamP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "class RoBERTaEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(RoBERTaEncoder, self).__init__(config)\n",
    "\n",
    "        # self.bert = BertModel(config)\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            # token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        # outputs = self.bert(\n",
    "        #     input_ids,\n",
    "        #     attention_mask=attention_mask,\n",
    "        #     token_type_ids=token_type_ids\n",
    "        # )\n",
    "        \n",
    "        outputs = self.roberta(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[\n",
    "            \"pooler_output\"\n",
    "        ]  # [CLS] token's hidden featrues(hidden state)\n",
    "\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/data/train_dataset')\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwer\n",
    "class DenseRetrieval:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        p_encoder,\n",
    "        q_encoder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        학습과 추론에 사용될 여러 셋업을 마쳐봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "\n",
    "    def train(self, args=None, tokenizer = None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        if tokenizer is None :\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        for i in (range(len(self.dataset))) :\n",
    "            if i == 0 :\n",
    "                q_seqs = tokenizer(\n",
    "                    self.dataset[i]['question'],\n",
    "                    padding='max_length',\n",
    "                    truncation = True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                p_seqs = tokenizer(\n",
    "                    self.dataset[i]['context'],\n",
    "                    truncation = True,\n",
    "                    stride = 128,\n",
    "                    padding='max_length',\n",
    "                    return_overflowing_tokens=True,\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "\n",
    "                p_seqs.pop('overflow_to_sample_mapping')\n",
    "                p_seqs.pop('offset_mapping')\n",
    "\n",
    "                for k in q_seqs.keys() :\n",
    "                    q_seqs[k] = q_seqs[k].tolist()\n",
    "                    p_seqs[k] = p_seqs[k].tolist()\n",
    "            else :\n",
    "                tmp_q_seq = tokenizer(\n",
    "                    self.dataset[i]['question'],\n",
    "                    padding='max_length',\n",
    "                    truncation = True,\n",
    "                    return_tensors='pt')\n",
    "                tmp_p_seq = tokenizer(\n",
    "                    self.dataset[i]['context'],\n",
    "                    truncation = True,\n",
    "                    stride = 128,\n",
    "                    padding='max_length',\n",
    "                    return_overflowing_tokens=True,\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "\n",
    "                tmp_p_seq.pop('overflow_to_sample_mapping')\n",
    "                tmp_p_seq.pop('offset_mapping')\n",
    "\n",
    "                for k in tmp_p_seq.keys() :\n",
    "                    tmp_p_seq[k] = tmp_p_seq[k].tolist()\n",
    "                    tmp_q_seq[k] = tmp_q_seq[k].tolist()\n",
    "\n",
    "\n",
    "                for j in range(len(tmp_p_seq['input_ids'])) :\n",
    "                    q_seqs['input_ids'].append(tmp_q_seq['input_ids'][0])\n",
    "                    # q_seqs['token_type_ids'].append(tmp_q_seq['token_type_ids'][0])\n",
    "                    q_seqs['attention_mask'].append(tmp_q_seq['attention_mask'][0])\n",
    "                    p_seqs['input_ids'].append(tmp_p_seq['input_ids'][j])\n",
    "                    # p_seqs['token_type_ids'].append(tmp_p_seq['token_type_ids'][j])\n",
    "                    p_seqs['attention_mask'].append(tmp_p_seq['attention_mask'][j])\n",
    "\n",
    "        for k in q_seqs.keys() :\n",
    "            q_seqs[k] = torch.tensor(q_seqs[k])\n",
    "            p_seqs[k] = torch.tensor(p_seqs[k])\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            p_seqs['input_ids'], \n",
    "            p_seqs['attention_mask'], \n",
    "            # p_seqs['token_type_ids'],\n",
    "            q_seqs['input_ids'], \n",
    "            q_seqs['attention_mask'], \n",
    "            # q_seqs['token_type_ids']\n",
    "            )\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=args.per_device_train_batch_size, shuffle = True)\n",
    "\n",
    "        no_decay = [\"bias\" ,\"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamP(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            # eps=args.adam_epsilon\n",
    "        )\n",
    "\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "        \n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "        self.q_encoder.train()\n",
    "        self.p_encoder.train()\n",
    "        for epoch, _ in enumerate(train_iterator):\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            loss_value=0 # Accumulation할 때 진행\n",
    "            losses = 0\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                if torch.cuda.is_available():\n",
    "                    batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "                p_inputs = {'input_ids': batch[0],\n",
    "                            'attention_mask': batch[1],\n",
    "                            # 'token_type_ids': batch[2]\n",
    "                            }\n",
    "                \n",
    "                q_inputs = {'input_ids': batch[2],\n",
    "                            'attention_mask': batch[3],\n",
    "                            # 'token_type_ids': batch[5]\n",
    "                            }\n",
    "\n",
    "                p_outputs = self.p_encoder(**p_inputs)  # (batch_size, emb_dim)\n",
    "                q_outputs = self.q_encoder(**q_inputs)  # (batch_size, emb_dim)\n",
    "\n",
    "                # Calculate similarity score & loss\n",
    "                sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "\n",
    "                # target: position of positive samples = diagonal element \n",
    "                # targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "                targets = torch.arange(0, len(p_inputs['input_ids'])).long()\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    targets = targets.to('cuda')\n",
    "                    \n",
    "                sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                loss = F.nll_loss(sim_scores, targets)\n",
    "                losses += loss.item()\n",
    "                if step % 100 == 0 :\n",
    "                    print(f'{epoch}epoch loss: {losses/(step+1)}') # Accumulation할 경우 주석처리\n",
    "\n",
    "                # ################ACCUMULATION###############################\n",
    "                # if (step+1) % args.gradient_accumulation_steps == 0 :\n",
    "                #     optimizer.step()\n",
    "                #     scheduler.step()\n",
    "                #     self.q_encoder.zero_grad()\n",
    "                #     self.p_encoder.zero_grad()\n",
    "\n",
    "                # losses += loss.item()\n",
    "                # if (step+1) % 64 == 0:\n",
    "                #     train_loss = losses / 64\n",
    "                #     print(f'training loss: {train_loss:4.4}')\n",
    "                #     losses = 0\n",
    "                # ###########################################################\n",
    "                self.q_encoder.zero_grad()\n",
    "                self.p_encoder.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                # global_step += 1\n",
    "                \n",
    "                #torch.cuda.empty_cache()\n",
    "                del p_inputs, q_inputs\n",
    "\n",
    "        return self.p_encoder, self.q_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Q_Encoder & P_Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RoBERTaEncoder: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RoBERTaEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RoBERTaEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RoBERTaEncoder were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RoBERTaEncoder: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RoBERTaEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RoBERTaEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RoBERTaEncoder were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01 # https://github.com/clovaai/AdamP#usage\n",
    ")\n",
    "model_checkpoint = \"klue/roberta-base\"\n",
    "\n",
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_input_names = [\"input_ids\", \"attention_mask\"])\n",
    "p_encoder = RoBERTaEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "q_encoder = RoBERTaEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sat Oct 30 10:28:04 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   37C    P0    37W / 250W |  30035MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bcd25059ae426da463dd5d8b6fff0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0epoch loss: 2.8824448585510254\n",
      "0epoch loss: 2.8657144933643908\n",
      "0epoch loss: 2.834868020679227\n",
      "0epoch loss: 2.8186536912506206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 1/5 [06:33<26:14, 393.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefe75fb949e4e59865269e55e1f1159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1epoch loss: 2.788330078125\n",
      "1epoch loss: 2.616762633370881\n",
      "1epoch loss: 2.4828301780852513\n",
      "1epoch loss: 2.173629090041417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 2/5 [13:06<19:39, 393.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9f8ccc4f224ed79be2c331c0e8e4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2epoch loss: 0.9492713809013367\n",
      "2epoch loss: 0.8629638266445386\n",
      "2epoch loss: 0.7986272352548381\n",
      "2epoch loss: 0.7252614517693108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [19:39<13:06, 393.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44584cd0a3674c0db797225cc163f8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3epoch loss: 0.138513445854187\n",
      "3epoch loss: 0.3210593914926642\n",
      "3epoch loss: 0.31473527405763146\n",
      "3epoch loss: 0.2985780317695236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 4/5 [26:13<06:33, 393.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ed43fc613348bb95de30c9b2b1c93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=350.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4epoch loss: 0.3123779594898224\n",
      "4epoch loss: 0.16973136275047712\n",
      "4epoch loss: 0.17104625576801263\n",
      "4epoch loss: 0.1630939555407016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [32:47<00:00, 393.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retriever는 아래와 같이 사용할 수 있도록 코드를 짜봅시다.\n",
    "retriever = DenseRetrieval(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder\n",
    ")\n",
    "p_encoder, q_encoder = retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "corpus = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ")  # set 은 매번 순서가 바뀌므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2c43e64fa74edea35f0866bb934f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56737.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# p_encoder = retriever.p_encoder\n",
    "# q_encoder = retriever.q_encoder\n",
    "with torch.no_grad() :\n",
    "    p_encoder.eval()\n",
    "\n",
    "    p_embs = []\n",
    "    for p in tqdm(corpus) :\n",
    "        p = tokenizer(p, padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "        p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "        p_embs.append(p_emb)\n",
    "p_embs = torch.Tensor(p_embs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = '/opt/ml/mrc-level2-nlp-15/custom/passage_embedding_special_shuffle_100.bin'\n",
    "with open(file_path, 'wb') as file :\n",
    "    pickle.dump(p_embs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder.cpu()\n",
    "del p_encoder\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sat Oct 30 10:29:29 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   37C    P0    37W / 250W |   2291MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(q_encoder, '/opt/ml/mrc-level2-nlp-15/custom/q_encoder_special_shuffle_100.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Relavant Documnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "corpus = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ")  # set 은 매번 순서가 바뀌므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/roberta-base', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(file_path, 'rb') as file :\n",
    "    p_embs = pickle.load(file)\n",
    "p_embs = p_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_encoder = torch.load('/opt/ml/mrc-level2-nlp-15/custom/q_encoder_special_shuffle_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1개 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dataset['validation']['question'][0]\n",
    "queries = dataset['validation']['question'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad() :\n",
    "    q_encoder.eval()\n",
    "    q_seqs_val = tokenizer([query], padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "    q_emb = q_encoder(**q_seqs_val).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.1055,  9.2019,  9.8460,  ...,  6.1538, 12.8961, 14.9071]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "dot_prod_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18691, 17136, 15977,  ..., 55737,  7144, 42082]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad() :\n",
    "    q_encoder.eval()\n",
    "    q_seqs_val = tokenizer(queries, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "    q_emb = q_encoder(**q_seqs_val).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sort_result[0]\n",
    "ranks = sort_result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " 처음으로 부실 경영인에 대한 보상 선고를 받은 회사는? \n",
      "\n",
      "[Ground truth passage]\n",
      "순천여자고등학교 졸업, 1973년 이화여자대학교를 졸업하고 1975년 제17회 사법시험에 합격하여 판사로 임용되었고 대법원 재판연구관, 수원지법 부장판사, 사법연수원 교수, 특허법원 부장판사 등을 거쳐 능력을 인정받았다. 2003년 최종영 대법원장의 지명으로 헌법재판소 재판관을 역임하였다.\\n\\n경제민주화위원회(위원장 장하성이 소액주주들을 대표해 한보철강 부실대출에 책임이 있는 이철수 전 제일은행장 등 임원 4명을 상대로 제기한 손해배상청구소송에서 서울지방법원 민사합의17부는 1998년 7월 24일에 \"한보철강에 부실 대출하여 은행에 막대한 손해를 끼친 점이 인정된다\"며 \"원고가 배상을 청구한 400억원 전액을 은행에 배상하라\"고 하면서 부실 경영인에 대한 최초의 배상 판결을 했다. \\n\\n2004년 10월 신행정수도의건설을위한특별조치법 위헌 확인 소송에서 9인의 재판관 중 유일하게 각하 견해를 내었다. 소수의견에서 전효숙 재판관은 다수견해의 문제점을 지적하면서 관습헌법 법리를 부정하였다. 전효숙 재판관은 서울대학교 근대법학교육 백주년 기념관에서 열린 강연에서, 국회가 고도의 정치적인 사안을 정치로 풀기보다는 헌법재판소에 무조건 맡겨서 해결하려는 자세는 헌법재판소에게 부담스럽다며 소회를 밝힌 바 있다. \n",
      "\n",
      "Top-1 passage with score 23.8853\n",
      "진실규명 결정을 받은 김지태의 유가족들은 2010년 6월에야 법원에 정수장학회와 국가를 상대로 낸 주식양도 등 청구소송을 냈다. 김씨 측은 \"박 전 대통령이 사망하고 난 이후 1980년에 토지 반환청구 의사를 표시했고, 과거사정리위원회의 진실규명 결정을 송달받은 이후 손해배상을 청구한 것이므로 공소시효가 남아있다\"고 주장했다.\\n\\n하지만 1심 재판부는 \"소멸시효가 지났다\"며 김씨 측의 청구를 기각했고, 2심 재판부도 김씨가 국가의 강박행위로 인해 재산을 헌납한 것은 인정하면서도 의사결정권이 완전히 박탈당한 상태는 아니었던 것으로 판단해 원고 패소 판결했다. 2012년 2월 24일 서울중앙지법 민사합의17부(재판장 염원섭)에 의해 5.16장학회의 ‘헌납’ 과정에서 강압이 있었음이 다시 한 번 입증되었다. 하지만 재판부는 김영우가 제기한 과거 부일장학회의 주식반환에 대해서는 공소시효 소멸을 이유로 기각하였다. 이에 국가의 범죄에 대해서는 공소 시효의 범위를 폭넓게 인정해줘야 한다는 비판도 제기되었다. \\n\\n대법원은 2014년 2월 13일 김지태씨 장남 영구 씨를 비롯한 유가족 6명이 정수장학회와 국가를 상대로 낸 주식양도 등 청구소송 상고심에서 심리불속행 기각 결정을 내렸다. '심리불속행'은 상고 사건 가운데 상고 대상이 아니라고 판단되는 사건은 더이상 심리하지 않고 기각하는 제도다.\n",
      "Top-2 passage with score 23.0414\n",
      "2008년 7월 신영철 대법관의 촛불 관련 사건의 몰아주기 배당이 문제가 되어 서울중앙지법 형사단독 판사가 문제 제기를 하였으며, 2009년 2월 19일 광고불매 소비자운동을 한 24명이 1심에서 전원 유죄 판결을 받았다. 2009년 3월 19일 언론소비자주권 국민캠페인은 신영철 대법관을 《직권남용권리행사방해죄 및 국회에서의 증언·감정 등에 관한 법률》 위반 혐의로 서울중앙지검에 고발하였다. 2009년 5월 13일 이용훈 대법원장은 신 대법관에게 엄중경고와 사건에 대한 유감을 표명하였다.  신영철은 야당과 시민단체들의 사퇴압박이 이어졌으나 끝내 사퇴하지 않았다.\n",
      "Top-3 passage with score 22.7350\n",
      "2008년 7월 14일 촛불집회 관련 재판의 배당에 대하여 이정렬(40ㆍ연수원 23기) 서울동부지법 판사와 송승용(35ㆍ연수원 29기) 울산지법 판사 등이 문제를 제기하였다.\n",
      "\n",
      "2009년 2월 10일 국회에서 열린 신영철 〈대법관 임명동의에 관한 인사청문회〉에서 촛불집회관련 사건들의 배당과정에서 문제가 없었는지에 대해 질문하자 ‘컴퓨터 프로그램에 의해 기계적으로 배당됐겠거니 생각하고 있다’고 답변했다.\n",
      "\n",
      "2009년 2월 26일 대법원, 몰아주기 배당 의혹 조사 후 \"문제없다\"는 결론을 내리고 국회 법사위 보고되었다. 그러나 2009년 3월 5일 KBS 신영철 대법관 '촛불재판 판결 독촉 이메일' 발송 사실이라는  보도를 하였다. 3월 9일 경향신문은 신 대법관, 전화로도 재판 개입을 했다고 보도하였다. \n",
      "\n",
      "2009년 3월 6일 대법원 진상조사단을 구성하여, 조사를 착수하여, 촛불집회의 재판과 관련하여, 부당한 재판 압력을 가했다는 이유로 2009년 3월 19일 대법원공직자 윤리위원회에 공식 회부되었다. \n",
      "\n",
      "2009년 5월 8일 대법원 윤리위원회는 2008년 중앙지법원장 재직시절 촛불집회 관련사건을 맡고 있던 판사들에게 전화 또는 이메일을 보낸 것은 법원장의 직무감독 범위를 넘어서 재판에 관여한 행위라는 결론을 내리고, 신 대법관에게 경고 또는 주의촉구 등의 조치를 할 것을 권고했다.\n",
      "Top-4 passage with score 22.7315\n",
      "1994년 8월 12일 건축물 철거 처분 취소 청구소송 재판을 방청하던 소송 당사자인 60세 김모씨가 상고기각 판결이 있자 \"당신은 매국노\"라며 고함을 지르고 소란을 피워 즉심에 회부된 사건에서 구류 3일 감치 명령을 선고했다. \\n\\n1996년 7월 3일 지방자치 선거에서 상대 후보의 전과사실을 공개해 후보자 비방으로 기소된 동대문구의원 출마자인 정병걸에 대한 상고심에서 \"표현의 자유라는 공공의 이익과 개인의 명예보호라는 사적 이익이 충돌할 때 전자가 현저히 중요할 때만이 아니라 후보자에 대한 충분한 정보가 제공돼야 하는 공직선거에서 후보자에 대한 충분한 정보가 제공되어야 하는 만큼 설령 사적이익보다 우월하지 않더라도 공공의 이익이 존재하고 거기에 상당한 이유가 있다\"는 이유로 유죄를 판결한 원심을 파기하고 무죄 취지로 서울고등법원으로 환송했다. \\n\\n1996년 7월 28일 1993년에 있었던 부천시 초등학교 여학생 살인사건의 중학생 피고인에 대해 \"혐의를 확신하게 증거가 없음에도 유죄를 선고한 원심은 채증법칙 위배의 위법이 있다\"는 이유로 유죄를 선고한 원심을 파기하고 서울고등법원으로 \\n\\n1996년 8월 27일 여성 전화교환원 정년을 다른 직원보다 5년이 낮은 53세로 제한한 것은 근로기준법상 남녀차별대우 금지조항에 해당하지 않는다고 했다. 이 판결로 김형선은 1997년 3월 8일 여성의 날에 한국여성단체협의회가 여권 신장 걸림돌에 선정했다. \\n\\n1997년 7월 6일 특정경제범죄가중처벌법상 수재 혐의로 기소된 문모씨 상고심에서 \"피고인의 자백이 임의적 진술이라기 보다 30여시간 잠을 재우지 않은 채 검사가 교대로 신문을 하면서 회유했다는 의심이 든다\"는 이유로 \"피의자 신문조서의 증거능력이 없다\"고 하면서도 \"금품 제공자의 검찰조사 내용과 압수된 경비 장부에 기재된 금융거래 내용 등 그밖의증거로 판단하여 유죄가 인정된다\"며 징역1년6월 집행유예3년을 선고한 원심을 확정했다. \\n\\n1996년 10월 30일에 \"약정서에 최대한 협조한다는 문구를 썻더라도 그 의무를 법적으로 이행할 수는 없지만 사정이 허락하는 한 지키겠다\"는 뜻으로 강제력을 가진 것으로 볼 수 없다는 이유로 원심을 파기하고 창원지방법원으로 환송했다. \\n\\n1997년 7월 25일에 아랍인 교수로 위장해 국내에서 간첩활동을 한 혐의로 징역15년 자격정지12년을 선고받은 정수일(일명 무하마드 깐수)의 국가보안법위반 사건 상고심에서 \"신문 방송 등에 보도돼 이미 공지의 사실이 된 사항은 국가보안법상 국가기밀이 아니다\"는 이유로 국가기밀탐지,수집전달 부분 중에서 일부에 대해 무죄를 판단하면서 서울고등법원으로 환송했다 \\n\\n1998년 7월 6일에 삼성전자 온양공장 주변 주민들이 회사를 상대로 제기한 송전선로 철거 요구 소송 상고심에서 \"삼성전자 측이 가처분 소송 중에 폭력적인 방법으로 공사를 시행한 것은 잘못\"이라며 원고패소 판결을 한 원심을 파기하고 서울고등법원으로 환송했다. \\n\\n1998년 10월 30일에 교통사고 가해자로부터 \"사건 처리를 하지 말고 합의할 수 있도록 도와달라\"는 부탁과 함께 20만원을 받아 징계위원회에서 해임된 전직 경찰관이 서울지방경찰청장을 상대로 제기한 해임취소 소송 상고심에서 \"소액이라도 돈을 받고 합의를 종용하고 사건 처리를 미룬 것은 해임 사유에 해당한다\"는 이유로 무죄판결한 원심을 깨고 서울고등법원으로 환송했다.\n",
      "Top-5 passage with score 22.6724\n",
      "서울형사지방법원 판사로 재직하던 1974년 7월 19일에 한일병원 원장으로 있을 때 연구실적이 없는 직원 52명에게 학술연구비를 지급하거나 직원이 소개한 환자들의 진료비를 깍아주는 등 병원에 3000여만원의 손해를 끼친 업무상 배임과 거래 제약회사가 병원 운영보조비 형식으로 주는 의약품 할인액 등 120만원을 횡령한 손선석에 대해 벌금 20만월 선고했으며  중국요리점 아서원 등기부상 대표가 사망하자 장녀가 단속 상속인인 것 처럼 등기부를 위조해 롯데제과공업 대표 신춘호에게 대지와 건물을 6천만원을 받고 매도한 관리인 2명에게 횡령죄를 적용하여 징역1년 집행유예2년을 선고했다. \n",
      "\n",
      "대구고등법원 형사2부 재판장으로 재직하던 1987년 11월 12일에 형제복지원 사건 항소심 공판에서 유죄를 인정했으나 정상을 참작하여 주범에 대해 징역4년을 선고하는 등 피고인 7명에 대해 형량을 낮추고 주범의 아들에게 무죄를 선고했다. 이후 대법원에서 특수감금죄는 정당방위라는 이유로 파기환송됐다.\n",
      "\n",
      "서울고등법원 형사3부 재판장으로 재직하던 1989년 10월 26일에 성남 일대 유흥가에서\n",
      "폭력을 휘두르다 구속되어 1심에서 징역1년6월이 선고된 솜리파 조직폭력배 피고인 9명에 대해 범죄단체 구성죄를 적용해 징역5년을 선고했다.  10월 30일에  절도와 강도는 형법의 같은 장에 적용되어 있으나 행위\n",
      "유형이 다르므로 소년 시절의 절도죄와 그 뒤 두 차례의 강도죄만으로 절도의 상습성이 있다고 볼 수 없다고 하면서 보호감호 청구를 기각하면서 특수절도죄만을 인정하며 징역1년을 선고했다.  1990년 1월 19일에 집회 도중 집회시간이 끝났다는 이유로 해산을 종용한 경찰과 시비를 벌여 집시법과 폭력행위 처벌에 관한 법률 위반으로 구속되어 1심에서 징역3년을 선고받은 피고인에 대해 집시법 위바에 대해 무죄를 선고하면서 특수공무집행방해치상죄만 적용해 징역2년 집행유예4년을 선고했다.  1월 25일에 대전시내 유흥업소 주변에서 폭력을 일삼다 범죄단체 조직 등 혐의로 구속되어 1심에서 징역1년~집행유예2년을 선고받은 피고인 5명에 대한 항소심에서 \"범죄단체란 구성원들이 최소한의 통솔 체계를 갖춰야 하는데 피고인들은 범죄를 공동 목적으로 하는 결합체를 조직한 것으로 볼 수 없다\"며 이들 모두에게 무죄를 선고했다.  1월 29일에 10살 여자 어린이를 강제로 추행한 뒤에 살해하여 구속되어 1심에서 징역12년을 선고받은 이대우에게 무기징역을 선고했다.  5월 25일에 동양공업전문대학 학생 설인종 상해치사 사건으로 기소된 연세대학교, 고려대학교 학생 9명에 대한 항소심에서 1심에서 징역8년씩을 선고받은 양영준, 김중표에게 징역4년, 1심에서 징역7년을 선고받은 2명에게 각각 징역4년과 징역3년, 김현철에게 징역3년 집행유예4년,  1심에서 징역3년 집행유예5년을 선고받은 그외 4명의 항소를 기각했다.  또, 같은 날에는 전투경찰 복무 중에 탈영하여 전투경찰 해체 투쟁위원회를 결성해 위원장을 맡아서 국가보안법과 전경대 설치에 관한 법률 위반 등으로 구속되어 1심에서 징역2년 자격정지2년을 선고받고 항소한 양승균(23세)에게 징역1년6월 자격정지2년을 선고하면서 \"전두환, 이순자를 체포해 구속하라\"는 성명서를 제작해 배포한 것에 대한 이적표현물 제작, 반포는 무죄를 선고했다.  시영아파트 가짜 입주권을 장당 1500만원~2300만원씩 받고 팔아 30억원을 챙긴 혐의로 구속된 영등포구청 주택정비계장 박사원(57세)에 대해 항소를 기각하면서 원심대로 징역7년을 선고하고 박씨의 부인에게 원심을 깨고 징역5년을 선고했다.  6월 1일에 <민족해방운동사>란 대형 걸개 그림을 슬라이드로 제작해 평양축전에 보내 국가보안법위반으로 구속되어 1심에서 징역7년 자격정지7년을 선고받은 민족민중미술운동연합 공동대표 홍성담에 대해 원심대로 선고했다.  6월 11일에 평양축전 참가와 관련하여 국가보안법위반 혐의로 구속되어 1심에서 징역10년 자격정지10년과 징역8년 자격정지8년을 각각 선고받은 임수경과 문규현 신부에 대해 \"감상적 통일론이든 이기적 영웅주의에 기초한 것이든 국민 여론을 무시하고 독선에 빠져 밀입북하여 자유민주적 기본질서를 위배한 사실이 인정된다\"고 하면서도 \"시대적 상황에 따라 평가를 달리할 수 있다\"며 피고인 모두 징역5년 자격정지5년을 선고했다. 7월 28일에 대낮에 남의 집에 들어가 아기를 돌보고 있던 부인을 칼로 위협해 금품을 요구하고 두 차례 성폭행하여 강도강간 혐의로 구속되어 1심에서 장기3년 단기2년6월의 징역을 선고받고 항소한 고등학생 오모군에 대해 장기5년 단기3년의 징역을 선고했다.  8월 10일에 설계업자들로부터 뇌물을 받아 구속되어 1심에서 징역3년 추징금 3580만원을 선고받은 전 건설부 도로국장 이경진(53세)에게  수뢰액이 2,000만원 이상인 때에는 사형·무기 또는 10년 이상의 징역에 처하는 특정범죄가중처벌법 위반 뇌물수수죄를 적용하여 징역3년 집행유예5년 추징금 3580만원을 선고했다. 같은 혐의로 1심에서 징역2년6월, 추징금 1100만원을 선고받은 전 한국전력 지중선사업처장 윤석공(56세)에게 징역2년6월 집행유예4년 추징금 1100만원을 선고했다. \n",
      "\n",
      "서울고등법원 민사13부 재판장으로 재직하던 1991년 8월 29일에 \"1989년 9월 MBC노조파업 걸개그림 사건을 보도한 동아일보가 MBC의 명예를 훼손했다\"며 MBC가 동아일보를 상대로 사죄광고와 손해배상을 요구하며 제기한 소송에서 원고패소 판결한 원심을 인정하며 항소를 기각했다.\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "print(\"[Search query]\\n\", query, \"\\n\")\n",
    "print(\"[Ground truth passage]\")\n",
    "print(dataset['validation']['context'][0], \"\\n\")\n",
    "\n",
    "for i in range(k):\n",
    "  print(\"Top-%d passage with score %.4f\" % (i+1, scores[0].squeeze()[i]))\n",
    "  print(corpus[ranks[0][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size:16/epoch:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc778060d70548908496b05230e82c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 50)\n",
    "\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_50 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3625\n"
     ]
    }
   ],
   "source": [
    "correct_length = []\n",
    "for i in range(len(cqas_50)) :\n",
    "    if cqas_50['original_context'][i] in cqas_50['context'][i] :\n",
    "        correct_length.append(i)\n",
    "print(len(correct_length) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size:16/epoch:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045e6de043714e819e8d3aa9583a8c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3625\n"
     ]
    }
   ],
   "source": [
    "correct_length = []\n",
    "for i in range(len(cqas)) :\n",
    "    if cqas['original_context'][i] in cqas['context'][i] :\n",
    "        correct_length.append(i)\n",
    "print(len(correct_length) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cqas.to_csv('/opt/ml/mrc-level2-nlp-15/custom/valid_dpr_b16_e10_t50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3625\n"
     ]
    }
   ],
   "source": [
    "correct_length_50 = []\n",
    "for i in range(len(cqas_50)) :\n",
    "    if cqas_50['original_context'][i] in cqas_50['context'][i] :\n",
    "        correct_length_50.append(i)\n",
    "print(len(correct_length_50) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7416666666666667\n"
     ]
    }
   ],
   "source": [
    "correct_length_100 = []\n",
    "for i in range(len(cqas_100)) :\n",
    "    if cqas_100['original_context'][i] in cqas_100['context'][i] :\n",
    "        correct_length_100.append(i)\n",
    "print(len(correct_length_100) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7625\n"
     ]
    }
   ],
   "source": [
    "correct_length_150 = []\n",
    "for i in range(len(cqas_150)) :\n",
    "    if cqas_150['original_context'][i] in cqas_150['context'][i] :\n",
    "        correct_length_150.append(i)\n",
    "print(len(correct_length_150) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825\n"
     ]
    }
   ],
   "source": [
    "correct_length_200 = []\n",
    "for i in range(len(cqas_200)) :\n",
    "    if cqas_200['original_context'][i] in cqas_200['context'][i] :\n",
    "        correct_length_200.append(i)\n",
    "print(len(correct_length_200) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqas_200.to_csv('/opt/ml/custom/valid_dpr_200.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825\n"
     ]
    }
   ],
   "source": [
    "correct_length_300 = []\n",
    "for i in range(len(cqas_300)) :\n",
    "    if cqas_300['original_context'][i] in cqas_300['context'][i] :\n",
    "        correct_length_300.append(i)\n",
    "print(len(correct_length_200) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqas.to_csv('/opt/ml/custom/valid_dpr.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size:16/epoch:40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9a326c440a4138b1c66820f23af793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 200)\n",
    "\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_50 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "correct_length = []\n",
    "for i in range(len(cqas_50)) :\n",
    "    if cqas_50['original_context'][i] in cqas_50['context'][i] :\n",
    "        correct_length.append(i)\n",
    "print(len(correct_length) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqas_50.to_csv('/opt/ml/custom/valid_dpr_b16_e40_t200.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size:16/epoch:100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d01b7965a7d485785d4cd6024ebdab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 200)\n",
    "\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_50 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8166666666666667\n"
     ]
    }
   ],
   "source": [
    "correct_length = []\n",
    "for i in range(len(cqas_50)) :\n",
    "    if cqas_50['original_context'][i] in cqas_50['context'][i] :\n",
    "        correct_length.append(i)\n",
    "print(len(correct_length) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqas_50.to_csv('/opt/ml/custom/valid_dpr_b16_e100_t200.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size 128/epoch:10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc9e6cb0d1e40a9a22ca9258f97fcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_50 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6083333333333333\n"
     ]
    }
   ],
   "source": [
    "correct_length_50 = []\n",
    "for i in range(len(cqas_50)) :\n",
    "    if cqas_50['original_context'][i] in cqas_50['context'][i] :\n",
    "        correct_length_50.append(i)\n",
    "print(len(correct_length_50) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_length_50 = []\n",
    "for i in range(len(cqas_20)) :\n",
    "    if cqas_20['original_context'][i] in cqas_20['context'][i] :\n",
    "        correct_length_20.append(i)\n",
    "print(len(correct_length_20) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size 128/epoch:5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789873e1b3364e52a9ddd332bb695c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 50)\n",
    "\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_50 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30416666666666664\n"
     ]
    }
   ],
   "source": [
    "correct_length_50 = []\n",
    "for i in range(len(cqas_50)) :\n",
    "    if cqas_50['original_context'][i] in cqas_50['context'][i] :\n",
    "        correct_length_50.append(i)\n",
    "print(len(correct_length_50) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size 128/epoch:20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcc5c871b4f431f91c27304ec3f48c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 20)\n",
    "\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_100 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5041666666666667\n"
     ]
    }
   ],
   "source": [
    "correct_length_100 = []\n",
    "for i in range(len(cqas_100)) :\n",
    "    if cqas_100['original_context'][i] in cqas_100['context'][i] :\n",
    "        correct_length_100.append(i)\n",
    "print(len(correct_length_100) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_length_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size 16/epoch:10/Special/NoShuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relavant_doc(queries, q_encoder, p_embs, k=1) :\n",
    "    with torch.no_grad() :\n",
    "        q_encoder.eval()\n",
    "        q_seqs_val = tokenizer(queries, padding='max_length',truncation=True,return_tensors='pt').to(device)\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    dot_prod_scores = torch.mm(q_emb, p_embs.T)\n",
    "    sort_result = torch.sort(dot_prod_scores, dim=1, descending=True)\n",
    "\n",
    "    scores, ranks = sort_result[0], sort_result[1]\n",
    "\n",
    "    result_scores = []\n",
    "    result_indices = []\n",
    "    for i in range(len(ranks)) :\n",
    "        result_scores.append(scores[i].tolist()[:k])\n",
    "        result_indices.append(ranks[i].tolist()[:k])\n",
    "    \n",
    "    return result_scores, result_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bff4b58370a4933a9876da005c2e857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 100)\n",
    "\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_100 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6916666666666667\n"
     ]
    }
   ],
   "source": [
    "correct_length_100 = []\n",
    "for i in range(len(cqas_100)) :\n",
    "    if cqas_100['original_context'][i] in cqas_100['context'][i] :\n",
    "        correct_length_100.append(i)\n",
    "print(len(correct_length_100) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size 16/epoch:10/Special/Shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583efac1e46f49c98579311c5a793881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 100)\n",
    "\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_100 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775\n"
     ]
    }
   ],
   "source": [
    "correct_length_100 = []\n",
    "for i in range(len(cqas_100)) :\n",
    "    if cqas_100['original_context'][i] in cqas_100['context'][i] :\n",
    "        correct_length_100.append(i)\n",
    "print(len(correct_length_100) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 시작 - Batch_size 16/epoch:100/Special/Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6049a2085e45409c92ccf9708708ecb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dense retrieval: ', max=240.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_scores, doc_indices = get_relavant_doc(dataset['validation']['question'], q_encoder, p_embs, k = 50)\n",
    "\n",
    "total = []\n",
    "for idx, example in enumerate(\n",
    "        tqdm(dataset['validation'], desc=\"Dense retrieval: \")\n",
    "    ):\n",
    "        tmp = {\n",
    "            # Query와 해당 id를 반환합니다.\n",
    "            \"question\": example[\"question\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "            \"context_id\": doc_indices[idx],\n",
    "            \"context\": \" \".join(  # 기존에는 ' '.join()\n",
    "                [corpus[pid] for pid in doc_indices[idx]]\n",
    "            ),\n",
    "        }\n",
    "        if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "            # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "            tmp[\"original_context\"] = example[\"context\"]\n",
    "            tmp[\"answers\"] = example[\"answers\"]\n",
    "        total.append(tmp)\n",
    "\n",
    "cqas_100 = pd.DataFrame(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7458333333333333\n"
     ]
    }
   ],
   "source": [
    "correct_length_100 = []\n",
    "for i in range(len(cqas_100)) :\n",
    "    if cqas_100['original_context'][i] in cqas_100['context'][i] :\n",
    "        correct_length_100.append(i)\n",
    "print(len(correct_length_100) / len(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
