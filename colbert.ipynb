{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# colbert\n",
    "stanford-futuredata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, RobertaModel,\n",
    "    BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "from torch.utils.data import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/data/train_dataset')\n",
    "train_dataset = dataset['train']\n",
    "model_checkpoint = 'klue/bert-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colbert , tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "import string\n",
    "import pickle\n",
    "import os.path as p\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datasets import load_from_disk\n",
    "from transformers import AdamW, TrainingArguments\n",
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizerFast, BertConfig\n",
    "\n",
    "\n",
    "class QueryTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tok = BertTokenizerFast.from_pretrained('klue/bert-base')\n",
    "\n",
    "        self.Q_marker_token, self.Q_marker_token_id = \"[Q]\", self.tok.convert_tokens_to_ids(\"[unused0]\")\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "        self.mask_token, self.mask_token_id = self.tok.mask_token, self.tok.mask_token_id\n",
    "        self.query_maxlen = self.tok.model_max_length\n",
    "\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.Q_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix + [self.mask_token] * (self.query_maxlen - (len(lst) + 3)) for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.Q_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix + [self.mask_token_id] * (self.query_maxlen - (len(lst) + 3)) for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        # add placehold for the [Q] marker\n",
    "        batch_text = [\". \" + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(\n",
    "            batch_text, padding=\"longest\", truncation=True, return_tensors=\"pt\", max_length=self.tok.model_max_length\n",
    "        )\n",
    "\n",
    "        ids, mask = obj[\"input_ids\"], obj[\"attention_mask\"]\n",
    "\n",
    "        # postprocess for the [Q] marker and the [MASK] augmentation\n",
    "        ids[:, 1] = self.Q_marker_token_id\n",
    "        ids[ids == 0] = self.mask_token_id\n",
    "\n",
    "        if bsize:\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "\n",
    "class DocTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tok = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.D_marker_token, self.D_marker_token_id = \"[D]\", self.tok.convert_tokens_to_ids(\"[unused1]\")\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "\n",
    "        # assert self.D_marker_token_id == 1\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.D_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.D_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], type(batch_text)\n",
    "\n",
    "        # add placehold for the [D] marker\n",
    "        batch_text = [\". \" + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(\n",
    "            batch_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=self.tok.model_max_length\n",
    "        )\n",
    "\n",
    "        ids, mask = obj[\"input_ids\"], obj[\"attention_mask\"]\n",
    "\n",
    "        # postprocess for the [D] marker\n",
    "        ids[:, 1] = self.D_marker_token_id\n",
    "\n",
    "        if bsize:\n",
    "            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches, reverse_indices\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "\n",
    "def tensorize_triples(query_tokenizer, doc_tokenizer, queries, positives, negatives, bsize):\n",
    "    # assert len(queries) == len(positives) == len(negatives)\n",
    "    # assert bsize is None or len(queries) % bsize == 0\n",
    "\n",
    "    N = len(queries)\n",
    "    queries = queries.to_list()\n",
    "    # print(f'queries-----------------------------------------------------')\n",
    "    # print(queries)\n",
    "    Q_ids, Q_mask = query_tokenizer.tensorize(queries)\n",
    "\n",
    "    positives = positives.to_list()\n",
    "    negatives = negatives.to_list()\n",
    "\n",
    "    D_ids, D_mask = doc_tokenizer.tensorize(positives + negatives)\n",
    "    D_ids, D_mask = D_ids.view(2, N, -1), D_mask.view(2, N, -1)\n",
    "\n",
    "    # Compute max among {length of i^th positive, length of i^th negative} for i \\in N\n",
    "    maxlens = D_mask.sum(-1).max(0).values\n",
    "\n",
    "    # Sort by maxlens\n",
    "    indices = maxlens.sort().indices\n",
    "    Q_ids, Q_mask = Q_ids[indices], Q_mask[indices]\n",
    "    D_ids, D_mask = D_ids[:, indices], D_mask[:, indices]\n",
    "\n",
    "    (positive_ids, negative_ids), (positive_mask, negative_mask) = D_ids, D_mask\n",
    "\n",
    "    query_batches = _split_into_batches(Q_ids, Q_mask, bsize)\n",
    "    positive_batches = _split_into_batches(positive_ids, positive_mask, bsize)\n",
    "    negative_batches = _split_into_batches(negative_ids, negative_mask, bsize)\n",
    "\n",
    "    batches = []\n",
    "    for (q_ids, q_mask), (p_ids, p_mask), (n_ids, n_mask) in zip(query_batches, positive_batches, negative_batches):\n",
    "        Q = (torch.cat((q_ids, q_ids)), torch.cat((q_mask, q_mask)))\n",
    "        D = (torch.cat((p_ids, n_ids)), torch.cat((p_mask, n_mask)))\n",
    "        batches.append((Q, D))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def _sort_by_length(ids, mask, bsize):\n",
    "    if ids.size(0) <= bsize:\n",
    "        return ids, mask, torch.arange(ids.size(0))\n",
    "\n",
    "    indices = mask.sum(-1).sort().indices\n",
    "    reverse_indices = indices.sort().indices\n",
    "\n",
    "    return ids[indices], mask[indices], reverse_indices\n",
    "\n",
    "\n",
    "def _split_into_batches(ids, mask, bsize):\n",
    "    batches = []\n",
    "    for offset in range(0, ids.size(0), bsize):\n",
    "        batches.append((ids[offset : offset + bsize], mask[offset : offset + bsize]))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "\n",
    "def print_message(*s, condition=True):\n",
    "    s = ' '.join([str(x) for x in s])\n",
    "    msg = \"[{}] {}\".format(datetime.datetime.now().strftime(\"%b %d, %H:%M:%S\"), s)\n",
    "\n",
    "    if condition:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "    return msg\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    format_str = \"%Y-%m-%d_%H.%M.%S\"\n",
    "    result = datetime.datetime.now().strftime(format_str)\n",
    "    return result\n",
    "\n",
    "\n",
    "def file_tqdm(file):\n",
    "    print(f\"#> Reading {file.name}\")\n",
    "\n",
    "    with tqdm.tqdm(total=os.path.getsize(file.name) / 1024.0 / 1024.0, unit=\"MiB\") as pbar:\n",
    "        for line in file:\n",
    "            yield line\n",
    "            pbar.update(len(line) / 1024.0 / 1024.0)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "\n",
    "def save_checkpoint(path, epoch_idx, mb_idx, model, optimizer, arguments=None):\n",
    "    print(f\"#> Saving a checkpoint to {path} ..\")\n",
    "\n",
    "    if hasattr(model, 'module'):\n",
    "        model = model.module  # extract model from a distributed/data-parallel wrapper\n",
    "\n",
    "    checkpoint = {}\n",
    "    checkpoint['epoch'] = epoch_idx\n",
    "    checkpoint['batch'] = mb_idx\n",
    "    checkpoint['model_state_dict'] = model.state_dict()\n",
    "    checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
    "    checkpoint['arguments'] = arguments\n",
    "\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None, do_print=True):\n",
    "    if do_print:\n",
    "        print_message(\"#> Loading checkpoint\", path, \"..\")\n",
    "\n",
    "    if path.startswith(\"http:\") or path.startswith(\"https:\"):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(path, map_location='cpu')\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k\n",
    "        if k[:7] == 'module.':\n",
    "            name = k[7:]\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    checkpoint['model_state_dict'] = new_state_dict\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except:\n",
    "        print_message(\"[WARNING] Loading checkpoint with strict=False\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    if do_print:\n",
    "        print_message(\"#> checkpoint['epoch'] =\", checkpoint['epoch'])\n",
    "        print_message(\"#> checkpoint['batch'] =\", checkpoint['batch'])\n",
    "\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def create_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        print('\\n')\n",
    "        print_message(\"#> Note: Output directory\", path, 'already exists\\n\\n')\n",
    "    else:\n",
    "        print('\\n')\n",
    "        print_message(\"#> Creating directory\", path, '\\n\\n')\n",
    "        os.makedirs(path)\n",
    "\n",
    "# def batch(file, bsize):\n",
    "#     while True:\n",
    "#         L = [ujson.loads(file.readline()) for _ in range(bsize)]\n",
    "#         yield L\n",
    "#     return\n",
    "\n",
    "\n",
    "def f7(seq):\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/a/480227/1493011\n",
    "    \"\"\"\n",
    "\n",
    "    seen = set()\n",
    "    return [x for x in seq if not (x in seen or seen.add(x))]\n",
    "\n",
    "\n",
    "def batch(group, bsize, provide_offset=False):\n",
    "    offset = 0\n",
    "    while offset < len(group):\n",
    "        L = group[offset: offset + bsize]\n",
    "        yield ((offset, L) if provide_offset else L)\n",
    "        offset += len(L)\n",
    "    return\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"\n",
    "    dot.notation access to dictionary attributes\n",
    "    Credit: derek73 @ https://stackoverflow.com/questions/2352181\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "def flatten(L):\n",
    "    return [x for y in L for x in y]\n",
    "\n",
    "\n",
    "def zipstar(L, lazy=False):\n",
    "    \"\"\"\n",
    "    A much faster A, B, C = zip(*[(a, b, c), (a, b, c), ...])\n",
    "    May return lists or tuples.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(L) == 0:\n",
    "        return L\n",
    "\n",
    "    width = len(L[0])\n",
    "\n",
    "    if width < 100:\n",
    "        return [[elem[idx] for elem in L] for idx in range(width)]\n",
    "\n",
    "    L = zip(*L)\n",
    "\n",
    "    return L if lazy else list(L)\n",
    "\n",
    "\n",
    "def zip_first(L1, L2):\n",
    "    length = len(L1) if type(L1) in [tuple, list] else None\n",
    "\n",
    "    L3 = list(zip(L1, L2))\n",
    "\n",
    "    assert length in [None, len(L3)], \"zip_first() failure: length differs!\"\n",
    "\n",
    "    return L3\n",
    "\n",
    "\n",
    "def int_or_float(val):\n",
    "    if '.' in val:\n",
    "        return float(val)\n",
    "        \n",
    "    return int(val)\n",
    "\n",
    "def load_ranking(path, types=None, lazy=False):\n",
    "    print_message(f\"#> Loading the ranked lists from {path} ..\")\n",
    "\n",
    "    try:\n",
    "        lists = torch.load(path)\n",
    "        lists = zipstar([l.tolist() for l in tqdm.tqdm(lists)], lazy=lazy)\n",
    "    except:\n",
    "        if types is None:\n",
    "            types = itertools.cycle([int_or_float])\n",
    "\n",
    "        with open(path) as f:\n",
    "            lists = [[typ(x) for typ, x in zip_first(types, line.strip().split('\\t'))]\n",
    "                     for line in file_tqdm(f)]\n",
    "\n",
    "    return lists\n",
    "\n",
    "\n",
    "def save_ranking(ranking, path):\n",
    "    lists = zipstar(ranking)\n",
    "    lists = [torch.tensor(l) for l in lists]\n",
    "\n",
    "    torch.save(lists, path)\n",
    "\n",
    "    return lists\n",
    "\n",
    "\n",
    "def groupby_first_item(lst):\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    for first, *rest in lst:\n",
    "        rest = rest[0] if len(rest) == 1 else rest\n",
    "        groups[first].append(rest)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def process_grouped_by_first_item(lst):\n",
    "    \"\"\"\n",
    "        Requires items in list to already be grouped by first item.\n",
    "    \"\"\"\n",
    "\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    started = False\n",
    "    last_group = None\n",
    "\n",
    "    for first, *rest in lst:\n",
    "        rest = rest[0] if len(rest) == 1 else rest\n",
    "\n",
    "        if started and first != last_group:\n",
    "            yield (last_group, groups[last_group])\n",
    "            assert first not in groups, f\"{first} seen earlier --- violates precondition.\"\n",
    "\n",
    "        groups[first].append(rest)\n",
    "\n",
    "        last_group = first\n",
    "        started = True\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"\"\"\n",
    "    Collect data into fixed-length chunks or blocks\n",
    "        Example: grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "        Source: https://docs.python.org/3/library/itertools.html#itertools-recipes\n",
    "    \"\"\"\n",
    "\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "\n",
    "# see https://stackoverflow.com/a/45187287\n",
    "class NullContextManager(object):\n",
    "    def __init__(self, dummy_resource=None):\n",
    "        self.dummy_resource = dummy_resource\n",
    "    def __enter__(self):\n",
    "        return self.dummy_resource\n",
    "    def __exit__(self, *args):\n",
    "        pass\n",
    "\n",
    "\n",
    "def load_batch_backgrounds(args, qids):\n",
    "    if args.qid2backgrounds is None:\n",
    "        return None\n",
    "\n",
    "    qbackgrounds = []\n",
    "\n",
    "    for qid in qids:\n",
    "        back = args.qid2backgrounds[qid]\n",
    "\n",
    "        if len(back) and type(back[0]) == int:\n",
    "            x = [args.collection[pid] for pid in back]\n",
    "        else:\n",
    "            x = [args.collectionX.get(pid, '') for pid in back]\n",
    "\n",
    "        x = ' [SEP] '.join(x)\n",
    "        qbackgrounds.append(x)\n",
    "    \n",
    "    return qbackgrounds\n",
    "\n",
    "\n",
    "class ColBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, mask_punctuation=string.punctuation, dim=128, similarity_metric=\"cosine\"):\n",
    "        super(ColBERT, self).__init__(config)\n",
    "\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.dim = dim\n",
    "\n",
    "        self.mask_punctuation = mask_punctuation\n",
    "        self.skiplist = {}\n",
    "\n",
    "        if self.mask_punctuation:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('klue/bert-base')\n",
    "            self.skiplist = {\n",
    "                w: True\n",
    "                for symbol in string.punctuation\n",
    "                for w in [symbol, self.tokenizer.encode(symbol, add_special_tokens=False)[0]]\n",
    "            }\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.linear = nn.Linear(config.hidden_size, dim, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, Q=None, D=None):\n",
    "        # return self.query(**Q), self.doc(**D)\n",
    "        return self.score(self.query(**Q), self.doc(**D))\n",
    "\n",
    "    def query(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # input_ids, attention_mask = input_ids.to(\"cuda\"), attention_mask.to(\"cuda\")\n",
    "        Q = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        # Q_pooled_outputs = Q_outputs[1]\n",
    "        Q = self.linear(Q)\n",
    "        # return Q\n",
    "        return torch.nn.functional.normalize(Q, p=2, dim=2)\n",
    "\n",
    "    def doc(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # input_ids, attention_mask = input_ids.to(\"cuda\"), attention_mask.to(\"cuda\")\n",
    "        D = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        D = self.linear(D)\n",
    "\n",
    "        mask = torch.tensor(self.mask(input_ids), device=\"cuda\").unsqueeze(2).float()\n",
    "        D = D * mask\n",
    "\n",
    "        D = torch.nn.functional.normalize(D, p=2, dim=2)\n",
    "\n",
    "        # if not keep_dims:\n",
    "        #     D, mask = D.cpu().to(dtype=torch.float16), mask.cpu().bool().squeeze(-1)\n",
    "        #     D = [d[mask[idx]] for idx, d in enumerate(D)]\n",
    "\n",
    "        return D\n",
    "\n",
    "    def score(self, Q, D):\n",
    "        if self.similarity_metric == \"cosine\":\n",
    "            return (Q @ D.permute(0, 2, 1)).max(2).values.sum(1)\n",
    "\n",
    "        assert self.similarity_metric == \"l2\"\n",
    "        return (-1.0 * ((Q.unsqueeze(2) - D.unsqueeze(1)) ** 2).sum(-1)).max(-1).values.sum(-1)\n",
    "\n",
    "    def mask(self, input_ids):\n",
    "        mask = [[(x not in self.skiplist) and (x != 0) for x in d] for d in input_ids.cpu().tolist()]\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSampler(Sampler) :\n",
    "    def __init__(self, data_source, batch_size) :\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self) :\n",
    "        n = len(self.data_source)\n",
    "        index_list = []\n",
    "        while True :\n",
    "            out = True\n",
    "            for i in range(self.batch_size) :\n",
    "                tmp_data = random.randint(0, n-1)\n",
    "                index_list.append(tmp_data)\n",
    "            for f, s in zip(index_list, index_list[1:]) :\n",
    "                if abs(s-f) <= 2 :\n",
    "                    out = False\n",
    "            if out == True :\n",
    "                break\n",
    "\n",
    "        while True : # 추가 삽입\n",
    "            tmp_data = random.randint(0, n-1)\n",
    "            if (tmp_data not in index_list) and \\\n",
    "                (abs(tmp_data-index_list[-i]) > 2 for i in range(1,self.batch_size+1)) \\\n",
    "            : \n",
    "                index_list.append(tmp_data)\n",
    "            if len(index_list) == n :\n",
    "                break\n",
    "        return iter(index_list)\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRetrieval:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        # tokenizer,\n",
    "        q_tokenizer,\n",
    "        d_tokenizer,\n",
    "        colbert_encoder,\n",
    "        df\n",
    "\n",
    "    ):\n",
    "        \"\"\"\n",
    "        학습과 추론에 사용될 여러 셋업을 마쳐봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.q_tokenizer = q_tokenizer\n",
    "        self.d_tokenizer = d_tokenizer\n",
    "        self.colbert_encoder = colbert_encoder\n",
    "\n",
    "        self.df = df\n",
    "        # self.p_encoder=None\n",
    "        # self.q_encoder=None\n",
    "\n",
    "    def train(self, args=None, tokenizer = None, df=None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        if tokenizer is None :\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        # q_seqs = tokenizer(self.dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        # p_seqs = tokenizer(self.dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt') \n",
    "\n",
    "        # train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "        #                 q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])\n",
    "        # train_dataloader = DataLoader(train_dataset, batch_size=args.per_device_train_batch_size)\n",
    "\n",
    "        # tensorize_triples(query_tokenizer, doc_tokenizer, queries, positives, negatives, bsize):\n",
    "        train_dataloader = tensorize_triples(\n",
    "            self.q_tokenizer\n",
    "            , self.d_tokenizer\n",
    "            , self.df[\"question\"]\n",
    "            , self.df[\"original_context\"]\n",
    "            , self.df[\"context\"]\n",
    "            , self.args.per_device_train_batch_size, )\n",
    "\n",
    "        no_decay = [\"bias\" ,\"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.colbert_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.colbert_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            # eps=args.adam_epsilon\n",
    "        )\n",
    "\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "        \n",
    "        global_step = 0\n",
    "\n",
    "        # self.p_encoder.zero_grad()\n",
    "        # self.q_encoder.zero_grad()\n",
    "        self.colbert_encoder.zero_grad()\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "        # self.q_encoder.train()\n",
    "        # self.p_encoder.train()\n",
    "        self.colbert_encoder.train()\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch, _ in enumerate(train_iterator):\n",
    "            # epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            # loss_value=0 # Accumulation할 때 진행\n",
    "            losses = 0\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                # if torch.cuda.is_available():\n",
    "                #     batch = tuple(t.to_device('cuda') for t in batch)\n",
    "\n",
    "                # D\n",
    "                p_inputs = {'input_ids': batch[0].cuda(),\n",
    "                            'attention_mask': batch[1].cuda(),\n",
    "                            'token_type_ids': batch[2].cuda()\n",
    "                            }\n",
    "                # Q\n",
    "                q_inputs = {'input_ids': batch[3].cuda(),\n",
    "                            'attention_mask': batch[4].cuda(),\n",
    "                            'token_type_ids': batch[5].cuda()}\n",
    "\n",
    "                sim_scores = self.colbert_encoder(Q=q_inputs, D=p_inputs)\n",
    "\n",
    "                # Calculate similarity score & loss\n",
    "                # sim_scores = self.colbert_encoder.score(q_outputs, p_outputs)\n",
    "                print(f'sim_scores {sim_scores}')\n",
    "\n",
    "                # target: position of positive samples = diagonal element \n",
    "                targets = torch.zeros(args.per_device_train_batch_size).long()\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    targets = targets.to('cuda')\n",
    "\n",
    "                # sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                # loss = -F.log_softmax(sim_scores)[:,0].mean()\n",
    "                print(f'sim_scores.shape, targets.shate {sim_scores.shape} | {targets.shape}')\n",
    "\n",
    "                loss = criterion(sim_scores.size(1), targets.size(1))\n",
    "                # loss = F.kl_div(F.log_softmax(sim_scores, 1), targets, reduction='none').sum(1)\n",
    "                # loss = F.nll_loss(sim_scores, targets)\n",
    "                losses += loss.item()\n",
    "                if step % 100 == 0 :\n",
    "                    print(f'{epoch}epoch loss: {losses/(step+1)}') # Accumulation할 경우 주석처리\n",
    "\n",
    "                \n",
    "                #################ACCUMULATION###############################\n",
    "                # loss_value += loss\n",
    "                # if (step+1) % args.gradient_accumulation_steps == 0 :\n",
    "                #     optimizer.step()\n",
    "                #     scheduler.step()\n",
    "                #     self.q_encoder.zero_grad()\n",
    "                #     self.p_encoder.zero_grad()\n",
    "                #     global_step += 1\n",
    "                #     print(loss_value/args.gradient_accumulation_steps)\n",
    "                #     loss_value = 0\n",
    "                ############################################################\n",
    "                self.colbert_encoder.zero_grad()\n",
    "                # self.p_encoder.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                global_step += 1\n",
    "                \n",
    "                #torch.cuda.empty_cache()\n",
    "                del p_inputs, q_inputs\n",
    "\n",
    "        return self.colbert_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# q_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing ColBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "colbert_encoder = ColBERT.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/opt/ml/data/colbertdata_join_top10_wikipedia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4fd673cc6b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcolbert_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-d80a24ddb4f9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, args, tokenizer, df)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;31m# D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 p_inputs = {'input_ids': batch[0].cuda(),\n\u001b[0m\u001b[1;32m     88\u001b[0m                             \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                             \u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "# Retriever는 아래와 같이 사용할 수 있도록 코드를 짜봅시다.\n",
    "q_tokenizer = QueryTokenizer()\n",
    "d_tokenizer = DocTokenizer()\n",
    "\n",
    "retriever = DenseRetrieval(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    q_tokenizer = q_tokenizer,\n",
    "    d_tokenizer = d_tokenizer,\n",
    "    colbert_encoder=colbert_encoder,\n",
    "    df=df\n",
    ")\n",
    "colbert_encoder = retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
